# -*- coding: utf-8 -*-
import os
import sys
import json
import time
import hashlib
import requests
import pandas as pd
import shutil
from datetime import datetime, timedelta
from pathlib import Path

# é…ç½®UTF-8ç¼–ç 
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ æ•°æ®åº“æ¥å£å’Œæ–‡ä»¶åˆå¹¶æ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
sys.path.append(r'D:\testyd\mysql')

try:
    from crawler_db_interface import CrawlerDBInterface
    from merge_excel_files import ExcelMerger
    print("âœ“ æˆåŠŸå¯¼å…¥æ•°æ®åº“æ¥å£å’Œæ–‡ä»¶åˆå¹¶æ¨¡å—")
except ImportError as e:
    print(f"âœ— å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

class TmallBadScoreCollector:
    """å¤©çŒ«å·®è¯„æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        # æ•°æ®åº“æ¥å£åˆå§‹åŒ–
        self.db_interface = CrawlerDBInterface(
            platform='tm',
            shops_table='tm_shops',
            tasks_table='tm_tasks',
            database='company'
        )
        
        # åŸºç¡€ç›®å½•é…ç½®
        self.base_dir = Path(r"D:\yingdao\tm\å¤©çŒ«å·®è¯„æ•°æ®")
        
        # åˆå¹¶æ–‡ä»¶ç›®å½•
        self.merged_dir = Path(r"D:\yingdao\tm\åˆå¹¶æ–‡ä»¶\å¤©çŒ«å·®è¯„æ•°æ®")
        
        # ç›®æ ‡æ—¥æœŸï¼ˆ13å¤©å‰ï¼Œt-13ï¼‰
        self.target_date = datetime.now() - timedelta(days=13)
        self.target_date_str = self.target_date.strftime('%Y-%m-%d')
        
        # APIé…ç½®
        self.API_URL = "https://h5api.m.taobao.com/h5/mtop.rm.sellercenter.list.data.pc/1.0/"
        self.app_key = "12574478"
        
        # MinIOé…ç½®
        self.minio_api_url = "http://127.0.0.1:8009/api/upload"
        
        # Dremioé…ç½®
        self.dremio_config = {
            'host': 'localhost',
            'port': 9047,
            'username': 'admin',
            'password': 'admin123'
        }
        
        print(f"ç›®æ ‡æ—¥æœŸ: {self.target_date} ({self.target_date_str})")
        
    def get_h5_token(self, cookies_str):
        """ä»cookieå­—ç¬¦ä¸²ä¸­æå–h5 token"""
        try:
            if not cookies_str:
                print("Cookieå­—ç¬¦ä¸²ä¸ºç©º")
                return None
                
            for cookie in cookies_str.split(';'):
                if '_m_h5_tk=' in cookie:
                    token_value = cookie.split('_m_h5_tk=')[1].strip()
                    # tokenæ ¼å¼ä¸º: token_expireTimeï¼Œæˆ‘ä»¬åªéœ€è¦tokenéƒ¨åˆ†
                    token = token_value.split('_')[0]
                    print(f"âœ“ æˆåŠŸæå–token: {token[:20]}...")
                    return token
            
            print(f"âš ï¸ åœ¨cookieä¸­æœªæ‰¾åˆ°_m_h5_tkï¼Œcookieå¼€å¤´: {cookies_str[:100]}...")
            return None
        except Exception as e:
            print(f"æå–tokenå¤±è´¥: {e}")
            return None
    
    def generate_sign(self, token, timestamp, data):
        """ç”Ÿæˆç­¾å - æŒ‰ç…§æ·˜å®mtop APIæ ‡å‡†ç®—æ³•"""
        try:
            # ç­¾åç®—æ³•: md5(token + '&' + timestamp + '&' + appKey + '&' + data)
            sign_str = f"{token}&{timestamp}&{self.app_key}&{data}"
            return hashlib.md5(sign_str.encode('utf-8')).hexdigest()
        except Exception as e:
            print(f"ç”Ÿæˆç­¾åå¤±è´¥: {e}")
            return None
    
    def generate_daily_tasks(self):
        """ç”Ÿæˆå½“æ—¥ä»»åŠ¡"""
        print("\n=== ç”Ÿæˆå½“æ—¥ä»»åŠ¡ ===")
        
        try:
            # å®šä¹‰ä»»åŠ¡åˆ—
            task_columns = ['badscore_status']
            
            # ç”Ÿæˆä»»åŠ¡
            created_count = self.db_interface.generate_tasks(self.target_date_str, task_columns)
            print(f"âœ“ æˆåŠŸç”Ÿæˆ {created_count} ä¸ªä»»åŠ¡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡ç”Ÿæˆ
            if created_count == 0:
                print("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•ä»»åŠ¡ï¼Œå¯èƒ½æ˜¯å› ä¸ºä»»åŠ¡å·²å­˜åœ¨æˆ–æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åº—é“º")
            
            return True
        except Exception as e:
            print(f"âœ— ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
            return False
    
    def get_shops_with_tasks(self):
        """è·å–æœ‰å¾…å¤„ç†badscore_statusä»»åŠ¡çš„åº—é“ºä¿¡æ¯"""
        try:
            # è·å–å¾…å¤„ç†ä»»åŠ¡ - ä½¿ç”¨ä¸tm_kpi.pyç›¸åŒçš„æ–¹æ³•
            pending_tasks = self.db_interface.get_pending_tasks(self.target_date_str, 'badscore_status')
            
            if not pending_tasks:
                print(f"æ²¡æœ‰æ‰¾åˆ° badscore_status ç±»å‹çš„å¾…å¤„ç†ä»»åŠ¡")
                return {}
            
            print(f"æ‰¾åˆ° {len(pending_tasks)} ä¸ªå¾…å¤„ç†çš„badscore_statusä»»åŠ¡")
            
            # æ„å»ºåº—é“ºä¿¡æ¯å­—å…¸
            shops_info = {}
            for task in pending_tasks:
                shop_name = task[1] if len(task) > 1 else None  # dt.shop_name
                qncookie = task[7] if len(task) > 7 else None  # s.qncookie (ç¬¬6åˆ—)
                sycmcookie = task[8] if len(task) > 8 else None  # s.sycmcookie (ç¬¬7åˆ—)
                
                if sycmcookie or qncookie:
                    shops_info[shop_name] = {
                        'shop_name': shop_name,
                        'qncookie': qncookie,
                        'sycmcookie': sycmcookie,
                        'task_info': task
                    }
                    print(f"âœ“ åº—é“º {shop_name} - qncookie: {qncookie[:50] if qncookie else 'None'}...")
                else:
                    print(f"âš ï¸ åº—é“º {shop_name} ç¼ºå°‘cookieä¿¡æ¯ï¼Œè·³è¿‡")
            
            return shops_info
            
        except Exception as e:
            print(f"è·å–åº—é“ºä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def fetch_comments(self, cookies_str, start_date="20250924", end_date="20250926", page_num=1, page_size=20):
        """è·å–è¯„ä»·æ•°æ®"""
        try:
            # æå–token
            token = self.get_h5_token(cookies_str)
            if not token:
                print("æ— æ³•æå–tokenï¼Œè¯·æ£€æŸ¥cookies")
                return None
            
            print(f"æå–åˆ°token: {token}")
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            timestamp = str(int(time.time() * 1000))
            print(f"æ—¶é—´æˆ³: {timestamp}")
            
            # æ„é€ è¯·æ±‚æ•°æ® - æŒ‰ç…§å¤©çŒ«æç¤ºè¯æ–‡æ¡£çš„æ ¼å¼
            json_body = {
                "pageType": "rateWait4PC",
                "pagination": {
                    "current": page_num,
                    "pageSize": page_size
                },
                "emotion":13,
                "dateRange": [start_date, end_date]
            }
            
            # å°†jsonBodyè½¬æ¢ä¸ºå­—ç¬¦ä¸²ç”¨äºç­¾åè®¡ç®—
            json_body_str = json.dumps(json_body, separators=(',', ':'))
            
            # æ„é€ å®Œæ•´çš„è¯·æ±‚æ•°æ®ç»“æ„
            request_data = {
                "jsonBody": json_body_str
            }
            
            data_str = json.dumps(request_data, separators=(',', ':'))
            print(f"è¯·æ±‚æ•°æ®: {data_str}")
            
            # ç”Ÿæˆç­¾å
            sign = self.generate_sign(token, timestamp, data_str)
            if not sign:
                print("ç­¾åç”Ÿæˆå¤±è´¥")
                return None
            
            # æ„é€ å®Œæ•´çš„URL - æŒ‰ç…§ç”¨æˆ·æä¾›çš„æ ¼å¼æ·»åŠ vå’ŒsyncCookieModeå‚æ•°
            params = {
                'jsv': '2.6.1',
                'appKey': self.app_key,
                't': timestamp,
                'sign': sign,
                'api': 'mtop.rm.sellercenter.list.data.pc',
                'v': '1.0',
                'syncCookieMode': 'true',
                'type': 'originaljson',
                'dataType': 'json'
            }
            
            # æ„é€ è¯·æ±‚å¤´ - æŒ‰ç…§å¤©çŒ«æç¤ºè¯æ–‡æ¡£çš„æ ¼å¼
            headers = {
                'Cookie': cookies_str,
                'origin': 'https://myseller.taobao.com',
                'referer': 'https://myseller.taobao.com/home.htm/comment-manage/list/rateWait4PC?current=1&pageSize=20&dateRange=20250924%2C20250926',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36',
                'accept': 'application/json',
                'accept-language': 'zh-CN,zh;q=0.9',
                'Content-Type': 'application/x-www-form-urlencoded',
                'priority': 'u=1, i',
                'sec-ch-ua': '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"',
                'sec-fetch-dest': 'empty',
                'sec-fetch-mode': 'cors',
                'sec-fetch-site': 'same-site'
            }
            
            # æ„é€ POSTæ•°æ® - æŒ‰ç…§mtop APIæ ‡å‡†æ ¼å¼
            # ç›´æ¥ä½¿ç”¨JSONå­—ç¬¦ä¸²ä½œä¸ºdataå‚æ•°å€¼ï¼Œä¸éœ€è¦URLç¼–ç 
            post_data = f'data={data_str}'
            
            print(f"è¯·æ±‚URL: {self.API_URL}")
            print(f"è¯·æ±‚å‚æ•°: {params}")
            print(f"POSTæ•°æ®: {post_data}")
            
            # å‘é€è¯·æ±‚
            response = requests.post(
                self.API_URL,
                params=params,
                headers=headers,
                data=post_data,
                timeout=30
            )
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print("=" * 50)
            print("å®Œæ•´å“åº”å†…å®¹:")
            print(response.text)
            print("=" * 50)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print("[OK] æˆåŠŸè·å–è¯„ä»·æ•°æ®")
                    print(f"å“åº”æ•°æ®ç±»å‹: {type(data)}")
                    print(f"å“åº”é”®: {list(data.keys())}")
                    
                    # æ‰“å°è¯¦ç»†çš„æ•°æ®ç»“æ„
                    print("\nè¯¦ç»†æ•°æ®ç»“æ„:")
                    if 'data' in data:
                        print(f"dataå­—æ®µå†…å®¹: {json.dumps(data['data'], indent=2, ensure_ascii=False)}")
                    if 'ret' in data:
                        print(f"retå­—æ®µå†…å®¹: {data['ret']}")
                    
                    return data
                except json.JSONDecodeError as e:
                    print(f"JSONè§£æå¤±è´¥: {e}")
                    return None
            else:
                print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"è·å–è¯„ä»·æ•°æ®å¤±è´¥: {e}")
            return None
    
    def process_comments_data(self, data):
        """å¤„ç†è¯„ä»·æ•°æ®"""
        try:
            if isinstance(data, dict) and 'data' in data:
                comments = data['data'].get('result', {}).get('list', [])
                processed_data = []
                
                for comment in comments:
                    processed_comment = {
                        'è¯„ä»·ID': comment.get('id', ''),
                        'å•†å“æ ‡é¢˜': comment.get('itemTitle', ''),
                        'ä¹°å®¶æ˜µç§°': comment.get('buyerNick', ''),
                        'è¯„ä»·å†…å®¹': comment.get('content', ''),
                        'è¯„ä»·æ—¶é—´': comment.get('createTime', ''),
                        'è¯„åˆ†': comment.get('rate', ''),
                        'è®¢å•å·': comment.get('orderId', ''),
                        'å•†å“ID': comment.get('itemId', ''),
                    }
                    processed_data.append(processed_comment)
                
                return processed_data
            else:
                print("æ•°æ®æ ¼å¼ä¸æ­£ç¡®")
                return []
                
        except Exception as e:
            print(f"å¤„ç†è¯„ä»·æ•°æ®å¤±è´¥: {e}")
            return []
    
    def save_to_excel(self, data, filename=None):
        """ä¿å­˜æ•°æ®åˆ°Excel"""
        try:
            if not data:
                print("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
                return False
            
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"å¤©çŒ«è¯„ä»·æ•°æ®_{timestamp}.xlsx"
            
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            save_dir = Path("d:/testyd/tm")
            save_dir.mkdir(exist_ok=True)
            
            filepath = save_dir / filename
            
            df = pd.DataFrame(data)
            df.to_excel(filepath, index=False, engine='openpyxl')
            
            print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
            return True
            
        except Exception as e:
            print(f"ä¿å­˜Excelæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def login(self):
        """ç®€åŒ–çš„ç™»å½•æ–¹æ³•ï¼Œè¿”å›cookieå­—ç¬¦ä¸²"""
        cookie_str, page, context = self.login_and_get_cookies()
        
        # æ¸…ç†èµ„æº
        if page:
            page.close()
        if context:
            context.close()
            
        return cookie_str
    
    def fetch_and_save_bad_reviews(self, shop_name, qncookie, sycmcookie):
        """è·å–å¹¶ä¿å­˜å·®è¯„æ•°æ® - ä½¿ç”¨æ­£ç¡®çš„fetch_commentsæ–¹æ³•"""
        try:
            # ä¼˜å…ˆä½¿ç”¨qncookieï¼ˆç¬¬6åˆ—ï¼‰ï¼Œè¿™æ˜¯æ­£ç¡®çš„cookie
            cookies_str = qncookie if qncookie else sycmcookie
            if not cookies_str:
                print(f"åº—é“º {shop_name} æ²¡æœ‰æœ‰æ•ˆçš„cookie")
                return False
            
            # è°ƒç”¨æ­£ç¡®çš„fetch_commentsæ–¹æ³• - ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸ
            target_date_str = self.target_date.strftime('%Y%m%d')
            result = self.fetch_comments(cookies_str, target_date_str, target_date_str, 1, 20)
            
            if result:
                # ç›´æ¥ä½¿ç”¨parse_and_save_dataæ–¹æ³•å¤„ç†æ•°æ®
                success = self.parse_and_save_data(result, shop_name)
                if success:
                    print(f"âœ“ åº—é“º {shop_name} å·®è¯„æ•°æ®å¤„ç†æˆåŠŸ")
                    return True
                else:
                    print(f"âœ— åº—é“º {shop_name} å·®è¯„æ•°æ®å¤„ç†å¤±è´¥")
                    return False
            else:
                print(f"âœ— åº—é“º {shop_name} è·å–å·®è¯„æ•°æ®å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âœ— åº—é“º {shop_name} è·å–å·®è¯„æ•°æ®å¼‚å¸¸: {e}")
            return False

    def parse_and_save_data(self, data, shop_name):
        """è§£æAPIå“åº”æ•°æ®å¹¶ä¿å­˜ä¸ºExcel"""
        try:
            print(f"è§£æåº—é“º {shop_name} çš„å·®è¯„æ•°æ®...")
            print(f"åŸå§‹å“åº”æ•°æ®ç»“æ„: {type(data)}")
            
            # æ£€æŸ¥å“åº”ç»“æ„
            if not isinstance(data, dict):
                print("âœ— å“åº”æ•°æ®æ ¼å¼é”™è¯¯")
                return False
            
            # è·å–è¯„ä»·åˆ—è¡¨ - ä½¿ç”¨æ­£ç¡®çš„æ•°æ®è·¯å¾„: data.data.dataSource
            comments_list = []
            if 'data' in data and isinstance(data['data'], dict):
                data_section = data['data'].get('data', {})
                if isinstance(data_section, dict):
                    comments_list = data_section.get('dataSource', [])
            
            print(f"è§£æåˆ°çš„è¯„ä»·æ•°æ®æ•°é‡: {len(comments_list)}")
            
            if not comments_list:
                print("æ²¡æœ‰æ‰¾åˆ°è¯„ä»·æ•°æ®")
                return True  # æ²¡æœ‰æ•°æ®ä¹Ÿç®—æˆåŠŸ
            
            print(f"æ‰¾åˆ° {len(comments_list)} æ¡è¯„ä»·æ•°æ®")
            
            # å¤„ç†æ‰€æœ‰è¯„ä»·æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•ç­›é€‰
            records = []
            for i, comment in enumerate(comments_list):
                # è·å–è¯„ä»·åŸºæœ¬ä¿¡æ¯
                rate_content = comment.get('rateContent', {})
                main_rate = rate_content.get('mainRate', {})
                append_rate = rate_content.get('appendRate', {})
                item_info = comment.get('itemInfo', {})
                user_info = comment.get('userInfo', {})
                order_info = comment.get('orderInfo', {})
                emotion_type = comment.get('emotionType', {})
                operator = comment.get('operator', {})
                
                print(f"å¤„ç†ç¬¬ {i+1} æ¡è¯„ä»·: feedId={main_rate.get('feedId', 'N/A')}")
                
                # æå–feedbackID - ä»operator.dataSourceä¸­è·å–
                feedback_id = ''
                if operator and 'dataSource' in operator:
                    for data_item in operator['dataSource']:
                        if 'params' in data_item and 'feedbackID' in data_item['params']:
                            feedback_id = data_item['params']['feedbackID']
                            break
                
                # å¤„ç†å›¾ç‰‡é“¾æ¥ - ä»mainRateå’ŒappendRateçš„mediaListä¸­æå–
                picture_columns = {}
                picture_count = 1
                
                # å¤„ç†ä¸»è¯„ä»·çš„å›¾ç‰‡
                if main_rate and 'mediaList' in main_rate:
                    for media in main_rate['mediaList']:
                        if media.get('uiType') == 'image' and media.get('thumbnail'):
                            thumbnail = media['thumbnail']
                            # æ·»åŠ http:å‰ç¼€
                            if thumbnail.startswith('//'):
                                thumbnail = 'http:' + thumbnail
                            picture_columns[f'picture{picture_count}'] = thumbnail
                            picture_count += 1
                
                # å¤„ç†è¿½è¯„çš„å›¾ç‰‡
                if append_rate and 'mediaList' in append_rate:
                    for media in append_rate['mediaList']:
                        if media.get('uiType') == 'image' and media.get('thumbnail'):
                            thumbnail = media['thumbnail']
                            # æ·»åŠ http:å‰ç¼€
                            if thumbnail.startswith('//'):
                                thumbnail = 'http:' + thumbnail
                            picture_columns[f'picture{picture_count}'] = thumbnail
                            picture_count += 1
                
                # å¤„ç†è¡¨è¾¾å¼å†…å®¹ - ä»mainRateå’ŒappendRateçš„expressionä¸­æå–
                expressions = []
                if main_rate and 'expression' in main_rate:
                    for expr in main_rate['expression']:
                        if 'content' in expr:
                            expressions.append(expr['content'])
                
                if append_rate and 'expression' in append_rate:
                    for expr in append_rate['expression']:
                        if 'content' in expr:
                            expressions.append(expr['content'])
                
                expression_text = '; '.join(expressions) if expressions else ''
                
                # æ„å»ºåŸºç¡€è®°å½•
                record = {
                    'åº—é“ºåç§°': shop_name,
                    'feedbackID': feedback_id,
                    'feedId': main_rate.get('feedId', ''),
                    'å•†å“æ ‡é¢˜': item_info.get('title', ''),
                    'ä¹°å®¶æ˜µç§°': user_info.get('userName', ''),
                    'ç”¨æˆ·è¯„ä»·': main_rate.get('content', ''),
                    'è¿½è¯„å†…å®¹': append_rate.get('content', '') if append_rate else '',
                    'expression': expression_text,
                    'è¯„ä»·æ—¶é—´': main_rate.get('date', ''),
                    'è¯„åˆ†': 0,  # APIå“åº”ä¸­æ²¡æœ‰ç›´æ¥çš„è¯„åˆ†å­—æ®µ
                    'è®¢å•å·': order_info.get('orderId', ''),
                    'å•†å“ID': item_info.get('itemId', ''),
                    'ç»Ÿè®¡æ—¥æœŸ': self.target_date_str,
                    'æƒ…æ„ŸçŠ¶æ€': emotion_type.get('status', ''),
                    'è¿½è¯„çŠ¶æ€': emotion_type.get('appendRateStatus', ''),
                    'ç”¨æˆ·æ˜Ÿçº§': user_info.get('userStar', ''),
                    'æ˜¯å¦å¤–å›½äºº': user_info.get('isForeigner', False),
                    'å•†å“é“¾æ¥': item_info.get('link', ''),
                    'åŸå§‹æ•°æ®': str(comment)  # ä¿ç•™å®Œæ•´çš„åŸå§‹æ•°æ®ç”¨äºåˆ†æ
                }
                
                # æ·»åŠ å›¾ç‰‡åˆ—
                for pic_key, pic_url in picture_columns.items():
                    record[pic_key] = pic_url
                
                records.append(record)
            
            print(f"å‡†å¤‡ä¿å­˜ {len(records)} æ¡è®°å½•")
            
            if not records:
                print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è¯„ä»·æ•°æ®")
                return True  # æ²¡æœ‰æ•°æ®ä¹Ÿç®—æˆåŠŸ
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(records)
            
            # åˆ›å»ºæ—¥æœŸç›®å½•
            date_dir = self.base_dir / self.target_date_str
            date_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ä¸ºExcelæ–‡ä»¶ï¼Œä»¥åº—é“ºåç§°å‘½å
            filename = f"{shop_name}.xlsx"
            file_path = date_dir / filename
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if file_path.exists():
                file_path.unlink()
                print(f"åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
            
            df.to_excel(file_path, index=False, engine='openpyxl')
            print(f"âœ“ å·®è¯„æ•°æ®å·²ä¿å­˜: {file_path}")
            print(f"âœ“ å…±ä¿å­˜ {len(records)} æ¡å·®è¯„è®°å½•")
            
            return str(file_path)
            
        except Exception as e:
            print(f"âœ— è§£æå’Œä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            return False

    def update_task_status(self, shop_name, task_type, status="å·²å®Œæˆ"):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            success = self.db_interface.update_task_status(
                self.target_date_str, 
                shop_name, 
                task_type, 
                status
            )
            if success:
                print(f"âœ“ ä»»åŠ¡çŠ¶æ€æ›´æ–°æˆåŠŸ: {shop_name} - {task_type} -> {status}")
            else:
                print(f"âœ— ä»»åŠ¡çŠ¶æ€æ›´æ–°å¤±è´¥: {shop_name} - {task_type}")
            return success
        except Exception as e:
            print(f"âœ— æ›´æ–°ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False

    def merge_and_upload_files(self):
        """åˆå¹¶æ–‡ä»¶å¹¶ä¸Šä¼ åˆ°MinIO"""
        print(f"\n=== åˆå¹¶å·®è¯„æ–‡ä»¶å¹¶ä¸Šä¼  ===")
        
        try:
            # ç¡®å®šæºç›®å½•å’Œç›®æ ‡ç›®å½•
            source_dir = self.base_dir / self.target_date_str
            target_dir = self.merged_dir
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # ç›®æ ‡æ–‡ä»¶è·¯å¾„
            target_merged_file = target_dir / f"{self.target_date_str}.xlsx"
            
            # æ£€æŸ¥æºç›®å½•æ˜¯å¦å­˜åœ¨æ–‡ä»¶
            if source_dir.exists() and any(source_dir.glob("*.xlsx")):
                print(f"ğŸ“ æºç›®å½•: {source_dir}")
                print(f"ğŸ“ ç›®æ ‡ç›®å½•: {target_dir}")
                
                # ä½¿ç”¨ExcelMergeråˆå¹¶æ–‡ä»¶
                merger = ExcelMerger(str(source_dir))
                success = merger.merge_excel_files(f"{self.target_date_str}.xlsx")
                
                if success:
                    # ç§»åŠ¨åˆå¹¶åçš„æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
                    source_merged_file = source_dir / f"{self.target_date_str}.xlsx"
                    if source_merged_file.exists():
                        # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                        if target_merged_file.exists():
                            target_merged_file.unlink()
                        
                        shutil.move(str(source_merged_file), str(target_merged_file))
                        print(f"âœ“ åˆå¹¶æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {target_merged_file}")
                    else:
                        print(f"âœ— åˆå¹¶æ–‡ä»¶ä¸å­˜åœ¨: {source_merged_file}")
                        return False
                else:
                    print("âœ— æ–‡ä»¶åˆå¹¶å¤±è´¥")
                    return False
            else:
                print(f"âš ï¸ æºç›®å½•æ— æ–‡ä»¶ï¼Œåˆ›å»ºç©ºæ–‡ä»¶: {target_merged_file}")
                # åˆ›å»ºç©ºçš„DataFrameå¹¶ä¿å­˜ä¸ºExcelï¼Œä½†åŒ…å«åŸºæœ¬åˆ—ç»“æ„
                empty_df = pd.DataFrame({
                    'shop': ['æ— æ•°æ®'],
                    'comment_id': ['æ— æ•°æ®'],
                    'content': ['æ— æ•°æ®'],
                    'rating': ['æ— æ•°æ®'],
                    'date': ['æ— æ•°æ®']
                })
                empty_df.to_excel(target_merged_file, index=False)
            
            # ä¸Šä¼ åˆ°MinIO
            minio_path = f"ods/tm/tm_badscore/dt={self.target_date_str}/{self.target_date_str}.parquet"
            success = self.upload_to_minio(str(target_merged_file), minio_path)
            
            if success:
                print(f"âœ“ å·®è¯„æ–‡ä»¶ä¸Šä¼ MinIOæˆåŠŸ")
                
                # åˆ·æ–°Dremioè¡¨
                dremio_table = 'minio.warehouse.ods.tm.tm_badscore'
                self.refresh_dremio_table(dremio_table)
                return True
            else:
                print(f"âœ— å·®è¯„æ–‡ä»¶ä¸Šä¼ MinIOå¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âœ— åˆå¹¶ä¸Šä¼ è¿‡ç¨‹å¼‚å¸¸: {e}")
            return False

    def run(self):
        """ä¸»è¿è¡Œå‡½æ•° - å¤šåº—é“ºå·®è¯„æ•°æ®é‡‡é›†"""
        print("=== å¤©çŒ«å·®è¯„æ•°æ®é‡‡é›†ç¨‹åºå¯åŠ¨ ===")
        print(f"ç›®æ ‡æ—¥æœŸ: {self.target_date_str}")
        
        # 1. ç”Ÿæˆæ¯æ—¥ä»»åŠ¡
        print("\n=== ç”Ÿæˆæ¯æ—¥ä»»åŠ¡ ===")
        created_count = self.generate_daily_tasks()
        print(f"ç”Ÿæˆä»»åŠ¡æ•°é‡: {created_count}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡ç”Ÿæˆ
        if created_count == 0:
            print("âš ï¸ è­¦å‘Š: æ²¡æœ‰ç”Ÿæˆä»»ä½•ä»»åŠ¡ï¼Œå¯èƒ½æ˜¯å› ä¸º:")
            print("   - è¯¥æ—¥æœŸçš„ä»»åŠ¡å·²å­˜åœ¨")
            print("   - æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„åº—é“º")
            print("   - æ•°æ®åº“è¿æ¥é—®é¢˜")
        
        # 2. è·å–å¾…å¤„ç†çš„åº—é“ºä»»åŠ¡
        print("\n=== è·å–å¾…å¤„ç†ä»»åŠ¡ ===")
        shops_info = self.get_shops_with_tasks()
        
        if not shops_info:
            print("æ²¡æœ‰æ‰¾åˆ° badscore_status ç±»å‹çš„å¾…å¤„ç†ä»»åŠ¡")
            # å³ä½¿æ²¡æœ‰ä»»åŠ¡ä¹Ÿè¦æ‰§è¡Œåˆå¹¶ä¸Šä¼ ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
            self.merge_and_upload_files()
            return
        
        print(f"æ‰¾åˆ° {len(shops_info)} ä¸ªåº—é“ºçš„å¾…å¤„ç†ä»»åŠ¡")
        
        # 3. å¤„ç†æ¯ä¸ªåº—é“ºçš„å·®è¯„æ•°æ®
        success_count = 0
        total_count = len(shops_info)
        
        for shop_name, shop_info in shops_info.items():
            print(f"\n=== å¤„ç†åº—é“º: {shop_name} ===")
            
            try:
                # è·å–åº—é“ºçš„cookieå’Œä»»åŠ¡ä¿¡æ¯
                qncookie = shop_info['qncookie'] if 'qncookie' in shop_info else ''
                sycmcookie = shop_info['sycmcookie'] if 'sycmcookie' in shop_info else ''
                task_info = shop_info['task_info'] if 'task_info' in shop_info else None
                
                if not qncookie and not sycmcookie:
                    print(f"âœ— åº—é“º {shop_name} ç¼ºå°‘cookieä¿¡æ¯ï¼Œè·³è¿‡å¤„ç†")
                    continue
                
                # è·å–å·®è¯„æ•°æ®
                result = self.fetch_and_save_bad_reviews(shop_name, qncookie, sycmcookie)
                
                if result:
                    print(f"âœ“ åº—é“º {shop_name} å·®è¯„æ•°æ®å¤„ç†æˆåŠŸ")
                    success_count += 1
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    self.update_task_status(shop_name, 'badscore_status', 'å·²å®Œæˆ')
                else:
                    print(f"âœ— åº—é“º {shop_name} å·®è¯„æ•°æ®å¤„ç†å¤±è´¥")

                        
            except Exception as e:
                print(f"âœ— å¤„ç†åº—é“º {shop_name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                if 'task_id' in locals() and task_id:
                    self.update_task_status(shop_name, task_id, 'failed')
        
        # 4. åˆå¹¶æ–‡ä»¶å¹¶ä¸Šä¼ åˆ°MinIO
        print(f"\n=== æ•°æ®å¤„ç†å®Œæˆ ===")
        print(f"æˆåŠŸå¤„ç†: {success_count}/{total_count} ä¸ªåº—é“º")
        
        # æ‰§è¡Œåˆå¹¶ä¸Šä¼ 
        merge_success = self.merge_and_upload_files()
        
        if merge_success:
            print("âœ“ æ‰€æœ‰æ•°æ®å¤„ç†å’Œä¸Šä¼ å®Œæˆ")
        else:
            print("âœ— æ•°æ®åˆå¹¶ä¸Šä¼ å¤±è´¥")
        
        print("=== ç¨‹åºæ‰§è¡Œå®Œæˆ ===")

    def upload_to_minio(self, file_path, minio_path):
        """
        å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°MinIO
        ä¸tm_kpi.pyçš„upload_to_minioæ–¹æ³•ä¿æŒä¸€è‡´
        """
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(file_path)
            
            # å¤„ç†NaNå€¼ï¼Œç¡®ä¿æ•°æ®èƒ½å¤Ÿæ­£å¸¸åºåˆ—åŒ–
            df = df.fillna('')  # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            
            # å¤„ç†æ— ç©·å¤§å€¼
            df = df.replace([float('inf'), float('-inf')], '')
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½æ­£å¸¸åºåˆ—åŒ–
            for col in df.columns:
                if df[col].dtype in ['float64', 'float32']:
                    df[col] = df[col].replace([float('inf'), float('-inf')], '')
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…åºåˆ—åŒ–é—®é¢˜
                df[col] = df[col].astype(str)
            
            # å‡†å¤‡ä¸Šä¼ æ•°æ®
            upload_data = {
                "data": df.to_dict('records'),  # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                "target_path": minio_path,
                "format": "parquet",
                "bucket": "warehouse"
            }
            
            # å‘é€POSTè¯·æ±‚åˆ°MinIO API
            headers = {'Content-Type': 'application/json'}
            response = requests.post(self.minio_api_url, json=upload_data, headers=headers)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    print(f"âœ“ æˆåŠŸä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO: {minio_path}")
                    return True
                else:
                    print(f"âœ— MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âœ— MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return False
                    
        except Exception as e:
            print(f"âœ— ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIOæ—¶å‡ºé”™: {str(e)}")
            return False
    
    def refresh_dremio_table(self, table_name):
        """åˆ·æ–°Dremioè¡¨"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åˆ·æ–°Dremioè¡¨: {table_name}")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„Dremioåˆ·æ–°é€»è¾‘
            # ç›®å‰å…ˆç®€å•æ‰“å°ï¼Œåç»­å¯ä»¥é›†æˆå®é™…çš„Dremio APIè°ƒç”¨
            print(f"âœ“ Dremioè¡¨åˆ·æ–°å®Œæˆ: {table_name}")
            return True
        except Exception as e:
            print(f"âœ— åˆ·æ–°Dremioè¡¨å¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    collector = TmallBadScoreCollector()
    collector.run()

if __name__ == "__main__":
    main()