# -*- coding: utf-8 -*-
"""
å¤©çŒ«KPIæ•°æ®è·å–å·¥å…· - å‡çº§ç‰ˆ
åŠŸèƒ½ä¸€ï¼šè·å–è‡ªåˆ¶æŠ¥è¡¨æ•°æ®
åŠŸèƒ½äºŒï¼šè·å–å”®åè§£å†³åˆ†ææ•°æ®
é›†æˆä»»åŠ¡ç”Ÿæˆæ¨¡å—å’Œæ•°æ®åº“åŠ¨æ€è¯»å–åŠŸèƒ½
"""

import requests
import time
import json
import os
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
import sys
import shutil

# é…ç½®UTF-8ç¼–ç 
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ æ•°æ®åº“æ¥å£å’Œæ–‡ä»¶åˆå¹¶æ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
sys.path.append(r'D:\testyd\mysql')

try:
    from crawler_db_interface import CrawlerDBInterface
    from merge_excel_files import ExcelMerger
    print("âœ“ æˆåŠŸå¯¼å…¥æ•°æ®åº“æ¥å£å’Œæ–‡ä»¶åˆå¹¶æ¨¡å—")
except ImportError as e:
    print(f"âœ— å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    sys.exit(1)

class TmallKpiCollector:
    def __init__(self):
        # æ•°æ®åº“æ¥å£åˆå§‹åŒ–
        self.db_interface = CrawlerDBInterface(
            platform='tm',
            shops_table='tm_shops',
            tasks_table='tm_tasks',
            database='company'
        )
        
        # åŸºç¡€ç›®å½•é…ç½®
        self.base_report_dir = Path(r"D:\yingdao\tm\å¤©çŒ«å®¢æœç»©æ•ˆè‡ªåˆ¶æŠ¥è¡¨")
        self.base_analysis_dir = Path(r"D:\yingdao\tm\å¤©çŒ«å®¢æœç»©æ•ˆè§£å†³åˆ†ææŠ¥è¡¨")
        
        # åˆå¹¶æ–‡ä»¶ç›®å½•
        self.merged_report_dir = Path(r"D:\yingdao\tm\åˆå¹¶æ–‡ä»¶\å¤©çŒ«å®¢æœç»©æ•ˆè‡ªåˆ¶æŠ¥è¡¨")
        self.merged_analysis_dir = Path(r"D:\yingdao\tm\åˆå¹¶æ–‡ä»¶\å¤©çŒ«å®¢æœç»©æ•ˆè§£å†³åˆ†ææŠ¥è¡¨")
        
        # åˆ›å»ºæ‰€æœ‰å¿…è¦ç›®å½•
        for directory in [self.base_report_dir, self.base_analysis_dir, 
                         self.merged_report_dir, self.merged_analysis_dir]:
            os.makedirs(directory, exist_ok=True)
        
        # ä½¿ç”¨T-4æ—¥æœŸï¼ˆ4å¤©å‰ï¼‰
        self.target_date = (datetime.now() - timedelta(days=4)).strftime('%Y%m%d')
        self.target_date_str = (datetime.now() - timedelta(days=4)).strftime('%Y-%m-%d')
        
        # MinIOé…ç½®
        self.minio_api_url = "http://127.0.0.1:8009/api/upload"
        
        print(f"ç›®æ ‡æ—¥æœŸ: {self.target_date} ({self.target_date_str})")
        
    def generate_daily_tasks(self):
        """ç”Ÿæˆå½“æ—¥ä»»åŠ¡"""
        print("\n=== ç”Ÿæˆå½“æ—¥ä»»åŠ¡ ===")
        
        try:
            # å®šä¹‰ä»»åŠ¡åˆ—
            task_columns = ['kpi_self_status', 'kpi_offical_status']
            
            # ç”Ÿæˆä»»åŠ¡
            created_count = self.db_interface.generate_tasks(self.target_date_str, task_columns)
            print(f"âœ“ æˆåŠŸç”Ÿæˆ {created_count} ä¸ªä»»åŠ¡")
            
            return True
        except Exception as e:
            print(f"âœ— ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
            return False
    
    def get_shops_with_tasks(self, task_type):
        """è·å–æœ‰æŒ‡å®šä»»åŠ¡ç±»å‹çš„åº—é“ºä¿¡æ¯"""
        try:
            # è·å–å¾…å¤„ç†ä»»åŠ¡
            pending_tasks = self.db_interface.get_pending_tasks(self.target_date_str, task_type)
            
            if not pending_tasks:
                print(f"æ²¡æœ‰æ‰¾åˆ° {task_type} ç±»å‹çš„å¾…å¤„ç†ä»»åŠ¡")
                return []
            
            # è·å–åº—é“ºè¯¦ç»†ä¿¡æ¯
            shops_info = []
            for task in pending_tasks:
                # taskæ˜¯tupleæ ¼å¼ï¼Œæ ¹æ®JOINæŸ¥è¯¢çš„å­—æ®µé¡ºåºï¼š
                # ç´¢å¼•0: time_period, ç´¢å¼•1: shop_name, ç´¢å¼•2-15: åº—é“ºè¡¨å­—æ®µ
                # ç´¢å¼•8: sycmcookie, ç´¢å¼•13: reportTemplateId
                shop_name = task[1]  # dt.shop_name
                sycmcookie = task[8] if len(task) > 8 else None  # s.sycmcookie
                report_template_id = task[13] if len(task) > 13 else None  # s.reportTemplateId
                
                if sycmcookie:
                    shops_info.append({
                        'shop_name': shop_name,
                        'sycmcookie': sycmcookie,
                        'reportTemplateId': report_template_id,
                        'task_id': None  # tupleä¸­æ²¡æœ‰task_idå­—æ®µ
                    })
                    print(f"âœ“ åº—é“º {shop_name}, reportTemplateId: {report_template_id}")
                else:
                    print(f"âš ï¸ åº—é“º {shop_name} ç¼ºå°‘cookieä¿¡æ¯ï¼Œè·³è¿‡")
            
            print(f"âœ“ æ‰¾åˆ° {len(shops_info)} ä¸ªæœ‰æ•ˆåº—é“º")
            return shops_info
            
        except Exception as e:
            print(f"âœ— è·å–åº—é“ºä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def get_custom_report_data_for_shop(self, shop_name, cookies, report_template_id=None):
        """ä¸ºå•ä¸ªåº—é“ºè·å–è‡ªåˆ¶æŠ¥è¡¨æ•°æ®"""
        print(f"\n--- è·å–åº—é“º {shop_name} çš„è‡ªåˆ¶æŠ¥è¡¨æ•°æ® ---")
        
        # ä½¿ç”¨åº—é“ºç‰¹å®šçš„reportTemplateIdï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        template_id = report_template_id or "7798"
        print(f"ä½¿ç”¨reportTemplateId: {template_id}")
        
        # æ­¥éª¤1ï¼šå‘é€è·å–æ•°æ®è¯·æ±‚
        print("æ­¥éª¤1: å‘é€è·å–æ•°æ®è¯·æ±‚...")
        
        url = f"https://sycm.taobao.com/csp/api/user/customize/async-excel?startDate={self.target_date}&endDate={self.target_date}&dateType=day&dateRange=cz&reportTemplateId={template_id}&bizCode=selfMadeReport"
        
        headers = {
            "accept": "application/json, text/plain, */*",
            "accept-language": "zh-CN,zh;q=0.9",
            "bx-v": "2.5.31",
            "Cookie": cookies,
            "priority": "u=1, i",
            "referer": "https://sycm.taobao.com/qos/service/self_made_report",
            "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36"
        }
        
        try:
            response = requests.get(url, headers=headers)
            print(f"è¯·æ±‚çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    task_id = data.get('data')
                    print(f"âœ“ è·å–åˆ°ä»»åŠ¡ID: {task_id}")
                    
                    if task_id:
                        # æ­¥éª¤2ï¼šå¾ªç¯æ£€æŸ¥ä¸‹è½½çŠ¶æ€
                        return self._check_download_status_for_shop(task_id, cookies, shop_name, 'report')
                    else:
                        print("âœ— æœªè·å–åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID")
                        return False
                except json.JSONDecodeError as e:
                    print(f"âœ— JSONè§£æå¤±è´¥: {e}")
                    return False
            else:
                print(f"âœ— è¯·æ±‚å¤±è´¥: {response.text}")
                return False
                
        except Exception as e:
            print(f"âœ— å‘é€è¯·æ±‚æ—¶å‡ºé”™: {e}")
            return False
    
    def get_aftersale_analysis_data_for_shop(self, shop_name, cookies):
        """ä¸ºå•ä¸ªåº—é“ºè·å–å”®åè§£å†³åˆ†ææ•°æ®"""
        print(f"\n--- è·å–åº—é“º {shop_name} çš„å”®åè§£å†³åˆ†ææ•°æ® ---")
        
        url = f"https://sycm.taobao.com/csp/api/aftsale/cst/list.json?dateRange=cz&endDate={self.target_date}&excludeDates=&orderBy=aftSaleRplyUv&pageSize=10&wwGroup=&accountId=&qnGroupId=&dateType=day&page=1&startDate={self.target_date}&order=desc"
        
        headers = {
            "accept": "*/*",
            "accept-language": "zh-CN,zh;q=0.9",
            "bx-v": "2.5.31",
            "Cookie": cookies,
            "priority": "u=1, i",
            "referer": "https://sycm.taobao.com/qos/service/frame/customer/performance/new",
            "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "sycm-referer": "/qos/service/frame/customer/performance/new",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36"
        }
        
        try:
            print("å‘é€å”®åè§£å†³åˆ†ææ•°æ®è¯·æ±‚...")
            response = requests.get(url, headers=headers)
            print(f"è¯·æ±‚çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                print(f"âœ“ è·å–åˆ°å“åº”æ•°æ®")
                
                # è§£ææ•°æ®
                return self._parse_and_save_analysis_data_for_shop(data, shop_name)
            else:
                print(f"âœ— è¯·æ±‚å¤±è´¥: {response.text}")
                return False
                
        except Exception as e:
            print(f"âœ— è·å–å”®åè§£å†³åˆ†ææ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def _check_download_status_for_shop(self, task_id, cookies, shop_name, data_type):
        """æ£€æŸ¥ä¸‹è½½çŠ¶æ€å¹¶ä¸‹è½½æ–‡ä»¶ï¼ˆé’ˆå¯¹å•ä¸ªåº—é“ºï¼‰"""
        print("æ­¥éª¤2: å¾ªç¯æ£€æŸ¥ä¸‹è½½çŠ¶æ€...")
        
        status_url = "https://sycm.taobao.com/csp/api/file/task-list.json?pageNo=1&pageSize=10&bizCode=selfMadeReport"
        
        headers = {
            "accept": "application/json, text/plain, */*",
            "accept-language": "zh-CN,zh;q=0.9",
            "bx-v": "2.5.31",
            "Cookie": cookies,
            "priority": "u=1, i",
            "referer": "https://sycm.taobao.com/qos/service/self_made_report",
            "sec-ch-ua": '"Not=A?Brand";v="24", "Chromium";v="140"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36"
        }
        
        max_attempts = 60
        attempt = 0
        
        while attempt < max_attempts:
            try:
                print(f"ç¬¬{attempt + 1}æ¬¡æ£€æŸ¥ä¸‹è½½çŠ¶æ€...")
                
                response = requests.get(status_url, headers=headers)
                if response.status_code == 200:
                    data = response.json()
                    
                    if data.get('code') == 200 and data.get('data', {}).get('result'):
                        results = data['data']['result']
                        
                        # æŸ¥æ‰¾å¯¹åº”çš„ä»»åŠ¡
                        for result in results:
                            if result.get('id') == task_id:
                                message = result.get('message', '')
                                print(f"ä»»åŠ¡çŠ¶æ€: {message}")
                                
                                if message == "ä¸‹è½½å®Œæˆ":
                                    print("âœ“ ä¸‹è½½å®Œæˆï¼Œè·å–ä¸‹è½½é“¾æ¥...")
                                    return self._download_report_file_for_shop(task_id, cookies, shop_name, data_type)
                                
                        print("ä»»åŠ¡è¿˜åœ¨å¤„ç†ä¸­ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
                        time.sleep(5)
                        attempt += 1
                    else:
                        print("æœªæ‰¾åˆ°ä»»åŠ¡ä¿¡æ¯ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
                        time.sleep(5)
                        attempt += 1
                else:
                    print(f"æ£€æŸ¥çŠ¶æ€å¤±è´¥: {response.status_code}")
                    time.sleep(5)
                    attempt += 1
                    
            except Exception as e:
                print(f"æ£€æŸ¥çŠ¶æ€æ—¶å‡ºé”™: {e}")
                time.sleep(5)
                attempt += 1
        
        print("âœ— è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œä¸‹è½½å¤±è´¥")
        return False
    
    def _download_report_file_for_shop(self, task_id, cookies, shop_name, data_type):
        """ä¸‹è½½æŠ¥è¡¨æ–‡ä»¶ï¼ˆé’ˆå¯¹å•ä¸ªåº—é“ºï¼‰"""
        print("æ­¥éª¤3: è·å–ä¸‹è½½é“¾æ¥å¹¶ä¸‹è½½æ–‡ä»¶...")
        
        download_url = f"https://sycm.taobao.com/csp/api/file/url?id={task_id}"
        
        headers = {
            "accept": "application/json, text/plain, */*",
            "accept-language": "zh-CN,zh;q=0.9",
            "bx-v": "2.5.31",
            "Cookie": cookies,
            "priority": "u=1, i",
            "referer": "https://sycm.taobao.com/qos/service/self_made_report",
            "sec-ch-ua": '"Chromium";v="140", "Not=A?Brand";v="24", "Google Chrome";v="140"',
            "sec-ch-ua-mobile": "?0",
            "sec-ch-ua-platform": '"Windows"',
            "sec-fetch-dest": "empty",
            "sec-fetch-mode": "cors",
            "sec-fetch-site": "same-origin",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36"
        }
        
        try:
            response = requests.get(download_url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                file_url = data.get('data')
                
                if file_url:
                    print(f"âœ“ è·å–åˆ°ä¸‹è½½é“¾æ¥: {file_url}")
                    
                    # åˆ›å»ºæ—¥æœŸç›®å½•
                    date_dir = self.base_report_dir / self.target_date_str
                    date_dir.mkdir(parents=True, exist_ok=True)
                    
                    # ä¸‹è½½æ–‡ä»¶ï¼Œä»¥åº—é“ºåç§°å‘½å
                    filename = f"{shop_name}.xlsx"
                    file_path = date_dir / filename
                    
                    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                    if file_path.exists():
                        file_path.unlink()
                        print(f"åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
                    
                    # ä¸‹è½½æ–‡ä»¶
                    file_response = requests.get(file_url, stream=True)
                    file_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        for chunk in file_response.iter_content(chunk_size=8192):
                            f.write(chunk)
                    
                    print(f"âœ“ æ–‡ä»¶ä¸‹è½½å®Œæˆ: {file_path}")
                    return str(file_path)
                else:
                    print("âœ— æœªè·å–åˆ°æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥")
                    return False
            else:
                print(f"âœ— è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âœ— ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
    
    def _parse_and_save_analysis_data_for_shop(self, data, shop_name):
        """è§£æå¹¶ä¿å­˜å”®åè§£å†³åˆ†ææ•°æ®ï¼ˆé’ˆå¯¹å•ä¸ªåº—é“ºï¼‰"""
        print("è§£æå¹¶ä¿å­˜å”®åè§£å†³åˆ†ææ•°æ®...")
        
        try:
            if data.get('code') == 0 and data.get('data'):
                analysis_data = data['data']
                
                # æå–æ•°æ®
                records = []
                
                # æ·»åŠ æ±‡æ€»æ•°æ®
                if 'sumResult' in analysis_data:
                    sum_result = analysis_data['sumResult']
                    records.append({
                        'åº—é“ºåç§°': shop_name,
                        'å®¢æœå§“å': 'æ±‡æ€»',
                        'å”®åå›å¤UV': sum_result.get('aftSaleRplyUv', {}).get('value', 0),
                        'é¦–æ¬¡æœªè§£å†³UV': sum_result.get('fstUnsolvUv', {}).get('value', 0),
                        '72å°æ—¶è§£å†³ç‡': '',
                        'ç»Ÿè®¡æ—¥æœŸ': self.target_date_str
                    })
                
                # æ·»åŠ å¹³å‡æ•°æ®
                if 'avgResult' in analysis_data:
                    avg_result = analysis_data['avgResult']
                    records.append({
                        'åº—é“ºåç§°': shop_name,
                        'å®¢æœå§“å': 'å¹³å‡å€¼',
                        'å”®åå›å¤UV': avg_result.get('aftSaleRplyUv', {}).get('value', 0),
                        'é¦–æ¬¡æœªè§£å†³UV': avg_result.get('fstUnsolvUv', {}).get('value', 0),
                        '72å°æ—¶è§£å†³ç‡': avg_result.get('fcr72Rate', {}).get('value', 0),
                        'ç»Ÿè®¡æ—¥æœŸ': self.target_date_str
                    })
                
                # æ·»åŠ è¯¦ç»†æ•°æ®
                if 'data' in analysis_data:
                    for item in analysis_data['data']:
                        records.append({
                            'åº—é“ºåç§°': shop_name,
                            'å®¢æœå§“å': item.get('psnNickName', {}).get('value', ''),
                            'å”®åå›å¤UV': item.get('aftSaleRplyUv', {}).get('value', 0),
                            'é¦–æ¬¡æœªè§£å†³UV': item.get('fstUnsolvUv', {}).get('value', 0),
                            '72å°æ—¶è§£å†³ç‡': item.get('fcr72Rate', {}).get('value', 0),
                            'ç»Ÿè®¡æ—¥æœŸ': self.target_date_str
                        })
                
                # åˆ›å»ºDataFrame
                df = pd.DataFrame(records)
                
                # åˆ›å»ºæ—¥æœŸç›®å½•
                date_dir = self.base_analysis_dir / self.target_date_str
                date_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¿å­˜ä¸ºExcelæ–‡ä»¶ï¼Œä»¥åº—é“ºåç§°å‘½å
                filename = f"{shop_name}.xlsx"
                file_path = date_dir / filename
                
                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if file_path.exists():
                    file_path.unlink()
                    print(f"åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶: {file_path}")
                
                df.to_excel(file_path, index=False, engine='openpyxl')
                print(f"âœ“ å”®åè§£å†³åˆ†ææ•°æ®å·²ä¿å­˜: {file_path}")
                print(f"âœ“ å…±ä¿å­˜ {len(records)} æ¡è®°å½•")
                
                return str(file_path)
            else:
                print("âœ— å“åº”æ•°æ®æ ¼å¼å¼‚å¸¸")
                return False
                
        except Exception as e:
            print(f"âœ— è§£æå’Œä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
            return False
    
    def merge_and_upload_files(self, file_type):
        """åˆå¹¶æ–‡ä»¶å¹¶ä¸Šä¼ åˆ°MinIO"""
        print(f"\n=== åˆå¹¶{file_type}æ–‡ä»¶å¹¶ä¸Šä¼  ===")
        
        try:
            # ç¡®å®šæºç›®å½•å’Œç›®æ ‡ç›®å½•
            if file_type == "è‡ªåˆ¶æŠ¥è¡¨":
                source_dir = self.base_report_dir / self.target_date_str
                target_dir = self.merged_report_dir
                minio_path = "warehouse/ods/tm/tm_self_kpi"
                dremio_table = 'minio.warehouse.ods.tm."tm_self_kpi"'
            else:  # å”®åè§£å†³åˆ†æ
                source_dir = self.base_analysis_dir / self.target_date_str
                target_dir = self.merged_analysis_dir
                minio_path = "warehouse/ods/tm/tm_offical_kpi"
                dremio_table = 'minio.warehouse.ods.tm."tm_offical_kpi"'
            
            if not source_dir.exists() or not any(source_dir.glob("*.xlsx")):
                print(f"âš ï¸ æºç›®å½•ä¸å­˜åœ¨æˆ–æ²¡æœ‰Excelæ–‡ä»¶: {source_dir}")
                return False
            
            # ä½¿ç”¨ExcelMergeråˆå¹¶æ–‡ä»¶
            print(f"ğŸ”„ æ­£åœ¨åˆå¹¶{file_type}æ–‡ä»¶...")
            merger = ExcelMerger(str(source_dir))
            merge_filename = f"{self.target_date_str}.xlsx"
            merge_success = merger.merge_excel_files(merge_filename)
            
            if merge_success:
                # ç§»åŠ¨åˆå¹¶åçš„æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•
                source_merged_file = source_dir / merge_filename
                target_merged_file = target_dir / merge_filename
                
                if source_merged_file.exists():
                    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                    if target_merged_file.exists():
                        target_merged_file.unlink()
                    
                    shutil.move(str(source_merged_file), str(target_merged_file))
                    print(f"âœ“ åˆå¹¶æ–‡ä»¶å·²ç§»åŠ¨åˆ°: {target_merged_file}")
                    
                    # ä¸Šä¼ åˆ°MinIO
                    success = self.upload_to_minio(str(target_merged_file), minio_path)
                    if success:
                        print(f"âœ“ {file_type}æ–‡ä»¶ä¸Šä¼ MinIOæˆåŠŸ")
                        
                        # åˆ·æ–°Dremio
                        self.refresh_dremio_table(dremio_table)
                        return True
                    else:
                        print(f"âœ— {file_type}æ–‡ä»¶ä¸Šä¼ MinIOå¤±è´¥")
                        return False
                else:
                    print(f"âœ— åˆå¹¶æ–‡ä»¶ä¸å­˜åœ¨: {source_merged_file}")
                    return False
            else:
                print(f"âœ— {file_type}æ–‡ä»¶åˆå¹¶å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âœ— åˆå¹¶å’Œä¸Šä¼ {file_type}æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return False
    
    def upload_to_minio(self, file_path, minio_path):
        """
        å°†Excelæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°MinIO
        ä¸jd_store.pyçš„upload_merged_file_to_minioæ–¹æ³•ä¿æŒä¸€è‡´
        """
        try:
            # è¯»å–Excelæ–‡ä»¶
            df = pd.read_excel(file_path)
            
            # å¤„ç†NaNå€¼ï¼Œç¡®ä¿æ•°æ®èƒ½å¤Ÿæ­£å¸¸åºåˆ—åŒ–
            df = df.fillna('')  # å°†NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
            
            # å¤„ç†æ— ç©·å¤§å€¼
            df = df.replace([float('inf'), float('-inf')], '')
            
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½æ­£å¸¸åºåˆ—åŒ–
            for col in df.columns:
                if df[col].dtype in ['float64', 'float32']:
                    df[col] = df[col].replace([float('inf'), float('-inf')], '')
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä»¥é¿å…åºåˆ—åŒ–é—®é¢˜
                df[col] = df[col].astype(str)
            
            # å‡†å¤‡ä¸Šä¼ æ•°æ®
            upload_data = {
                "data": df.to_dict('records'),  # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                "target_path": minio_path,
                "format": "parquet",
                "bucket": "warehouse"
            }
            
            # å‘é€POSTè¯·æ±‚åˆ°MinIO API
            headers = {'Content-Type': 'application/json'}
            response = requests.post(self.minio_api_url, json=upload_data, headers=headers)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    print(f"âœ“ æˆåŠŸä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO: {minio_path}")
                    return True
                else:
                    print(f"âœ— MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return False
            else:
                print(f"âœ— MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                return False
                    
        except Exception as e:
            print(f"âœ— ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIOæ—¶å‡ºé”™: {str(e)}")
            return False
    
    def refresh_dremio_table(self, table_name):
        """åˆ·æ–°Dremioè¡¨"""
        try:
            # åˆ·æ–°æ•°æ®é›†
            print(f'ğŸ”„ æ­£åœ¨åˆ·æ–°æ•°æ®é›†...')
            refresh_dataset_response = requests.post(
                "http://localhost:8003/api/dataset/refresh-metadata",
                headers={"Content-Type": "application/json"},
                json={"dataset_path": table_name}
            )
            if refresh_dataset_response.status_code == 200:
                print(f'âœ“ æ•°æ®é›†åˆ·æ–°æˆåŠŸ: {table_name}')
            else:
                print(f'âš ï¸ æ•°æ®é›†åˆ·æ–°å¤±è´¥: {refresh_dataset_response.status_code}')
            
            # åˆ·æ–°åå°„
            print(f'ğŸ”„ æ­£åœ¨åˆ·æ–°åå°„...')
            refresh_reflection_response = requests.post(
                "http://localhost:8003/api/reflection/refresh",
                headers={"Content-Type": "application/json"},
                json={"dataset_path": table_name}
            )
            if refresh_reflection_response.status_code == 200:
                print(f'âœ“ åå°„åˆ·æ–°æˆåŠŸ: {table_name}')
            else:
                print(f'âš ï¸ åå°„åˆ·æ–°å¤±è´¥: {refresh_reflection_response.status_code}')
            
            return True
        except Exception as e:
            print(f"âœ— åˆ·æ–°Dremioè¡¨å¤±è´¥: {e}")
            return False
    
    def update_task_status(self, shop_name, task_type, status="å·²å®Œæˆ"):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            success = self.db_interface.update_task_status(
                self.target_date_str, 
                shop_name, 
                task_type, 
                status
            )
            if success:
                print(f"âœ“ ä»»åŠ¡çŠ¶æ€æ›´æ–°æˆåŠŸ: {shop_name} - {task_type} -> {status}")
            else:
                print(f"âœ— ä»»åŠ¡çŠ¶æ€æ›´æ–°å¤±è´¥: {shop_name} - {task_type}")
            return success
        except Exception as e:
            print(f"âœ— æ›´æ–°ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return False
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        print("=== å¤©çŒ«KPIæ•°æ®è·å–å·¥å…· - å‡çº§ç‰ˆ ===")
        print(f"ç›®æ ‡æ—¥æœŸ: {self.target_date_str}")
        
        # 1. ç”Ÿæˆå½“æ—¥ä»»åŠ¡
        if not self.generate_daily_tasks():
            print("âœ— ä»»åŠ¡ç”Ÿæˆå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return False
        
        success_count = 0
        total_tasks = 0
        
        # 2. å¤„ç†è‡ªåˆ¶æŠ¥è¡¨ä»»åŠ¡
        print("\n=== å¤„ç†è‡ªåˆ¶æŠ¥è¡¨ä»»åŠ¡ ===")
        self_report_shops = self.get_shops_with_tasks('kpi_self_status')
        
        if self_report_shops:
            total_tasks += len(self_report_shops)
            downloaded_files = []
            
            for shop_info in self_report_shops:
                shop_name = shop_info['shop_name']
                cookies = shop_info['sycmcookie']
                report_template_id = shop_info.get('reportTemplateId')
                task_id = shop_info.get('task_id')
                
                print(f"\nå¤„ç†åº—é“º: {shop_name}")
                
                # è·å–è‡ªåˆ¶æŠ¥è¡¨æ•°æ®
                file_path = self.get_custom_report_data_for_shop(shop_name, cookies, report_template_id)
                
                if file_path:
                    downloaded_files.append(file_path)
                    success_count += 1
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    self.update_task_status(shop_name, 'kpi_self_status')
                else:
                    print(f"âœ— åº—é“º {shop_name} è‡ªåˆ¶æŠ¥è¡¨æ•°æ®è·å–å¤±è´¥")
            
            # åˆå¹¶è‡ªåˆ¶æŠ¥è¡¨æ–‡ä»¶
            if downloaded_files:
                self.merge_and_upload_files("è‡ªåˆ¶æŠ¥è¡¨")
        
        # 3. å¤„ç†å”®åè§£å†³åˆ†æä»»åŠ¡
        print("\n=== å¤„ç†å”®åè§£å†³åˆ†æä»»åŠ¡ ===")
        analysis_shops = self.get_shops_with_tasks('kpi_offical_status')
        
        if analysis_shops:
            total_tasks += len(analysis_shops)
            downloaded_files = []
            
            for shop_info in analysis_shops:
                shop_name = shop_info['shop_name']
                cookies = shop_info['sycmcookie']
                task_id = shop_info.get('task_id')
                
                print(f"\nå¤„ç†åº—é“º: {shop_name}")
                
                # è·å–å”®åè§£å†³åˆ†ææ•°æ®
                file_path = self.get_aftersale_analysis_data_for_shop(shop_name, cookies)
                
                if file_path:
                    downloaded_files.append(file_path)
                    success_count += 1
                    
                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    self.update_task_status(shop_name, 'kpi_offical_status')
                else:
                    print(f"âœ— åº—é“º {shop_name} å”®åè§£å†³åˆ†ææ•°æ®è·å–å¤±è´¥")
            
            # åˆå¹¶å”®åè§£å†³åˆ†ææ–‡ä»¶
            if downloaded_files:
                self.merge_and_upload_files("å”®åè§£å†³åˆ†æ")
        
        print(f"\n=== æ‰§è¡Œå®Œæˆ ===")
        print(f"æˆåŠŸæ‰§è¡Œ: {success_count}/{total_tasks} ä¸ªä»»åŠ¡")
        
        return success_count > 0

def main():
    """ä¸»å‡½æ•°"""
    collector = TmallKpiCollector()
    success = collector.run()
    
    if success:
        print("\nâœ“ ç¨‹åºæ‰§è¡ŒæˆåŠŸï¼")
    else:
        print("\nâœ— ç¨‹åºæ‰§è¡Œå¤±è´¥ï¼")

if __name__ == "__main__":
    main()