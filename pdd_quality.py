# -*- coding: utf-8 -*-
import json
import os
import requests
import pandas as pd
import time
from pathlib import Path
from tqdm import tqdm
import shutil
from datetime import datetime, timedelta
import sys

# é…ç½®UTF-8ç¼–ç 
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ merge_excel_filesæ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
from merge_excel_files import ExcelMerger

# é…ç½®
EXCEL_PATH = r'D:\pdd\æ‹¼å¤šå¤šåº—é“ºæ±‡æ€»è¡¨\æ‹¼å¤šå¤šåº—é“ºæ±‡æ€»è¡¨.xlsx'
EXCEL_BACKUP_PATH = r'D:\pdd\æ‹¼å¤šå¤šåº—é“ºæ±‡æ€»è¡¨\æ‹¼å¤šå¤šåº—é“ºæ±‡æ€»è¡¨.xlsx'
BASE_ARCHIVE_DIR = Path('D:/pdd/äº§å“è´¨é‡ä½“éªŒå­˜æ¡£')  # åŸºç¡€å­˜æ¡£ç›®å½•
MERGED_FILES_DIR = Path('D:/pdd/åˆå¹¶æ–‡ä»¶/äº§å“è´¨é‡ä½“éªŒå­˜æ¡£')  # åˆå¹¶æ–‡ä»¶å­˜å‚¨ç›®å½•
SHEET = 0

# MinIO APIé…ç½®
MINIO_API_URL = "http://127.0.0.1:8009/api/upload"
MINIO_BUCKET = "warehouse"

# åˆ›å»ºåŸºç¡€å­˜æ¡£ç›®å½•å’Œåˆå¹¶æ–‡ä»¶ç›®å½•
os.makedirs(BASE_ARCHIVE_DIR, exist_ok=True)
os.makedirs(MERGED_FILES_DIR, exist_ok=True)

TODAY_STR = datetime.now().strftime('%Y-%m-%d')

# äº§å“è´¨é‡æ•°æ®è¡¨å¤´å’Œå¯¹åº”å­—æ®µ
QUALITY_HEADERS = ['å•†å“id', 'å•†å“åç§°', 'å•†å“ä¸»å›¾é“¾æ¥', 'å•†å“è´¨é‡ä½“éªŒæ’å', 'è¿‘30å¤©å¼‚å¸¸è®¢å•æ•°', 
                   'å¼‚å¸¸è®¢å•å æ¯”', 'æƒç›ŠçŠ¶æ€', 'å•†å“è´¨é‡ç­‰çº§', 'è¿‘30å¤©å“è´¨æ±‚åŠ©å¹³å°ç‡', 
                   'è¿‘30å¤©å•†å“è¯„ä»·åˆ†æ’å', 'è€å®¢è®¢å•å æ¯”']

QUALITY_FIELDS = ['goods_id', 'goods_name', 'img_url', 'rank_percent', 'abnormal_order_num', 
                  'abnormal_order_ratio', 'right_status', 'quality_level', 'quality_help_rate_last30_days', 
                  'goods_rating_rank', 'repeat_purchase_ratio']

def update_header_once(df: pd.DataFrame):
    """å›ºå®šç”¨ K åˆ—ï¼ˆç´¢å¼• 10ï¼‰ä½œä¸ºçŠ¶æ€åˆ—ï¼Œæ¯å¤©ç¬¬ä¸€æ¬¡æŠŠè¡¨å¤´æ”¹æˆ ä»Šå¤©+äº§å“è´¨é‡ä¸‹è½½çŠ¶æ€"""
    new_status = f'{TODAY_STR}äº§å“è´¨é‡ä¸‹è½½çŠ¶æ€'
    old_header = df.columns[10]          # K åˆ—å½“å‰åå­—

    if old_header == new_status:
        return False                     # ä»Šå¤©å·²æ›´æ–°è¿‡ï¼Œè¿”å›Falseè¡¨ç¤ºæœªæ›´æ–°

    # æ”¹åï¼ˆåªæ”¹è¡¨å¤´ï¼Œæ•°æ®ä¸åŠ¨ï¼‰
    df.rename(columns={old_header: new_status}, inplace=True)
    # åªåœ¨æ¯å¤©ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ¸…ç©ºKåˆ—çš„æ‰€æœ‰æ•°æ®ï¼ˆé™¤äº†è¡¨å¤´ï¼‰
    # è¿™æ ·é¿å…äº†æ¯æ¬¡è¿è¡Œéƒ½æ¸…ç©ºçŠ¶æ€çš„é—®é¢˜
    df.iloc[:, 10] = ''
    
    # ç«‹å³å›å†™ Excel
    try:
        df.to_excel(EXCEL_PATH, index=False, engine='openpyxl')
        print(f"å·²æ›´æ–°è¡¨å¤´ä¸º: {new_status} å¹¶æ¸…ç©ºæ‰€æœ‰çŠ¶æ€")
        return True                      # è¿”å›Trueè¡¨ç¤ºå·²æ›´æ–°
    except PermissionError:
        print(f"æ— æ³•å†™å…¥Excelæ–‡ä»¶ï¼Œå¯èƒ½è¢«å…¶ä»–ç¨‹åºå ç”¨ã€‚ç»§ç»­ä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®...")
        return False


def fetch_quality_data(cookie: str):
    """
    è·å–äº§å“è´¨é‡ä½“éªŒæ•°æ®
    """
    url = 'https://mms.pinduoduo.com/api/price/mariana/quality_experience/goods_list'
    
    headers = {
        'accept': '*/*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'anti-content': '',
        'cache-control': '',
        'Content-Type': 'application/json',
        'Cookie': cookie,
        'etag': '',
        'origin': 'https://mms.pinduoduo.com',
        'priority': '',
        'referer': 'https://mms.pinduoduo.com/mms-marketing-mixin/quality-experience?msfrom=mms_globalsearch',
        'sec-ch-ua': '',
        'sec-ch-ua-mobile': '',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': '',
        'sec-fetch-mode': '',
        'sec-fetch-site': '',
        'user-agent': ''
    }

    # å…ˆè·å–ç¬¬ä¸€é¡µæ•°æ®ï¼Œç¡®å®šæ€»æ•°é‡
    body = {"sort_field": 1, "sort_type": "ASC", "page_size": 40, "page_no": 1}
    
    resp = requests.post(url, headers=headers, json=body, timeout=15)
    print(f'ã€å“åº”ã€‘{resp.text}')
    resp.raise_for_status()
    
    first_page_data = resp.json()
    if not first_page_data.get('success'):
        raise RuntimeError(f'APIè¯·æ±‚å¤±è´¥ï¼š{first_page_data.get("error_msg")}')
    
    result = first_page_data.get('result', {})
    total = result.get('total', 0)
    
    print(f'æ€»å…±æœ‰ {total} æ¡äº§å“è´¨é‡æ•°æ®')
    
    # è®¡ç®—éœ€è¦å¤šå°‘é¡µ
    page_size = 40
    total_pages = (total + page_size - 1) // page_size
    
    all_goods_list = []
    
    # è·å–æ‰€æœ‰é¡µé¢çš„æ•°æ®
    for page_no in range(1, total_pages + 1):
        print(f'æ­£åœ¨è·å–ç¬¬ {page_no}/{total_pages} é¡µæ•°æ®...')
        
        body = {"sort_field": 1, "sort_type": "ASC", "page_size": page_size, "page_no": page_no}
        
        resp = requests.post(url, headers=headers, json=body, timeout=15)
        resp.raise_for_status()
        
        page_data = resp.json()
        if page_data.get('success'):
            page_result = page_data.get('result', {})
            goods_list = page_result.get('goods_list', [])
            all_goods_list.extend(goods_list)
            print(f'ç¬¬ {page_no} é¡µè·å–åˆ° {len(goods_list)} æ¡æ•°æ®')
        else:
            print(f'ç¬¬ {page_no} é¡µè·å–å¤±è´¥ï¼š{page_data.get("error_msg")}')
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.15)
    
    return total, all_goods_list


def to_list(total, goods_list, cols):
    """
    è§£æå“åº”æ•°æ®ï¼Œè½¬æ¢ä¸ºçŸ©é˜µæ ¼å¼
    """
    matrix = [[row.get(c, '') for c in cols] for row in goods_list]
    return [total, matrix]


def save_quality_data_to_excel(shop_name: str, total: int, goods_list: list, date_str: str = None):
    """
    å°†äº§å“è´¨é‡æ•°æ®ä¿å­˜åˆ°Excelæ–‡ä»¶
    """
    if date_str is None:
        date_str = TODAY_STR
    
    # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
    date_dir = BASE_ARCHIVE_DIR / date_str
    date_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶ä¿å­˜è·¯å¾„
    file_name = f'{shop_name}äº§å“è´¨é‡æ•°æ®.xlsx'
    save_path = date_dir / file_name

    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
    if save_path.exists():
        print(f'å‘ç°åŒåæ–‡ä»¶ï¼Œæ­£åœ¨åˆ é™¤: {save_path}')
        save_path.unlink()
        print(f'åŒåæ–‡ä»¶å·²åˆ é™¤')

    # è½¬æ¢æ•°æ®ä¸ºDataFrameæ ¼å¼
    total_data, matrix = to_list(total, goods_list, QUALITY_FIELDS)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(matrix, columns=QUALITY_HEADERS)
    
    # ä¿å­˜åˆ°Excelæ–‡ä»¶
    df.to_excel(save_path, index=False, engine='openpyxl')
    
    print(f'äº§å“è´¨é‡æ•°æ®å·²ä¿å­˜: {save_path} (å…± {len(matrix)} æ¡æ•°æ®)')
    
    return save_path


def upload_merged_file_to_minio(merged_file_path: str, date_str: str = None) -> bool:
    """
    å°†åˆå¹¶åçš„Excelæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°MinIO
    """
    if date_str is None:
        date_str = TODAY_STR
    
    try:
        # è¯»å–åˆå¹¶åçš„Excelæ–‡ä»¶
        df = pd.read_excel(merged_file_path)
        
        # å¤„ç†NaNå€¼ï¼Œæ›¿æ¢ä¸ºNone
        df = df.replace({pd.NA: None})
        df = df.where(pd.notnull(df), None)
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_dict = df.to_dict(orient='records')
        
        # è¿›ä¸€æ­¥æ¸…ç†NaNå€¼
        cleaned_data = []
        for record in data_dict:
            cleaned_record = {}
            for key, value in record.items():
                if pd.isna(value) or value is pd.NA:
                    cleaned_record[key] = ""  # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²æ›¿ä»£NaN
                elif isinstance(value, float) and (value != value):  # æ£€æŸ¥NaN
                    cleaned_record[key] = ""
                else:
                    cleaned_record[key] = value
            cleaned_data.append(cleaned_record)
        
        # æ„å»ºMinIOè·¯å¾„ï¼šods/pdd/pdd_quality/dt=æ—¥æœŸ/merged_quality_data.parquet
        minio_path = f"ods/pdd/pdd_quality/dt={date_str}/merged_quality_data.parquet"
        
        # å‡†å¤‡ä¸Šä¼ æ•°æ®
        upload_data = {
            "data": cleaned_data,
            "target_path": minio_path,
            "format": "parquet",
            "bucket": MINIO_BUCKET
        }
        
        # å‘é€POSTè¯·æ±‚åˆ°MinIO API
        headers = {'Content-Type': 'application/json'}
        response = requests.post(MINIO_API_URL, json=upload_data, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print(f"æˆåŠŸä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO: {minio_path}")
                return True
            else:
                print(f"MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
        else:
            print(f"MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        print(f"ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIOæ—¶å‡ºé”™: {str(e)}")
        return False


def safe_read_excel():
    """å®‰å…¨è¯»å–Excelæ–‡ä»¶ï¼Œå¦‚æœä¸»æ–‡ä»¶æŸååˆ™ä½¿ç”¨å¤‡ä»½æ–‡ä»¶"""
    try:
        print(f"å°è¯•è¯»å–ä¸»Excelæ–‡ä»¶: {EXCEL_PATH}")
        df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)
        print("âœ… ä¸»Excelæ–‡ä»¶è¯»å–æˆåŠŸ")
        return df, EXCEL_PATH
    except Exception as e:
        print(f"âŒ ä¸»Excelæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        print(f"å°è¯•è¯»å–å¤‡ä»½Excelæ–‡ä»¶: {EXCEL_BACKUP_PATH}")
        try:
            df = pd.read_excel(EXCEL_BACKUP_PATH, sheet_name=SHEET)
            print("âœ… å¤‡ä»½Excelæ–‡ä»¶è¯»å–æˆåŠŸ")
            # å°†å¤‡ä»½æ–‡ä»¶å¤åˆ¶ä¸ºä¸»æ–‡ä»¶
            import shutil
            shutil.copy2(EXCEL_BACKUP_PATH, EXCEL_PATH)
            print("âœ… å·²å°†å¤‡ä»½æ–‡ä»¶å¤åˆ¶ä¸ºä¸»æ–‡ä»¶")
            return df, EXCEL_PATH
        except Exception as backup_e:
            print(f"âŒ å¤‡ä»½Excelæ–‡ä»¶ä¹Ÿè¯»å–å¤±è´¥: {backup_e}")
            raise Exception(f"ä¸»æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶éƒ½æ— æ³•è¯»å–: ä¸»æ–‡ä»¶é”™è¯¯={e}, å¤‡ä»½æ–‡ä»¶é”™è¯¯={backup_e}")


if __name__ == '__main__':
    # 1. å®‰å…¨è¯»è¡¨ & æ›´æ–°è¡¨å¤´
    df, used_file = safe_read_excel()
    header_updated = update_header_once(df)

    # 2. å®šä½å…³é”®åˆ—
    cookie_idx = 9    # Jåˆ— (cookie)
    status_idx = 10   # Kåˆ— (çŠ¶æ€åˆ—)
    shop_idx   = 1    # Båˆ— (åº—é“ºåç§°)

    print(f"DataFrameå½¢çŠ¶: {df.shape}")
    print(f"åˆ—å: {list(df.columns)}")
    print(f"çŠ¶æ€åˆ—å: {df.columns[status_idx]}")

    # 3. æ”¶é›†æ‰€æœ‰éœ€è¦æ‹‰å–æ•°æ®çš„åº—é“ºä¿¡æ¯
    download_tasks = []
    for row_idx in range(len(df)):
        shop   = df.iloc[row_idx, shop_idx]
        cookie = df.iloc[row_idx, cookie_idx]
        status = df.iloc[row_idx, status_idx]

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†
        status_str = str(status) if pd.notna(status) else ''
        
        print(f"æ£€æŸ¥ç¬¬{row_idx+2}è¡Œ: {shop}, çŠ¶æ€: '{status_str}'")

        # ç©ºå€¼/å·²å®Œæˆçš„è·³è¿‡ - ç”±äºæˆ‘ä»¬å·²ç»æ¸…ç©ºäº†çŠ¶æ€ï¼Œæ‰€ä»¥ä¸ä¼šæœ‰"å·²å®Œæˆ"
        if status_str.strip() == 'å·²å®Œæˆ':
            print(f'[è·³è¿‡] {shop}  çŠ¶æ€å·²å®Œæˆï¼Œè·³è¿‡')
            continue
        if pd.isna(cookie) or str(cookie).strip() == '' or str(cookie).strip() == 'nan':
            print(f'[è­¦å‘Š] {shop}  cookie ä¸ºç©ºï¼Œè·³è¿‡')
            continue
            
        download_tasks.append({
            'row_idx': row_idx,
            'shop': shop,
            'cookie': str(cookie)
        })
    
    print(f"å…±æ‰¾åˆ° {len(download_tasks)} ä¸ªéœ€è¦æ‹‰å–äº§å“è´¨é‡æ•°æ®çš„åº—é“º")
    
    # 4. æ‰¹é‡æ‹‰å–æ‰€æœ‰åº—é“ºçš„äº§å“è´¨é‡æ•°æ®
    saved_files = []
    for task in download_tasks:
        row_idx = task['row_idx']
        shop = task['shop']
        cookie = task['cookie']
        
        try:
            total, goods_list = fetch_quality_data(cookie)
            save_path = save_quality_data_to_excel(shop, total, goods_list)
            print(f'[æˆåŠŸ] {shop}  äº§å“è´¨é‡æ•°æ®æ‹‰å–å®Œæˆ: {save_path}')
            
            saved_files.append(save_path)
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
            df.iloc[row_idx, status_idx] = 'å·²å®Œæˆ'
        except Exception as e:
            # å¤±è´¥æ—¶ç½®ç©ºçŠ¶æ€ï¼Œä¾¿äºé‡è¯•
            df.iloc[row_idx, status_idx] = ''
            print(f'{shop} å¤±è´¥ï¼š{e}ï¼Œå·²ç½®ç©ºå¾…é‡è¯•')

    # 5. ä¿å­˜ExcelçŠ¶æ€æ›´æ–°
    df.to_excel(EXCEL_PATH, index=False, engine='openpyxl')
    print('ExcelçŠ¶æ€å·²æ›´æ–°ï¼')
    
    # 6. å¦‚æœæœ‰ä¿å­˜çš„æ–‡ä»¶ï¼Œè¿›è¡Œåˆå¹¶å’Œä¸Šä¼ 
    if saved_files:
        print(f'å¼€å§‹åˆå¹¶ {len(saved_files)} ä¸ªExcelæ–‡ä»¶...')
        
        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
        date_dir = BASE_ARCHIVE_DIR / TODAY_STR
        
        # ä½¿ç”¨ä¿®å¤åçš„ExcelMergeråˆå¹¶æ–‡ä»¶ï¼ŒæŒ‡å®šç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        merger = ExcelMerger(str(date_dir), output_dir=str(MERGED_FILES_DIR))
        merge_success = merger.merge_excel_files(f"{TODAY_STR}.xlsx")
        
        if merge_success:
            # åˆå¹¶åçš„æ–‡ä»¶ç›´æ¥åœ¨æœ€ç»ˆç›®å½•ä¸­
            final_merged_file_path = MERGED_FILES_DIR / f"{TODAY_STR}.xlsx"
            
            # è¯»å–åˆå¹¶åçš„æ–‡ä»¶è¿›è¡ŒéªŒè¯
            merged_df = pd.read_excel(final_merged_file_path)
            
            print(f'æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³: {final_merged_file_path}')
            print(f'ğŸ“Š åˆå¹¶æ–‡ä»¶ç»Ÿè®¡: {len(merged_df)} è¡Œ, {len(merged_df.columns)} åˆ—')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸæ ¼å¼çš„shop
            if 'shop' in merged_df.columns:
                date_shops = merged_df[merged_df['shop'].str.match(r'^\d{4}-\d{2}-\d{2}$', na=False)]
                if len(date_shops) > 0:
                    print(f'[è­¦å‘Š] è­¦å‘Š: å‘ç° {len(date_shops)} è¡Œæ—¥æœŸæ ¼å¼çš„shopæ•°æ®')
                else:
                    print(f'éªŒè¯é€šè¿‡: æ— æ—¥æœŸæ ¼å¼çš„shopæ•°æ®ï¼Œå…± {merged_df["shop"].nunique()} ä¸ªæœ‰æ•ˆåº—é“º')
            
            # ä¸Šä¼ åˆå¹¶åçš„æ–‡ä»¶åˆ°MinIO
            print(f'ğŸ”„ æ­£åœ¨ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO...')
            upload_success = upload_merged_file_to_minio(str(final_merged_file_path), TODAY_STR)
            
            if upload_success:
                print(f'[æˆåŠŸ] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ æˆåŠŸ')
            else:
                print(f'[è­¦å‘Š] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°æ–‡ä»¶å·²ä¿å­˜')
        else:
            print('[é”™è¯¯] æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåˆå¹¶æ–‡ä»¶')
else:
    print('[è­¦å‘Š] æ²¡æœ‰æ–°ä¿å­˜çš„æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶å’Œä¸Šä¼ æ­¥éª¤')

# 7. æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œåˆ·æ–°æ•°æ®é›†å’Œåå°„
print('æ­£åœ¨åˆ·æ–°æ•°æ®é›†...')
try:
    refresh_dataset_response = requests.post(
        "http://localhost:8003/api/dataset/refresh-metadata",
        headers={"Content-Type": "application/json"},
        json={"dataset_path": "minio.warehouse.ods.pdd.pdd_quality"}
    )
    if refresh_dataset_response.status_code == 200:
        print('æ•°æ®é›†åˆ·æ–°æˆåŠŸ')
    else:
        print(f'æ•°æ®é›†åˆ·æ–°å¤±è´¥: {refresh_dataset_response.status_code}')
except Exception as e:
    print(f'æ•°æ®é›†åˆ·æ–°å¼‚å¸¸: {e}')

print('æ­£åœ¨åˆ·æ–°åå°„...')
try:
    refresh_reflection_response = requests.post(
        "http://localhost:8003/api/reflection/refresh",
        headers={"Content-Type": "application/json"},
        json={"dataset_path": "minio.warehouse.ods.pdd.pdd_quality"}
    )
    if refresh_reflection_response.status_code == 200:
        print('åå°„åˆ·æ–°æˆåŠŸ')
    else:
        print(f'åå°„åˆ·æ–°å¤±è´¥: {refresh_reflection_response.status_code}')
except Exception as e:
    print(f'åå°„åˆ·æ–°å¼‚å¸¸: {e}')

print('ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼')