# -*- coding: utf-8 -*-
import json
import os
import requests
import pandas as pd
import time
from pathlib import Path
from tqdm import tqdm
import shutil
from datetime import datetime, timedelta
import sys
import math

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ merge_excel_filesæ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
from merge_excel_files import ExcelMerger

# é…ç½®
EXCEL_PATH = r'D:\pdd\æ‹¼å¤šå¤šåº—é“ºæ±‡æ€»è¡¨\æ‹¼å¤šå¤šåº—é“ºæ±‡æ€»è¡¨.xlsx'
BASE_ARCHIVE_DIR = Path('D:/pdd/è¯„ä»·æ–‡ä»¶å­˜æ¡£')  # åŸºç¡€å­˜æ¡£ç›®å½•
MERGED_FILES_DIR = Path('D:/pdd/åˆå¹¶æ–‡ä»¶/è¯„ä»·æ–‡ä»¶å­˜æ¡£')  # åˆå¹¶æ–‡ä»¶å­˜å‚¨ç›®å½•
SHEET = 0

# MinIO APIé…ç½®
MINIO_API_URL = "http://127.0.0.1:8009/api/upload"
MINIO_BUCKET = "warehouse"

# åˆ›å»ºåŸºç¡€å­˜æ¡£ç›®å½•å’Œåˆå¹¶æ–‡ä»¶ç›®å½•
os.makedirs(BASE_ARCHIVE_DIR, exist_ok=True)
os.makedirs(MERGED_FILES_DIR, exist_ok=True)

TODAY_STR = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

def update_header_once(df: pd.DataFrame):
    """å›ºå®šç”¨ N åˆ—ï¼ˆç´¢å¼• 13ï¼‰ä½œä¸ºçŠ¶æ€åˆ—ï¼Œæ¯å¤©ç¬¬ä¸€æ¬¡æŠŠè¡¨å¤´æ”¹æˆ ä»Šå¤©+ä¸‹è½½çŠ¶æ€"""
    new_status = f'{TODAY_STR}å·®è¯„ä¸‹è½½çŠ¶æ€'
    old_header = df.columns[13]         # N åˆ—å½“å‰åå­—

    if old_header == new_status:
        return False                     # ä»Šå¤©å·²æ›´æ–°è¿‡ï¼Œè¿”å›Falseè¡¨ç¤ºæœªæ›´æ–°

    # æ”¹åï¼ˆåªæ”¹è¡¨å¤´ï¼Œæ•°æ®ä¸åŠ¨ï¼‰
    df.rename(columns={old_header: new_status}, inplace=True)
    # åªåœ¨æ¯å¤©ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ¸…ç©ºNåˆ—çš„æ‰€æœ‰æ•°æ®ï¼ˆé™¤äº†è¡¨å¤´ï¼‰
    # è¿™æ ·é¿å…äº†æ¯æ¬¡è¿è¡Œéƒ½æ¸…ç©ºçŠ¶æ€çš„é—®é¢˜
    df.iloc[:, 13] = ''
    # ç«‹å³å›å†™ Excel
    df.to_excel(EXCEL_PATH, index=False, engine='openpyxl')
    return True                          # è¿”å›Trueè¡¨ç¤ºå·²æ›´æ–°


def fetch_reviews_data(cookie: str, shop_name: str):
    """
    è·å–æ‹¼å¤šå¤šå·®è¯„æ•°æ®
    """
    # æ˜¨å¤©çš„å¼€å§‹å’Œç»“æŸæ—¶é—´æˆ³ï¼ˆç§’çº§ï¼‰
    start_time = int(datetime.combine(datetime.now().date() - timedelta(days=1), datetime.min.time()).timestamp())
    end_time = int(datetime.combine(datetime.now().date() - timedelta(days=1), datetime.max.time()).timestamp())
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f'ğŸ” æŸ¥è¯¢æ—¥æœŸ: {TODAY_STR}')
    print(f'ğŸ” å¼€å§‹æ—¶é—´æˆ³: {start_time} ({datetime.fromtimestamp(start_time)})')
    print(f'ğŸ” ç»“æŸæ—¶é—´æˆ³: {end_time} ({datetime.fromtimestamp(end_time)})')
    
    url = 'https://mms.pinduoduo.com/saturn/reviews/list'
    
    headers = {
        'accept': '*/*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'anti-content': '',
        'cache-control': 'no-cache',
        'Content-Type': 'application/json',
        'Cookie': cookie,
        'etag': '',
        'origin': 'https://mms.pinduoduo.com',
        'priority': 'u=1, i',
        'referer': 'https://mms.pinduoduo.com/goods/evaluation/index?msfrom=mms_globalsearch',
        'sec-ch-ua': '"Chromium";v="130", "Google Chrome";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
    }
    
    # é¦–å…ˆè·å–ç¬¬ä¸€é¡µæ•°æ®ï¼Œç¡®å®šæ€»æ•°æ®é‡
    first_page_data = {
        "startTime": start_time,
        "endTime": end_time,
        "pageNo": 1,
        "pageSize": 40,
        "descScore": ["1", "2", "3"],
        "mainReviewContentStatus": "1",
        "orderSn": ""
    }
    
    print(f'ğŸ” æ­£åœ¨è·å– {shop_name} çš„å·®è¯„æ•°æ®...')
    
    try:
        response = requests.post(url, headers=headers, json=first_page_data, timeout=30)
        response.raise_for_status()
        
        # æ‰“å°å®Œæ•´çš„å“åº”å†…å®¹ç”¨äºè°ƒè¯•
        print(f'ğŸ” {shop_name} APIå“åº”çŠ¶æ€ç : {response.status_code}')
        print(f'ğŸ” {shop_name} APIå“åº”å†…å®¹: {response.text[:1000]}...')  # åªæ‰“å°å‰1000å­—ç¬¦
        
        result = response.json()
        print(f'ğŸ” {shop_name} è§£æåçš„JSON: {result}')
        
        if not result.get('success'):
            print(f'[é”™è¯¯] APIè¿”å›å¤±è´¥: {result}')
            raise RuntimeError(f'APIè¿”å›é”™è¯¯: {result.get("error_msg", "æœªçŸ¥é”™è¯¯")}')
        
        total_rows = result.get('result', {}).get('totalRows', 0)
        print(f'ğŸ“Š {shop_name} å…±æœ‰ {total_rows} æ¡å·®è¯„æ•°æ®')
        
        if total_rows == 0:
            return []
        
        # è®¡ç®—éœ€è¦çš„é¡µæ•°
        page_size = 40
        total_pages = math.ceil(total_rows / page_size)
        
        all_data = []
        
        # è·å–æ‰€æœ‰é¡µé¢çš„æ•°æ®
        for page_no in range(1, total_pages + 1):
            page_data = {
                "startTime": start_time,
                "endTime": end_time,
                "pageNo": page_no,
                "pageSize": page_size,
                "descScore": ["1", "2", "3"],
                "mainReviewContentStatus": "1",
                "orderSn": ""
            }
            
            print(f'ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page_no}/{total_pages} é¡µæ•°æ®...')
            
            page_response = requests.post(url, headers=headers, json=page_data, timeout=30)
            page_response.raise_for_status()
            
            print(f'ğŸ“„ ç¬¬ {page_no} é¡µå“åº”çŠ¶æ€ç : {page_response.status_code}')
            print(f'ğŸ“„ ç¬¬ {page_no} é¡µå“åº”å†…å®¹: {page_response.text[:500]}...')
            
            page_result = page_response.json()
            print(f'ğŸ“„ ç¬¬ {page_no} é¡µè§£æåçš„JSON: {page_result}')
            
            if page_result.get('success'):
                page_items = page_result.get('result', {}).get('data', [])
                print(f'ğŸ“„ ç¬¬ {page_no} é¡µè·å–åˆ° {len(page_items)} æ¡æ•°æ®')
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ŒæŸ¥çœ‹æ•°æ®ç»“æ„
                if page_items:
                    print(f'ğŸ“„ ç¬¬ä¸€æ¡æ•°æ®ç»“æ„: {page_items[0]}')
                    if 'orderSnapshotInfo' in page_items[0]:
                        print(f'ğŸ“„ orderSnapshotInfoå†…å®¹: {page_items[0]["orderSnapshotInfo"]}')
                
                all_data.extend(page_items)
            else:
                print(f'[è­¦å‘Š] ç¬¬ {page_no} é¡µè·å–å¤±è´¥: {page_result.get("error_msg", "æœªçŸ¥é”™è¯¯")}')
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        print(f'[æˆåŠŸ] {shop_name} æ•°æ®è·å–å®Œæˆï¼Œå…± {len(all_data)} æ¡è®°å½•')
        return all_data
    except Exception as e:
        print(f'[é”™è¯¯] {shop_name} æ•°æ®è·å–å¤±è´¥: {str(e)}')
        raise


def parse_reviews_data(data_list):
    """
    è§£æå·®è¯„æ•°æ®ï¼Œè½¬æ¢ä¸ºExcelæ ¼å¼
    """
    if not data_list:
        return []
    
    keys = [
        'descScore',           # ç”¨æˆ·è¯„ä»·åˆ†
        'comment',             # ç”¨æˆ·è¯„è®º
        'orderSn',             # è®¢å•ç¼–å·
        'name',                # å–å®¶æ˜µç§°ï¼ˆåœ¨ orderSnapshotInfo é‡Œï¼‰
        'goodsId',             # å•†å“ ID
        'goodsInfoUrl'         # é¡µè¿”å›é“¾æ¥ï¼ˆå•†å“é¡µï¼‰
    ]
    
    table = [keys]
    max_pictures = 0  # ç”¨äºè®°å½•æœ€å¤§å›¾ç‰‡æ•°é‡
    
    for item in data_list:
        row = []
        current_pictures = 0  # å½“å‰æ¡ç›®ä¸­çš„å›¾ç‰‡æ•°é‡
        
        # éå†æ¯ä¸ªå­—æ®µ
        for k in keys:
            if k == 'name':
                # nameå­—æ®µç›´æ¥åœ¨itemä¸­ï¼Œä¸åœ¨orderSnapshotInfoä¸­
                name_value = item.get('name', '')
                # ä¿®å¤ä»¥=å¼€å¤´çš„nameå€¼ï¼Œåœ¨å‰é¢åŠ ä¸Šå•å¼•å·é˜²æ­¢Excelè§£æä¸ºå…¬å¼
                if isinstance(name_value, str) and name_value.startswith('='):
                    name_value = "'" + name_value
                row.append(name_value)
            else:
                row.append(item.get(k, ''))
        
        # å¤„ç†å›¾ç‰‡é“¾æ¥ï¼Œæ¯ä¸ªå›¾ç‰‡å•ç‹¬ä¸€åˆ—
        pics = item.get('pictures', []) or []
        current_pictures = len(pics)
        if current_pictures > max_pictures:
            max_pictures = current_pictures
        for pic in pics:
            row.append(pic.get('url', ''))
        
        # å¦‚æœå½“å‰å›¾ç‰‡æ•°é‡å°‘äºæœ€å¤§å€¼ï¼Œå¡«å……ç©ºå­—ç¬¦ä¸²ä»¥å¯¹é½
        if current_pictures < max_pictures:
            row.extend([''] * (max_pictures - current_pictures))
        
        table.append(row)
    
    # æ›´æ–°è¡¨å¤´ä»¥åŒ…å«å›¾ç‰‡åˆ—
    table[0].extend([f'Picture_{i+1}' for i in range(max_pictures)])
    
    return table


def save_to_excel(table_data, shop_name, date_str=None):
    """
    å°†æ•°æ®ä¿å­˜åˆ°Excelæ–‡ä»¶
    """
    if date_str is None:
        date_str = TODAY_STR
    
    # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„ï¼šD:\pdd\è¯„ä»·æ–‡ä»¶å­˜æ¡£\2025-09-25\
    date_dir = BASE_ARCHIVE_DIR / date_str
    date_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼šD:\pdd\è¯„ä»·æ–‡ä»¶å­˜æ¡£\2025-09-25\åº—é“ºåç§°.xlsx
    file_name = f'{shop_name}.xlsx'
    save_path = date_dir / file_name
    
    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
    if save_path.exists():
        print(f'ğŸ—‘ï¸  å‘ç°åŒåæ–‡ä»¶ï¼Œæ­£åœ¨åˆ é™¤: {save_path}')
        save_path.unlink()  # åˆ é™¤æ–‡ä»¶
        print(f'[æˆåŠŸ] åŒåæ–‡ä»¶å·²åˆ é™¤')
    
    # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
    if table_data:
        df = pd.DataFrame(table_data[1:], columns=table_data[0])
        df.to_excel(save_path, index=False, engine='openpyxl')
        print(f'ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {save_path}')
        return save_path
    else:
        print(f'[è­¦å‘Š] {shop_name} æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜')
        return None


def upload_merged_file_to_minio(merged_file_path: str, date_str: str = None) -> bool:
    """
    å°†åˆå¹¶åçš„Excelæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°MinIO
    
    Args:
        merged_file_path: åˆå¹¶åçš„Excelæ–‡ä»¶è·¯å¾„
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä½¿ç”¨æ˜¨å¤©
    
    Returns:
        bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
    """
    if date_str is None:
        date_str = TODAY_STR
    
    try:
        # è¯»å–åˆå¹¶åçš„Excelæ–‡ä»¶
        df = pd.read_excel(merged_file_path)
        
        # æ¸…ç†NaNå€¼ï¼Œå°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²æˆ–None
        df = df.fillna('')  # å°†æ‰€æœ‰NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        
        # æ„å»ºMinIOè·¯å¾„ï¼šods/pdd/pdd_badscore/dt=æ—¥æœŸ/merged_badscore_data.parquet
        minio_path = f"ods/pdd/pdd_badscore/dt={date_str}/merged_badscore_data.parquet"
        
        # å‡†å¤‡ä¸Šä¼ æ•°æ®
        upload_data = {
            "data": df.to_dict('records'),  # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            "target_path": minio_path,
            "format": "parquet",
            "bucket": MINIO_BUCKET
        }
        
        # å‘é€POSTè¯·æ±‚åˆ°MinIO API
        headers = {'Content-Type': 'application/json'}
        response = requests.post(MINIO_API_URL, json=upload_data, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print(f"[æˆåŠŸ] æˆåŠŸä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO: {minio_path}")
                return True
            else:
                print(f"[é”™è¯¯] MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
        else:
            print(f"[é”™è¯¯] MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"[é”™è¯¯] ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIOæ—¶å‡ºé”™: {str(e)}")
        return False


if __name__ == '__main__':
    # 1. è¯»è¡¨ & æ›´æ–°è¡¨å¤´
    df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)  
    header_updated = update_header_once(df)
    
    # å¦‚æœè¡¨å¤´æ›´æ–°äº†ï¼Œéœ€è¦é‡æ–°è¯»å–DataFrame
    if header_updated:
        df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)

    # 2. å®šä½å…³é”®åˆ—
    cookie_idx = 9    # Jåˆ— (cookie)
    status_idx = 13   # Nåˆ— (çŠ¶æ€åˆ—)
    shop_idx   = 1    # Båˆ— (åº—é“ºåç§°)

    print(f"DataFrameå½¢çŠ¶: {df.shape}")
    print(f"åˆ—å: {list(df.columns)}")
    print(f"çŠ¶æ€åˆ—å: {df.columns[status_idx]}")

    # 3. æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„åº—é“ºä¿¡æ¯
    download_tasks = []
    for row_idx in range(len(df)):
        shop   = df.iloc[row_idx, shop_idx]
        cookie = df.iloc[row_idx, cookie_idx]
        status = df.iloc[row_idx, status_idx]

        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¤„ç†
        status_str = str(status) if pd.notna(status) else ''
        
        print(f"æ£€æŸ¥ç¬¬{row_idx+2}è¡Œ: {shop}, çŠ¶æ€: '{status_str}', cookie: '{str(cookie)[:50]}...'")

        # æ£€æŸ¥cookieæ˜¯å¦æœ‰æ•ˆ
        if pd.isna(cookie) or str(cookie).strip() == '' or str(cookie).strip() == 'nan':
            print(f'[è­¦å‘Š] {shop}  cookie ä¸ºç©ºï¼Œè·³è¿‡')
            continue
            
        # æ£€æŸ¥çŠ¶æ€ï¼šåªæœ‰ç©ºçŠ¶æ€æˆ–é"å·²å®Œæˆ"çŠ¶æ€æ‰éœ€è¦ä¸‹è½½
        if status_str.strip() == 'å·²å®Œæˆ':
            print(f'[è·³è¿‡] {shop}  çŠ¶æ€å·²å®Œæˆï¼Œè·³è¿‡')
            continue
        else:
            # ç©ºçŠ¶æ€æˆ–å…¶ä»–çŠ¶æ€éƒ½éœ€è¦ä¸‹è½½
            download_tasks.append({
                'row_idx': row_idx,
                'shop': shop,
                'cookie': str(cookie)
            })
    
    print(f"å…±æ‰¾åˆ° {len(download_tasks)} ä¸ªéœ€è¦ä¸‹è½½çš„åº—é“º")
    
    # 4. æ‰¹é‡è·å–æ‰€æœ‰åº—é“ºçš„å·®è¯„æ•°æ®
    downloaded_files = []
    for task in download_tasks:
        row_idx = task['row_idx']
        shop = task['shop']
        cookie = task['cookie']
        
        try:
            # è·å–å·®è¯„æ•°æ®
            reviews_data = fetch_reviews_data(cookie, shop)
            
            if reviews_data:
                # è§£ææ•°æ®
                table_data = parse_reviews_data(reviews_data)
                
                # ä¿å­˜åˆ°Excel
                save_path = save_to_excel(table_data, shop)
                if save_path:
                    downloaded_files.append(save_path)
                    print(f'[æˆåŠŸ] {shop}  å·®è¯„æ•°æ®å¤„ç†å®Œæˆ: {save_path}')
                else:
                    print(f'[è­¦å‘Š] {shop}  æ²¡æœ‰æ•°æ®ä¿å­˜')
            else:
                print(f'â„¹ï¸  {shop}  æ²¡æœ‰å·®è¯„æ•°æ®')
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
            df.iloc[row_idx, status_idx] = 'å·²å®Œæˆ'
            
        except Exception as e:
            # å¤±è´¥æ—¶ç½®ç©ºçŠ¶æ€ï¼Œä¾¿äºé‡è¯•
            df.iloc[row_idx, status_idx] = ''
            print(f'[é”™è¯¯] {shop}  å¤±è´¥ï¼š{e}ï¼Œå·²ç½®ç©ºå¾…é‡è¯•')

    # 5. ä¿å­˜ExcelçŠ¶æ€æ›´æ–°
    df.to_excel(EXCEL_PATH, index=False, engine='openpyxl')
    print('ExcelçŠ¶æ€å·²æ›´æ–°ï¼')
    
    # 6. å¦‚æœæœ‰ä¸‹è½½çš„æ–‡ä»¶ï¼Œè¿›è¡Œåˆå¹¶å’Œä¸Šä¼ 
    if downloaded_files:
        print(f'ğŸ”„ å¼€å§‹åˆå¹¶ {len(downloaded_files)} ä¸ªExcelæ–‡ä»¶...')
        
        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
        date_dir = BASE_ARCHIVE_DIR / TODAY_STR
        
        # ä½¿ç”¨ExcelMergeråˆå¹¶æ–‡ä»¶
        merger = ExcelMerger(str(date_dir))
        merge_success = merger.merge_excel_files(f"{TODAY_STR}.xlsx")
        
        if merge_success:
            # è¯»å–åˆå¹¶åçš„æ–‡ä»¶
            merged_file_path = date_dir / f"{TODAY_STR}.xlsx"
            merged_df = pd.read_excel(merged_file_path)
            
            # ç§»åŠ¨åˆ°æœ€ç»ˆç›®å½•
            final_merged_file_path = MERGED_FILES_DIR / f"{TODAY_STR}.xlsx"
            shutil.move(str(merged_file_path), str(final_merged_file_path))
            
            print(f'[æˆåŠŸ] æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³: {final_merged_file_path}')
            
            # ä¸Šä¼ åˆå¹¶åçš„æ–‡ä»¶åˆ°MinIO
            print(f'ğŸ”„ æ­£åœ¨ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO...')
            upload_success = upload_merged_file_to_minio(str(final_merged_file_path), TODAY_STR)
            
            if upload_success:
                print(f'[æˆåŠŸ] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ æˆåŠŸ')
            else:
                print(f'[è­¦å‘Š] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°æ–‡ä»¶å·²ä¿å­˜')
        else:
            print('[é”™è¯¯] æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåˆå¹¶æ–‡ä»¶')
    else:
        print('[è­¦å‘Š] æ²¡æœ‰æ–°ä¸‹è½½çš„æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶å’Œä¸Šä¼ æ­¥éª¤')
    
    # 7. æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œåˆ·æ–°æ•°æ®é›†å’Œåå°„
    print('ğŸ”„ æ­£åœ¨åˆ·æ–°æ•°æ®é›†...')
    try:
        refresh_dataset_response = requests.post(
            "http://localhost:8003/api/dataset/refresh-metadata",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_badscore"}
        )
        if refresh_dataset_response.status_code == 200:
            print('[æˆåŠŸ] æ•°æ®é›†åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] æ•°æ®é›†åˆ·æ–°å¤±è´¥: {refresh_dataset_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] æ•°æ®é›†åˆ·æ–°å¼‚å¸¸: {e}')
    
    print('ğŸ”„ æ­£åœ¨åˆ·æ–°åå°„...')
    try:
        refresh_reflection_response = requests.post(
            "http://localhost:8003/api/reflection/refresh",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_badscore"}
        )
        if refresh_reflection_response.status_code == 200:
            print('[æˆåŠŸ] åå°„åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] åå°„åˆ·æ–°å¤±è´¥: {refresh_reflection_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] åå°„åˆ·æ–°å¼‚å¸¸: {e}')
    
    print('ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼')