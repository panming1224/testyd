# -*- coding: utf-8 -*-
"""
æ‹¼å¤šå¤šå®¢æœç»©æ•ˆæ•°æ®é‡‡é›†ï¼ˆæ—¥åº¦ï¼‰
é‡‡é›†T-3æ—¥ï¼ˆ3å¤©å‰ï¼‰çš„å®¢æœç»©æ•ˆæ•°æ®
"""
import json
import os
import requests
import pandas as pd
import time
from pathlib import Path
from tqdm import tqdm
import shutil
from datetime import datetime, timedelta
import sys

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
sys.path.append(r'D:\testyd\mysql')

from merge_excel_files import ExcelMerger
from crawler_db_interface import CrawlerDBInterface

# é…ç½®
BASE_ARCHIVE_DIR = Path('D:/pdd/å®¢æœç»©æ•ˆæ–‡ä»¶å­˜æ¡£')  # åŸºç¡€å­˜æ¡£ç›®å½•
MERGED_FILES_DIR = Path('D:/pdd/åˆå¹¶æ–‡ä»¶/å®¢æœç»©æ•ˆæ–‡ä»¶å­˜æ¡£')  # åˆå¹¶æ–‡ä»¶å­˜å‚¨ç›®å½•

# MinIO APIé…ç½®
MINIO_API_URL = "http://127.0.0.1:8009/api/upload"
MINIO_BUCKET = "warehouse"

# Dremio APIé…ç½®
DREMIO_API_URL = "http://localhost:8003/api"

# åˆ›å»ºåŸºç¡€å­˜æ¡£ç›®å½•å’Œåˆå¹¶æ–‡ä»¶ç›®å½•
os.makedirs(BASE_ARCHIVE_DIR, exist_ok=True)
os.makedirs(MERGED_FILES_DIR, exist_ok=True)

# ç›®æ ‡æ—¥æœŸï¼šT-3æ—¥ï¼ˆ3å¤©å‰ï¼‰
TODAY_STR = (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d')


def fetch_download_link(cookie: str):
    """è·å–å®¢æœç»©æ•ˆä¸‹è½½é“¾æ¥"""
    from datetime import time as dt_time
    
    today = datetime.now().date()
    t3_day = today - timedelta(days=3)  # T-3 æ—¥æœŸ

    # 00:00:00 æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    ts_start = int(datetime.combine(t3_day, dt_time.min).timestamp())
    # 23:59:59 æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    ts_end = int(datetime.combine(t3_day, dt_time.max).timestamp())

    url = ('https://mms.pinduoduo.com/chats/csReportDetail/download?'
           f'starttime={ts_start}&endtime={ts_end}&csRemoveRefundSalesAmountGray=true&')

    headers = {
        'accept': '*/*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'Cookie': cookie,
        'referer': 'https://mms.pinduoduo.com/mms-chat/overview/merchant',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    resp = requests.get(url, headers=headers, timeout=15)
    resp.raise_for_status()
    data = resp.json()
    
    if data.get('success') and data.get('result'):
        return data['result']
    raise RuntimeError(f'æ— ä¸‹è½½é“¾æ¥ï¼š{data.get("error_msg")}')


def silent_download(url: str, shop_name: str, date_str: str = None):
    """
    é™é»˜ä¸‹è½½Excelæ–‡ä»¶åˆ°æŒ‡å®šæ—¥æœŸæ–‡ä»¶å¤¹
    
    Args:
        url: ä¸‹è½½é“¾æ¥
        shop_name: åº—é“ºåç§°
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ '2025-10-02'ï¼Œé»˜è®¤ä½¿ç”¨TODAY_STR
    
    Returns:
        str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    if date_str is None:
        date_str = TODAY_STR
    
    # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹
    date_folder = BASE_ARCHIVE_DIR / date_str
    date_folder.mkdir(parents=True, exist_ok=True)
    
    # ä¸‹è½½æ–‡ä»¶
    resp = requests.get(url, timeout=60)
    resp.raise_for_status()
    
    # ä¿å­˜æ–‡ä»¶
    save_path = date_folder / f"{shop_name}.xlsx"
    with open(save_path, 'wb') as f:
        f.write(resp.content)
    
    return str(save_path)


def upload_merged_file_to_minio(file_path, date_str):
    """
    ä¸Šä¼ åˆå¹¶åçš„Excelæ–‡ä»¶åˆ°MinIOï¼ˆè½¬æ¢ä¸ºParquetæ ¼å¼ï¼‰
    
    Args:
        file_path: æœ¬åœ°Excelæ–‡ä»¶è·¯å¾„
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œç”¨äºåˆ†åŒº
    
    Returns:
        bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
    """
    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(file_path)
        
        # å¤„ç†NaNå€¼å’Œæ— ç©·å¤§å€¼
        df = df.fillna('')
        df = df.replace([float('inf'), float('-inf')], '')
        
        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½èƒ½æ­£å¸¸åºåˆ—åŒ–
        for col in df.columns:
            if df[col].dtype in ['float64', 'float32']:
                df[col] = df[col].replace([float('inf'), float('-inf')], '')
            df[col] = df[col].astype(str)
        
        # æ„å»ºMinIOè·¯å¾„
        minio_path = f"ods/pdd/pdd_kpi_days/dt={date_str}/merged_data.parquet"
        
        # å‡†å¤‡ä¸Šä¼ æ•°æ®
        upload_data = {
            "data": df.to_dict('records'),
            "target_path": minio_path,
            "format": "parquet",
            "bucket": MINIO_BUCKET
        }
        
        # å‘é€POSTè¯·æ±‚
        headers = {'Content-Type': 'application/json'}
        response = requests.post(MINIO_API_URL, json=upload_data, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print(f"âœ“ MinIOä¸Šä¼ æˆåŠŸ: {minio_path}")
                return True
            else:
                print(f"âœ— MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
        else:
            print(f"âœ— MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        print(f"âœ— MinIOä¸Šä¼ å¼‚å¸¸: {e}")
        return False


if __name__ == '__main__':
    print("=" * 60)
    print(f"æ‹¼å¤šå¤šå®¢æœç»©æ•ˆæ•°æ®é‡‡é›†ï¼ˆæ—¥åº¦ï¼‰- {TODAY_STR}")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®åº“æ¥å£
    db_interface = CrawlerDBInterface(
        platform='pdd',
        shops_table='pdd_shops',
        tasks_table='pdd_tasks',
        database='company'
    )
    
    # è·å–å¾…å¤„ç†ä»»åŠ¡ï¼ˆä¸ç”Ÿæˆä»»åŠ¡ï¼Œç”± generate_daily_tasks.py ç»Ÿä¸€ç”Ÿæˆï¼‰
    print(f"\n=== è·å–å¾…å¤„ç†ä»»åŠ¡ ===")
    pending_tasks = db_interface.get_pending_tasks(TODAY_STR, 'kpi_days_status')
    
    if not pending_tasks:
        print("âœ“ æ²¡æœ‰å¾…å¤„ç†ä»»åŠ¡ï¼Œæ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        print("\næç¤ºï¼šå¦‚éœ€é‡æ–°æ‰§è¡Œï¼Œè¯·å…ˆè¿è¡Œ generate_daily_tasks.py ç”Ÿæˆä»»åŠ¡")
        sys.exit(0)
    
    print(f"æ‰¾åˆ° {len(pending_tasks)} ä¸ªå¾…å¤„ç†ä»»åŠ¡")
    
    # æ‰¹é‡ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
    downloaded_files = []
    success_count = 0
    
    for task in pending_tasks:
        shop_name = task[1] if len(task) > 1 else None  # dt.shop_name
        cookie = task[11] if len(task) > 11 else None  # s.cookie (ç´¢å¼•11)
        
        if not cookie:
            print(f'[è­¦å‘Š] {shop_name} cookieä¸ºç©ºï¼Œè·³è¿‡')
            continue
        
        print(f"\n=== å¤„ç†åº—é“º: {shop_name} ===")
        
        try:
            down_url = fetch_download_link(cookie)
            save_path = silent_download(down_url, shop_name)
            print(f'[æˆåŠŸ] {shop_name}  ä¸‹è½½å®Œæˆ: {save_path}')
            
            downloaded_files.append(save_path)
            success_count += 1
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å®Œæˆ
            db_interface.update_task_status(TODAY_STR, shop_name, 'kpi_days_status', 'å·²å®Œæˆ')
        except Exception as e:
            # å¤±è´¥æ—¶ä¸æ›´æ–°çŠ¶æ€ï¼Œä¿æŒå¾…æ‰§è¡ŒçŠ¶æ€ä¾¿äºé‡è¯•
            print(f'[é”™è¯¯] {shop_name}  å¤±è´¥ï¼š{e}ï¼Œä¿æŒå¾…æ‰§è¡ŒçŠ¶æ€')
    
    print(f"\n=== æ•°æ®å¤„ç†å®Œæˆ ===")
    print(f"æˆåŠŸå¤„ç†: {success_count}/{len(pending_tasks)} ä¸ªåº—é“º")
    
    # åˆå¹¶æ–‡ä»¶
    if downloaded_files:
        print(f'\nğŸ”„ å¼€å§‹åˆå¹¶ {len(downloaded_files)} ä¸ªExcelæ–‡ä»¶...')
        date_dir = BASE_ARCHIVE_DIR / TODAY_STR
        merger = ExcelMerger(str(date_dir), output_dir=str(MERGED_FILES_DIR))
        merge_success = merger.merge_excel_files(f"å®¢æœç»©æ•ˆåˆå¹¶_{TODAY_STR}.xlsx")
        
        if merge_success:
            final_merged_file_path = MERGED_FILES_DIR / f"å®¢æœç»©æ•ˆåˆå¹¶_{TODAY_STR}.xlsx"
            print(f'[æˆåŠŸ] æ–‡ä»¶åˆå¹¶å®Œæˆ: {final_merged_file_path}')
            
            # ä¸Šä¼ åˆ°MinIO
            print(f'ğŸ”„ æ­£åœ¨ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO...')
            upload_success = upload_merged_file_to_minio(str(final_merged_file_path), TODAY_STR)
            
            if upload_success:
                print(f'[æˆåŠŸ] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ æˆåŠŸ')
            else:
                print(f'[è­¦å‘Š] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°æ–‡ä»¶å·²ä¿å­˜')
        else:
            print('[é”™è¯¯] æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåˆå¹¶æ–‡ä»¶')
    else:
        print('[è­¦å‘Š] æ²¡æœ‰æ–°ä¸‹è½½çš„æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶å’Œä¸Šä¼ æ­¥éª¤')
    
    # åˆ·æ–°Dremio
    print('\nğŸ”„ æ­£åœ¨åˆ·æ–°Dremioæ•°æ®é›†å’Œåå°„...')
    try:
        refresh_dataset_response = requests.post(
            f"{DREMIO_API_URL}/dataset/refresh-metadata",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_kpi_days"}
        )
        if refresh_dataset_response.status_code == 200:
            print('[æˆåŠŸ] æ•°æ®é›†åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] æ•°æ®é›†åˆ·æ–°å¤±è´¥: {refresh_dataset_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] æ•°æ®é›†åˆ·æ–°å¼‚å¸¸: {e}')
    
    try:
        refresh_reflection_response = requests.post(
            f"{DREMIO_API_URL}/reflection/refresh",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_kpi_days"}
        )
        if refresh_reflection_response.status_code == 200:
            print('[æˆåŠŸ] åå°„åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] åå°„åˆ·æ–°å¤±è´¥: {refresh_reflection_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] åå°„åˆ·æ–°å¼‚å¸¸: {e}')
    
    print('\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼')

