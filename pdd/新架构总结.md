# 拼多多爬虫系统 - 新架构总结

## 🎯 核心设计理念

### 问题背景

你提出的核心需求：
1. **任务状态要区分"空值"和"待执行"**
   - 空值（NULL）= 暂时没有任务
   - "待执行" = 有任务需要执行
   - "已完成" = 任务已完成

2. **不同任务类型有不同的执行频率**
   - 日度任务：每天执行（差评、产品质量、客服绩效日度）
   - 周度任务：每周执行（客服绩效周度）
   - 月度任务：每月执行（客服绩效月度）

3. **新增爬虫要简单**
   - 只需添加固定模块
   - 不需要修改核心代码

### 解决方案

采用**配置驱动 + 基类继承**的架构：

```
配置文件 (crawler_config.py)
    ↓
任务生成器 (generate_tasks.py)
    ↓
数据库 (pdd_tasks表)
    ↓
爬虫程序 (继承BaseCrawler)
    ↓
数据处理 (合并、上传、刷新)
```

## 📁 核心文件说明

### 1. crawler_config.py - 配置中心

**作用：** 集中管理所有爬虫的配置

**内容：**
```python
CRAWLER_TASKS = {
    'badscore': {
        'name': '差评数据采集',
        'status_field': 'badsscore_status',
        'schedule': 'daily',
        'date_offset': -1,
        'minio_path': 'ods/pdd/pdd_badscore',
        'dremio_table': 'minio.warehouse.ods.pdd.pdd_badscore',
        'enabled': True
    },
    # ... 更多任务配置
}
```

**优势：**
- 所有配置集中管理
- 修改配置无需改代码
- 新增任务只需添加配置

### 2. generate_tasks.py - 统一任务生成器

**作用：** 根据配置自动生成所有类型的任务

**使用方法：**
```bash
# 生成每日任务
python testyd/pdd/generate_tasks.py --schedule daily

# 生成每周任务
python testyd/pdd/generate_tasks.py --schedule weekly

# 生成所有任务
python testyd/pdd/generate_tasks.py --schedule all
```

**核心逻辑：**
1. 读取配置文件
2. 筛选符合调度类型的任务
3. 为每个任务调用数据库接口生成任务记录
4. 使用 `INSERT ... ON DUPLICATE KEY UPDATE` 确保：
   - 新记录：插入，状态为"待执行"
   - 已存在记录：只更新NULL字段为"待执行"，已完成的不变

### 3. base_crawler.py - 爬虫基类

**作用：** 提供所有爬虫的通用功能

**功能模块：**
- ✅ 获取待处理任务
- ✅ 更新任务状态
- ✅ 文件合并
- ✅ MinIO上传
- ✅ Dremio刷新

**子类只需实现：**
```python
def process_shop(self, shop_name, cookie, **kwargs):
    """处理单个店铺的数据采集"""
    # 1. 调用API获取数据
    # 2. 保存到Excel
    # 3. 返回文件路径
```

### 4. crawler_db_interface.py - 数据库接口

**核心方法：**

#### generate_tasks()
```python
def generate_tasks(self, time_period: str, task_columns: List[str]) -> int:
    """
    为所有店铺生成指定时间周期的任务
    
    策略：
    1. INSERT时只设置指定的任务字段为"待执行"
    2. 如果记录已存在，只更新指定的任务字段（如果是NULL则设为"待执行"）
    3. 已完成的任务不会被重置
    """
```

**SQL逻辑：**
```sql
INSERT INTO pdd_tasks (time_period, shop_name, badsscore_status) 
VALUES ('2025-10-02', '361南宁专卖店', '待执行')
ON DUPLICATE KEY UPDATE 
    badsscore_status = IF(badsscore_status IS NULL OR badsscore_status = '', '待执行', badsscore_status)
```

#### get_pending_tasks()
```python
def get_pending_tasks(self, time_period: str, task_column: str) -> List:
    """
    获取指定时间周期和任务类型的待处理任务
    
    查询条件：status = '待执行'
    """
```

**SQL逻辑：**
```sql
SELECT dt.time_period, dt.shop_name, s.*
FROM pdd_tasks dt
JOIN pdd_shops s ON dt.shop_name = s.shop_name
WHERE dt.time_period = '2025-10-02' 
  AND dt.badsscore_status = '待执行'
```

## 🔄 完整工作流程

### 每日执行流程

```
1. 生成任务（每天最先执行）
   python testyd/pdd/generate_tasks.py --schedule daily
   ↓
   为所有店铺创建/更新任务记录
   - badsscore_status = '待执行'
   - quality_status = '待执行'
   - kpi_days_status = '待执行'

2. 执行爬虫程序
   python testyd/pdd/pdd_badscore.py
   ↓
   - 查询 badsscore_status = '待执行' 的任务
   - 逐个处理店铺数据
   - 成功后更新状态为 '已完成'
   - 失败保持 '待执行' 状态

3. 继续执行其他爬虫
   python testyd/pdd/pdd_quality.py
   python testyd/pdd/pdd_kpi.py
```

### 周度/月度执行流程

```
1. 每周/每月生成任务
   python testyd/pdd/generate_tasks.py --schedule weekly
   ↓
   只更新 kpi_weekly_status = '待执行'
   其他字段保持不变

2. 执行周度/月度爬虫
   python testyd/pdd/pdd_kpi_weekly.py
```

## ➕ 新增爬虫步骤

### 示例：新增"店铺评分"爬虫

#### 步骤1：添加配置

在 `crawler_config.py` 中添加：

```python
CRAWLER_TASKS = {
    # ... 现有配置 ...
    
    'shop_rating': {
        'name': '店铺评分数据采集',
        'script': 'pdd_shop_rating.py',
        'status_field': 'shop_rating_status',
        'schedule': 'daily',
        'date_offset': -1,
        'minio_path': 'ods/pdd/pdd_shop_rating',
        'dremio_table': 'minio.warehouse.ods.pdd.pdd_shop_rating',
        'enabled': True
    }
}
```

#### 步骤2：添加数据库字段

```sql
ALTER TABLE pdd_tasks 
ADD COLUMN shop_rating_status VARCHAR(20) DEFAULT NULL;
```

#### 步骤3：创建爬虫脚本

创建 `testyd/pdd/pdd_shop_rating.py`：

```python
# -*- coding: utf-8 -*-
"""店铺评分数据采集"""
import sys
from datetime import datetime, timedelta
sys.path.append(r'D:\testyd\pdd')

from base_crawler import BaseCrawler
from crawler_config import CRAWLER_TASKS

class ShopRatingCrawler(BaseCrawler):
    """店铺评分爬虫"""
    
    def __init__(self):
        task_config = CRAWLER_TASKS['shop_rating']
        super().__init__('shop_rating', task_config)
        
        # 设置目标日期
        offset = task_config.get('date_offset', -1)
        self.target_date = (datetime.now() + timedelta(days=offset)).strftime('%Y-%m-%d')
    
    def process_shop(self, shop_name, cookie, **kwargs):
        """处理单个店铺的数据采集"""
        try:
            # 1. 调用API获取店铺评分数据
            rating_data = self.fetch_rating_data(cookie)
            
            # 2. 保存到Excel
            save_path = self.save_to_excel(rating_data, shop_name)
            
            return save_path
        except Exception as e:
            print(f'[错误] {shop_name} 数据采集失败: {e}')
            return None
    
    def fetch_rating_data(self, cookie):
        """获取店铺评分数据"""
        # TODO: 实现API调用逻辑
        pass
    
    def save_to_excel(self, data, shop_name):
        """保存数据到Excel"""
        # TODO: 实现保存逻辑
        pass

if __name__ == '__main__':
    crawler = ShopRatingCrawler()
    crawler.run()
```

#### 步骤4：测试

```bash
# 1. 生成任务
python testyd/pdd/generate_tasks.py --schedule daily

# 2. 执行爬虫
python testyd/pdd/pdd_shop_rating.py

# 3. 检查状态
python testyd/check_task_status.py
```

**完成！** 无需修改任何核心代码！

## 🎯 核心优势

### 1. 配置驱动

- ✅ 所有配置集中在一个文件
- ✅ 修改配置无需改代码
- ✅ 易于维护和管理

### 2. 任务独立

- ✅ 不同任务类型独立执行
- ✅ 日度/周度/月度任务互不影响
- ✅ 失败任务可单独重试

### 3. 状态清晰

- ✅ NULL = 没有任务
- ✅ "待执行" = 需要执行
- ✅ "已完成" = 已完成
- ✅ 重复生成不会重置已完成状态

### 4. 易于扩展

- ✅ 新增爬虫只需3步
- ✅ 无需修改核心代码
- ✅ 真正的"即插即用"

### 5. 代码复用

- ✅ BaseCrawler提供通用功能
- ✅ 子类只需实现业务逻辑
- ✅ 减少重复代码

## 📊 数据库设计

### pdd_tasks 表结构

```sql
CREATE TABLE pdd_tasks (
    time_period VARCHAR(20) NOT NULL,           -- 时间周期
    shop_name VARCHAR(100) NOT NULL,            -- 店铺名称
    badsscore_status VARCHAR(20) DEFAULT NULL,  -- 差评数据状态
    quality_status VARCHAR(20) DEFAULT NULL,    -- 产品质量状态
    kpi_days_status VARCHAR(20) DEFAULT NULL,   -- 客服绩效（日度）状态
    kpi_weekly_status VARCHAR(20) DEFAULT NULL, -- 客服绩效（周度）状态
    kpi_monthly_status VARCHAR(20) DEFAULT NULL,-- 客服绩效（月度）状态
    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    PRIMARY KEY (time_period, shop_name)
);
```

### 状态值说明

- **NULL** - 没有任务（该店铺该日期不需要执行此任务）
- **"待执行"** - 有任务需要执行
- **"已完成"** - 任务已完成

## 🔍 关键技术点

### 1. INSERT ... ON DUPLICATE KEY UPDATE

```sql
INSERT INTO pdd_tasks (time_period, shop_name, badsscore_status) 
VALUES ('2025-10-02', '361南宁专卖店', '待执行')
ON DUPLICATE KEY UPDATE 
    badsscore_status = IF(badsscore_status IS NULL OR badsscore_status = '', '待执行', badsscore_status)
```

**作用：**
- 如果记录不存在，插入新记录
- 如果记录已存在，只更新NULL字段
- 已完成的任务不会被重置

### 2. 动态SQL构建

```python
# 只插入指定的任务字段
insert_columns = ['time_period', 'shop_name'] + available_columns
placeholders = ', '.join(['%s'] * len(insert_columns))

# 只更新指定的任务字段
update_clauses = []
for col in available_columns:
    update_clauses.append(
        f"{col} = IF({col} IS NULL OR {col} = '', '待执行', {col})"
    )
```

**优势：**
- 灵活支持不同任务类型
- 不同任务可独立更新
- 避免相互影响

## 🎉 总结

新架构完美解决了你提出的所有需求：

1. ✅ **状态区分明确**
   - NULL = 没有任务
   - "待执行" = 需要执行
   - "已完成" = 已完成

2. ✅ **支持不同执行频率**
   - 日度任务每天执行
   - 周度任务每周执行
   - 月度任务每月执行

3. ✅ **易于扩展**
   - 新增爬虫只需添加配置
   - 无需修改核心代码
   - 真正的"即插即用"

4. ✅ **代码复用**
   - BaseCrawler提供通用功能
   - 子类只需实现业务逻辑
   - 大大减少重复代码

5. ✅ **可维护性强**
   - 配置集中管理
   - 结构清晰
   - 易于理解和维护

**后续开发新爬虫，只需3步：**
1. 在配置文件中添加配置
2. 在数据库中添加字段
3. 创建继承BaseCrawler的脚本

**真正实现了"加固定模块就可以"的目标！** 🎯

