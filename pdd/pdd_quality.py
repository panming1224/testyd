# -*- coding: utf-8 -*-
import json
import os
import requests
import pandas as pd
import time
from pathlib import Path
from tqdm import tqdm
import shutil
from datetime import datetime, timedelta
import sys

# é…ç½®UTF-8ç¼–ç 
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
sys.path.append(r'D:\testyd\mysql')

from merge_excel_files import ExcelMerger
from crawler_db_interface import CrawlerDBInterface

# é…ç½®
BASE_ARCHIVE_DIR = Path('D:/pdd/äº§å“è´¨é‡ä½“éªŒå­˜æ¡£')  # åŸºç¡€å­˜æ¡£ç›®å½•
MERGED_FILES_DIR = Path('D:/pdd/åˆå¹¶æ–‡ä»¶/äº§å“è´¨é‡ä½“éªŒå­˜æ¡£')  # åˆå¹¶æ–‡ä»¶å­˜å‚¨ç›®å½•

# MinIO APIé…ç½®
MINIO_API_URL = "http://127.0.0.1:8009/api/upload"
MINIO_BUCKET = "warehouse"

# Dremio APIé…ç½®
DREMIO_API_URL = "http://localhost:8003/api"

# åˆ›å»ºåŸºç¡€å­˜æ¡£ç›®å½•å’Œåˆå¹¶æ–‡ä»¶ç›®å½•
os.makedirs(BASE_ARCHIVE_DIR, exist_ok=True)
os.makedirs(MERGED_FILES_DIR, exist_ok=True)

# ç›®æ ‡æ—¥æœŸï¼šTï¼ˆä»Šå¤©ï¼‰
TODAY_STR = datetime.now().strftime('%Y-%m-%d')

# äº§å“è´¨é‡æ•°æ®è¡¨å¤´å’Œå¯¹åº”å­—æ®µ
QUALITY_HEADERS = ['å•†å“id', 'å•†å“åç§°', 'å•†å“ä¸»å›¾é“¾æ¥', 'å•†å“è´¨é‡ä½“éªŒæ’å', 'è¿‘30å¤©å¼‚å¸¸è®¢å•æ•°', 
                   'å¼‚å¸¸è®¢å•å æ¯”', 'æƒç›ŠçŠ¶æ€', 'å•†å“è´¨é‡ç­‰çº§', 'è¿‘30å¤©å“è´¨æ±‚åŠ©å¹³å°ç‡', 
                   'è¿‘30å¤©å•†å“è¯„ä»·åˆ†æ’å', 'è€å®¢è®¢å•å æ¯”']

QUALITY_FIELDS = ['goods_id', 'goods_name', 'img_url', 'rank_percent', 'abnormal_order_num', 
                  'abnormal_order_ratio', 'right_status', 'quality_level', 'quality_help_rate_last30_days', 
                  'goods_rating_rank', 'repeat_purchase_ratio']

def update_header_once(df: pd.DataFrame):
    """å›ºå®šç”¨ K åˆ—ï¼ˆç´¢å¼• 10ï¼‰ä½œä¸ºçŠ¶æ€åˆ—ï¼Œæ¯å¤©ç¬¬ä¸€æ¬¡æŠŠè¡¨å¤´æ”¹æˆ ä»Šå¤©+äº§å“è´¨é‡ä¸‹è½½çŠ¶æ€"""
    new_status = f'{TODAY_STR}äº§å“è´¨é‡ä¸‹è½½çŠ¶æ€'
    old_header = df.columns[10]          # K åˆ—å½“å‰åå­—

    if old_header == new_status:
        return False                     # ä»Šå¤©å·²æ›´æ–°è¿‡ï¼Œè¿”å›Falseè¡¨ç¤ºæœªæ›´æ–°

    # æ”¹åï¼ˆåªæ”¹è¡¨å¤´ï¼Œæ•°æ®ä¸åŠ¨ï¼‰
    df.rename(columns={old_header: new_status}, inplace=True)
    # åªåœ¨æ¯å¤©ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ¸…ç©ºKåˆ—çš„æ‰€æœ‰æ•°æ®ï¼ˆé™¤äº†è¡¨å¤´ï¼‰
    # è¿™æ ·é¿å…äº†æ¯æ¬¡è¿è¡Œéƒ½æ¸…ç©ºçŠ¶æ€çš„é—®é¢˜
    df.iloc[:, 10] = ''
    
    # ç«‹å³å›å†™ Excel
    try:
        df.to_excel(EXCEL_PATH, index=False, engine='openpyxl')
        print(f"å·²æ›´æ–°è¡¨å¤´ä¸º: {new_status} å¹¶æ¸…ç©ºæ‰€æœ‰çŠ¶æ€")
        return True                      # è¿”å›Trueè¡¨ç¤ºå·²æ›´æ–°
    except PermissionError:
        print(f"æ— æ³•å†™å…¥Excelæ–‡ä»¶ï¼Œå¯èƒ½è¢«å…¶ä»–ç¨‹åºå ç”¨ã€‚ç»§ç»­ä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®...")
        return False


def fetch_quality_data(cookie: str):
    """
    è·å–äº§å“è´¨é‡ä½“éªŒæ•°æ®
    """
    url = 'https://mms.pinduoduo.com/api/price/mariana/quality_experience/goods_list'
    
    headers = {
        'accept': '*/*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'anti-content': '',
        'cache-control': '',
        'Content-Type': 'application/json',
        'Cookie': cookie,
        'etag': '',
        'origin': 'https://mms.pinduoduo.com',
        'priority': '',
        'referer': 'https://mms.pinduoduo.com/mms-marketing-mixin/quality-experience?msfrom=mms_globalsearch',
        'sec-ch-ua': '',
        'sec-ch-ua-mobile': '',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': '',
        'sec-fetch-mode': '',
        'sec-fetch-site': '',
        'user-agent': ''
    }

    # å…ˆè·å–ç¬¬ä¸€é¡µæ•°æ®ï¼Œç¡®å®šæ€»æ•°é‡
    body = {"sort_field": 1, "sort_type": "ASC", "page_size": 40, "page_no": 1}
    
    resp = requests.post(url, headers=headers, json=body, timeout=15)
    print(f'ã€å“åº”ã€‘{resp.text}')
    resp.raise_for_status()
    
    first_page_data = resp.json()
    if not first_page_data.get('success'):
        raise RuntimeError(f'APIè¯·æ±‚å¤±è´¥ï¼š{first_page_data.get("error_msg")}')
    
    result = first_page_data.get('result', {})
    total = result.get('total', 0)
    
    print(f'æ€»å…±æœ‰ {total} æ¡äº§å“è´¨é‡æ•°æ®')
    
    # è®¡ç®—éœ€è¦å¤šå°‘é¡µ
    page_size = 40
    total_pages = (total + page_size - 1) // page_size
    
    all_goods_list = []
    
    # è·å–æ‰€æœ‰é¡µé¢çš„æ•°æ®
    for page_no in range(1, total_pages + 1):
        print(f'æ­£åœ¨è·å–ç¬¬ {page_no}/{total_pages} é¡µæ•°æ®...')
        
        body = {"sort_field": 1, "sort_type": "ASC", "page_size": page_size, "page_no": page_no}
        
        resp = requests.post(url, headers=headers, json=body, timeout=15)
        resp.raise_for_status()
        
        page_data = resp.json()
        if page_data.get('success'):
            page_result = page_data.get('result', {})
            goods_list = page_result.get('goods_list', [])
            all_goods_list.extend(goods_list)
            print(f'ç¬¬ {page_no} é¡µè·å–åˆ° {len(goods_list)} æ¡æ•°æ®')
        else:
            print(f'ç¬¬ {page_no} é¡µè·å–å¤±è´¥ï¼š{page_data.get("error_msg")}')
        
        # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.15)
    
    return total, all_goods_list


def to_list(total, goods_list, cols):
    """
    è§£æå“åº”æ•°æ®ï¼Œè½¬æ¢ä¸ºçŸ©é˜µæ ¼å¼
    """
    matrix = [[row.get(c, '') for c in cols] for row in goods_list]
    return [total, matrix]


def save_quality_data_to_excel(shop_name: str, total: int, goods_list: list, date_str: str = None):
    """
    å°†äº§å“è´¨é‡æ•°æ®ä¿å­˜åˆ°Excelæ–‡ä»¶
    """
    if date_str is None:
        date_str = TODAY_STR
    
    # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
    date_dir = BASE_ARCHIVE_DIR / date_str
    date_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶ä¿å­˜è·¯å¾„
    file_name = f'{shop_name}äº§å“è´¨é‡æ•°æ®.xlsx'
    save_path = date_dir / file_name

    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
    if save_path.exists():
        print(f'å‘ç°åŒåæ–‡ä»¶ï¼Œæ­£åœ¨åˆ é™¤: {save_path}')
        save_path.unlink()
        print(f'åŒåæ–‡ä»¶å·²åˆ é™¤')

    # è½¬æ¢æ•°æ®ä¸ºDataFrameæ ¼å¼
    total_data, matrix = to_list(total, goods_list, QUALITY_FIELDS)
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(matrix, columns=QUALITY_HEADERS)
    
    # ä¿å­˜åˆ°Excelæ–‡ä»¶
    df.to_excel(save_path, index=False, engine='openpyxl')
    
    print(f'äº§å“è´¨é‡æ•°æ®å·²ä¿å­˜: {save_path} (å…± {len(matrix)} æ¡æ•°æ®)')
    
    return save_path


def upload_merged_file_to_minio(merged_file_path: str, date_str: str = None) -> bool:
    """
    å°†åˆå¹¶åçš„Excelæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°MinIO
    """
    if date_str is None:
        date_str = TODAY_STR
    
    try:
        # è¯»å–åˆå¹¶åçš„Excelæ–‡ä»¶
        df = pd.read_excel(merged_file_path)
        
        # å¤„ç†NaNå€¼ï¼Œæ›¿æ¢ä¸ºNone
        df = df.replace({pd.NA: None})
        df = df.where(pd.notnull(df), None)
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_dict = df.to_dict(orient='records')
        
        # è¿›ä¸€æ­¥æ¸…ç†NaNå€¼
        cleaned_data = []
        for record in data_dict:
            cleaned_record = {}
            for key, value in record.items():
                if pd.isna(value) or value is pd.NA:
                    cleaned_record[key] = ""  # ä½¿ç”¨ç©ºå­—ç¬¦ä¸²æ›¿ä»£NaN
                elif isinstance(value, float) and (value != value):  # æ£€æŸ¥NaN
                    cleaned_record[key] = ""
                else:
                    cleaned_record[key] = value
            cleaned_data.append(cleaned_record)
        
        # æ„å»ºMinIOè·¯å¾„ï¼šods/pdd/pdd_quality/dt=æ—¥æœŸ/merged_quality_data.parquet
        minio_path = f"ods/pdd/pdd_quality/dt={date_str}/merged_quality_data.parquet"
        
        # å‡†å¤‡ä¸Šä¼ æ•°æ®
        upload_data = {
            "data": cleaned_data,
            "target_path": minio_path,
            "format": "parquet",
            "bucket": MINIO_BUCKET
        }
        
        # å‘é€POSTè¯·æ±‚åˆ°MinIO API
        headers = {'Content-Type': 'application/json'}
        response = requests.post(MINIO_API_URL, json=upload_data, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print(f"æˆåŠŸä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO: {minio_path}")
                return True
            else:
                print(f"MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
        else:
            print(f"MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            return False
            
    except Exception as e:
        print(f"ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIOæ—¶å‡ºé”™: {str(e)}")
        return False


def safe_read_excel():
    """å®‰å…¨è¯»å–Excelæ–‡ä»¶ï¼Œå¦‚æœä¸»æ–‡ä»¶æŸååˆ™ä½¿ç”¨å¤‡ä»½æ–‡ä»¶"""
    try:
        print(f"å°è¯•è¯»å–ä¸»Excelæ–‡ä»¶: {EXCEL_PATH}")
        df = pd.read_excel(EXCEL_PATH, sheet_name=SHEET)
        print("âœ… ä¸»Excelæ–‡ä»¶è¯»å–æˆåŠŸ")
        return df, EXCEL_PATH
    except Exception as e:
        print(f"âŒ ä¸»Excelæ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        print(f"å°è¯•è¯»å–å¤‡ä»½Excelæ–‡ä»¶: {EXCEL_BACKUP_PATH}")
        try:
            df = pd.read_excel(EXCEL_BACKUP_PATH, sheet_name=SHEET)
            print("âœ… å¤‡ä»½Excelæ–‡ä»¶è¯»å–æˆåŠŸ")
            # å°†å¤‡ä»½æ–‡ä»¶å¤åˆ¶ä¸ºä¸»æ–‡ä»¶
            import shutil
            shutil.copy2(EXCEL_BACKUP_PATH, EXCEL_PATH)
            print("âœ… å·²å°†å¤‡ä»½æ–‡ä»¶å¤åˆ¶ä¸ºä¸»æ–‡ä»¶")
            return df, EXCEL_PATH
        except Exception as backup_e:
            print(f"âŒ å¤‡ä»½Excelæ–‡ä»¶ä¹Ÿè¯»å–å¤±è´¥: {backup_e}")
            raise Exception(f"ä¸»æ–‡ä»¶å’Œå¤‡ä»½æ–‡ä»¶éƒ½æ— æ³•è¯»å–: ä¸»æ–‡ä»¶é”™è¯¯={e}, å¤‡ä»½æ–‡ä»¶é”™è¯¯={backup_e}")


if __name__ == '__main__':
    print("=" * 60)
    print(f"æ‹¼å¤šå¤šäº§å“è´¨é‡æ•°æ®é‡‡é›† - {TODAY_STR}")
    print("=" * 60)

    # åˆå§‹åŒ–æ•°æ®åº“æ¥å£
    db_interface = CrawlerDBInterface(
        platform='pdd',
        shops_table='pdd_shops',
        tasks_table='pdd_tasks',
        database='company'
    )

    # è·å–å¾…å¤„ç†ä»»åŠ¡ï¼ˆä¸ç”Ÿæˆä»»åŠ¡ï¼Œç”± generate_daily_tasks.py ç»Ÿä¸€ç”Ÿæˆï¼‰
    print(f"\n=== è·å–å¾…å¤„ç†ä»»åŠ¡ ===")
    pending_tasks = db_interface.get_pending_tasks(TODAY_STR, 'quality_status')

    if not pending_tasks:
        print("âœ“ æ²¡æœ‰å¾…å¤„ç†ä»»åŠ¡ï¼Œæ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        print("\næç¤ºï¼šå¦‚éœ€é‡æ–°æ‰§è¡Œï¼Œè¯·å…ˆè¿è¡Œ generate_daily_tasks.py ç”Ÿæˆä»»åŠ¡")
        import sys
        sys.exit(0)

    print(f"æ‰¾åˆ° {len(pending_tasks)} ä¸ªå¾…å¤„ç†ä»»åŠ¡")

    # 3. æ‰¹é‡æ‹‰å–æ‰€æœ‰åº—é“ºçš„äº§å“è´¨é‡æ•°æ®
    saved_files = []
    success_count = 0

    for task in pending_tasks:
        shop_name = task[1] if len(task) > 1 else None  # dt.shop_name
        cookie = task[11] if len(task) > 11 else None  # s.cookie (ç´¢å¼•11)

        if not cookie:
            print(f'[è­¦å‘Š] {shop_name}  cookie ä¸ºç©ºï¼Œè·³è¿‡')
            continue

        print(f"\n=== å¤„ç†åº—é“º: {shop_name} ===")

        try:
            total, goods_list = fetch_quality_data(cookie)
            save_path = save_quality_data_to_excel(shop_name, total, goods_list)
            print(f'[æˆåŠŸ] {shop_name}  äº§å“è´¨é‡æ•°æ®æ‹‰å–å®Œæˆ: {save_path}')

            saved_files.append(save_path)
            success_count += 1

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å®Œæˆ
            db_interface.update_task_status(TODAY_STR, shop_name, 'quality_status', 'å·²å®Œæˆ')
        except Exception as e:
            # å¤±è´¥æ—¶ä¸æ›´æ–°çŠ¶æ€ï¼Œä¿æŒå¾…æ‰§è¡ŒçŠ¶æ€ä¾¿äºé‡è¯•
            print(f'[é”™è¯¯] {shop_name}  å¤±è´¥ï¼š{e}ï¼Œä¿æŒå¾…æ‰§è¡ŒçŠ¶æ€')

    print(f"\n=== æ•°æ®å¤„ç†å®Œæˆ ===")
    print(f"æˆåŠŸå¤„ç†: {success_count}/{len(pending_tasks)} ä¸ªåº—é“º")

    # 4. å¦‚æœæœ‰ä¿å­˜çš„æ–‡ä»¶ï¼Œè¿›è¡Œåˆå¹¶å’Œä¸Šä¼ 
    if saved_files:
        print(f'\nğŸ”„ å¼€å§‹åˆå¹¶ {len(saved_files)} ä¸ªExcelæ–‡ä»¶...')

        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
        date_dir = BASE_ARCHIVE_DIR / TODAY_STR

        # ä½¿ç”¨ExcelMergeråˆå¹¶æ–‡ä»¶ï¼ŒæŒ‡å®šç‹¬ç«‹çš„è¾“å‡ºç›®å½•
        merger = ExcelMerger(str(date_dir), output_dir=str(MERGED_FILES_DIR))
        merge_success = merger.merge_excel_files(f"{TODAY_STR}.xlsx")

        if merge_success:
            # åˆå¹¶åçš„æ–‡ä»¶ç›´æ¥åœ¨æœ€ç»ˆç›®å½•ä¸­
            final_merged_file_path = MERGED_FILES_DIR / f"{TODAY_STR}.xlsx"

            print(f'[æˆåŠŸ] æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³: {final_merged_file_path}')

            # ä¸Šä¼ åˆå¹¶åçš„æ–‡ä»¶åˆ°MinIO
            print(f'ğŸ”„ æ­£åœ¨ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO...')
            upload_success = upload_merged_file_to_minio(str(final_merged_file_path), TODAY_STR)

            if upload_success:
                print(f'[æˆåŠŸ] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ æˆåŠŸ')
            else:
                print(f'[è­¦å‘Š] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°æ–‡ä»¶å·²ä¿å­˜')
        else:
            print('[é”™è¯¯] æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåˆå¹¶æ–‡ä»¶')
    else:
        print('[è­¦å‘Š] æ²¡æœ‰æ–°ä¿å­˜çš„æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶å’Œä¸Šä¼ æ­¥éª¤')

    # 5. æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œåˆ·æ–°æ•°æ®é›†å’Œåå°„
    print('\nğŸ”„ æ­£åœ¨åˆ·æ–°Dremioæ•°æ®é›†å’Œåå°„...')
    try:
        refresh_dataset_response = requests.post(
            f"{DREMIO_API_URL}/dataset/refresh-metadata",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_quality"}
        )
        if refresh_dataset_response.status_code == 200:
            print('[æˆåŠŸ] æ•°æ®é›†åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] æ•°æ®é›†åˆ·æ–°å¤±è´¥: {refresh_dataset_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] æ•°æ®é›†åˆ·æ–°å¼‚å¸¸: {e}')

    try:
        refresh_reflection_response = requests.post(
            f"{DREMIO_API_URL}/reflection/refresh",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_quality"}
        )
        if refresh_reflection_response.status_code == 200:
            print('[æˆåŠŸ] åå°„åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] åå°„åˆ·æ–°å¤±è´¥: {refresh_reflection_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] åå°„åˆ·æ–°å¼‚å¸¸: {e}')

    print('\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼')