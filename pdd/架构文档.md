# 拼多多爬虫系统 - 架构文档

## 📋 文档说明

**本文档目的：**
- 快速了解系统功能和调用方式
- 避免重复开发
- 新增功能时更新此文档

**最后更新：** 2025-10-03

---

## 🏗️ 系统架构

### 核心设计理念

1. **配置驱动** - 所有爬虫配置集中管理
2. **任务状态管理** - 精确的状态跟踪（NULL/待执行/已完成）
3. **基类复用** - 通用功能封装在基类中
4. **易于扩展** - 新增爬虫只需3步

### 架构图

```
┌─────────────────────────────────────────────────────────┐
│                    任务调度层                              │
│  (Prefect/Cron) - 定时触发任务生成和执行                    │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                  任务生成器                                │
│  generate_tasks.py - 根据配置生成待执行任务                 │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                  数据库任务表                              │
│  pdd_tasks - 存储任务状态（待执行/已完成）                   │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                  爬虫执行层                                │
│  各个爬虫程序 - 读取待执行任务，采集数据，更新状态            │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                  数据存储层                                │
│  MinIO + Dremio - 数据存储和查询                           │
└─────────────────────────────────────────────────────────┘
```

---

## 📁 文件结构

```
testyd/pdd/
├── crawler_config.py          # 【核心】爬虫配置文件
├── generate_tasks.py          # 【核心】任务生成器
├── base_crawler.py            # 【核心】爬虫基类
├── pdd_badscore.py           # 差评数据采集（T-1）
├── pdd_quality.py            # 产品质量数据采集（T）
├── pdd_kpi.py                # 客服绩效数据采集（日度，T-3）
├── pdd_kpi_weekly.py         # 客服绩效数据采集（周度）
├── pdd_kpi_monthly.py        # 客服绩效数据采集（月度）
├── pdd_chat.py               # 聊天数据采集（T-1）- 待实现
├── run_all_shops.py          # 完整测试脚本
└── 架构文档.md               # 本文档
```

---

## 🔧 核心模块

### 1. crawler_config.py - 配置中心

**功能：** 集中管理所有爬虫配置

**配置结构：**
```python
CRAWLER_TASKS = {
    'task_key': {
        'name': '任务名称',
        'script': '脚本文件名',
        'status_field': '数据库状态字段名',
        'schedule': 'daily/weekly/monthly',
        'date_offset': -1,  # 日期偏移（T-1表示昨天）
        'minio_path': 'MinIO存储路径',
        'dremio_table': 'Dremio表名',
        'enabled': True/False
    }
}
```

**调用方式：**
```python
from testyd.pdd.crawler_config import CRAWLER_TASKS, get_enabled_tasks

# 获取所有启用的任务
enabled_tasks = get_enabled_tasks()

# 获取特定任务配置
task_config = CRAWLER_TASKS['badscore']
```

**当前配置的任务：**

| 任务Key | 任务名称 | 日期偏移 | 状态字段 | 调度频率 | 状态 |
|---------|---------|---------|---------|---------|------|
| badscore | 差评数据采集 | T-1 | badsscore_status | daily | ✅ 启用 |
| quality | 产品质量数据采集 | T | quality_status | daily | ✅ 启用 |
| kpi_days | 客服绩效数据采集（日度） | T-3 | kpi_days_status | daily | ✅ 启用 |
| chat | 聊天数据采集 | T-1 | chat_status | daily | ✅ 启用 |
| kpi_weekly | 客服绩效数据采集（周度） | T-9到T-3 | kpi_weekly_status | weekly | ⏸️ 待实现 |
| kpi_monthly | 客服绩效数据采集（月度） | 上月 | kpi_monthly_status | monthly | ⏸️ 待实现 |

---

### 2. generate_tasks.py - 任务生成器

**功能：** 根据配置生成待执行任务

**命令行参数：**
```bash
# 生成每日任务
python testyd/pdd/generate_tasks.py --schedule daily

# 生成每周任务
python testyd/pdd/generate_tasks.py --schedule weekly

# 生成每月任务
python testyd/pdd/generate_tasks.py --schedule monthly

# 生成所有任务
python testyd/pdd/generate_tasks.py --schedule all

# 指定日期生成任务（用于补数据）
python testyd/pdd/generate_tasks.py --schedule daily --date 2025-10-01
```

**核心逻辑：**
1. 读取配置文件中启用的任务
2. 根据日期偏移计算目标日期
3. 使用 `INSERT IGNORE` + `UPDATE WHERE` 精确更新状态字段
4. 只更新 NULL 或空值的字段为"待执行"，已完成的保持不变

**调用方式（Python）：**
```python
from testyd.pdd.generate_tasks import generate_tasks_by_schedule

# 生成每日任务
results = generate_tasks_by_schedule('daily')

# 生成指定日期的任务
results = generate_tasks_by_schedule('daily', force_date='2025-10-01')
```

---

### 3. base_crawler.py - 爬虫基类

**功能：** 提供通用的爬虫功能

**主要方法：**

#### 3.1 获取待处理任务
```python
from testyd.pdd.base_crawler import BaseCrawler

class MyCrawler(BaseCrawler):
    def __init__(self):
        super().__init__(
            task_name='my_task',
            status_field='my_task_status',
            target_date='2025-10-03'
        )
    
    def run(self):
        # 获取待处理任务
        pending_tasks = self.get_pending_tasks()
        
        for task in pending_tasks:
            shop_name = task['shop_name']
            cookie = task['cookie']
            
            # 处理任务
            self.process_shop(shop_name, cookie)
```

#### 3.2 更新任务状态
```python
# 更新为已完成
self.update_task_status(shop_name, '已完成')

# 更新为失败（保持待执行状态便于重试）
# 不调用update_task_status，任务保持"待执行"状态
```

#### 3.3 文件合并
```python
# 合并Excel文件
merged_file = self.merge_files(
    source_dir='D:/pdd/差评数据',
    output_dir='D:/pdd/合并文件',
    file_pattern='差评*.xlsx',
    output_filename='差评合并_2025-10-03.xlsx'
)
```

#### 3.4 MinIO上传
```python
# 上传到MinIO
success = self.upload_to_minio(
    file_path=merged_file,
    minio_path='ods/pdd/pdd_badscore',
    date_partition='2025-10-03'
)
```

#### 3.5 Dremio刷新
```python
# 刷新Dremio数据集和反射
self.refresh_dremio(
    table_name='minio.warehouse.ods.pdd.pdd_badscore'
)
```

---

### 4. 数据库接口

**位置：** `testyd/mysql/crawler_db_interface.py`

**主要方法：**

#### 4.1 生成任务
```python
from testyd.mysql.crawler_db_interface import CrawlerDBInterface

db = CrawlerDBInterface()

# 生成任务
created_count = db.generate_tasks(
    time_period='2025-10-03',
    task_columns=['quality_status']
)
```

#### 4.2 获取待处理任务
```python
# 获取待处理任务
pending_tasks = db.get_pending_tasks(
    time_period='2025-10-03',
    status_field='quality_status'
)
```

#### 4.3 更新任务状态
```python
# 更新任务状态
db.update_task_status(
    time_period='2025-10-03',
    shop_name='361南宸专卖店',
    status_field='quality_status',
    status='已完成'
)
```

---

## 🚀 新增爬虫指南

### 步骤1：添加配置

在 `crawler_config.py` 中添加配置：

```python
CRAWLER_TASKS = {
    # ... 现有配置 ...
    
    'new_task': {
        'name': '新任务名称',
        'script': 'pdd_new_task.py',
        'status_field': 'new_task_status',
        'schedule': 'daily',
        'date_offset': -1,  # T-1（昨天）
        'minio_path': 'ods/pdd/pdd_new_task',
        'dremio_table': 'minio.warehouse.ods.pdd.pdd_new_task',
        'enabled': True
    }
}
```

### 步骤2：添加数据库字段

```sql
ALTER TABLE pdd_tasks 
ADD COLUMN new_task_status VARCHAR(20) DEFAULT NULL;
```

### 步骤3：创建爬虫脚本

```python
# pdd_new_task.py
from testyd.pdd.base_crawler import BaseCrawler
from datetime import datetime, timedelta

class NewTaskCrawler(BaseCrawler):
    def __init__(self):
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        super().__init__(
            task_name='新任务',
            status_field='new_task_status',
            target_date=target_date
        )
    
    def process_shop(self, shop_name, cookie):
        """实现具体的数据采集逻辑"""
        # 1. 采集数据
        data = self.crawl_data(shop_name, cookie)
        
        # 2. 保存数据
        self.save_data(data, shop_name)
        
        return True
    
    def run(self):
        """主执行流程"""
        pending_tasks = self.get_pending_tasks()
        
        for task in pending_tasks:
            try:
                success = self.process_shop(task['shop_name'], task['cookie'])
                if success:
                    self.update_task_status(task['shop_name'], '已完成')
            except Exception as e:
                print(f"处理失败: {e}")
                # 保持待执行状态便于重试

if __name__ == '__main__':
    crawler = NewTaskCrawler()
    crawler.run()
```

**完成！** 无需修改任何核心代码！

---

## 📊 任务状态管理

### 状态流转

```
NULL（无任务）
  ↓ 任务生成器
待执行
  ↓ 爬虫程序（成功）
已完成
  
  ↓ 爬虫程序（失败）
待执行（保持不变，便于重试）
```

### 状态说明

| 状态值 | 含义 | 说明 |
|--------|------|------|
| NULL | 无任务 | 该店铺该日期不需要执行此任务 |
| "待执行" | 待处理 | 任务已生成，等待执行 |
| "已完成" | 已完成 | 任务执行成功 |

### 核心设计

**精确的状态管理：**
- 使用 `INSERT IGNORE` + `UPDATE WHERE` 精确控制每个字段
- 不同任务类型的状态字段完全独立
- 例如：`2025-10-02` 只有 `badsscore_status='待执行'`，其他字段为NULL

**示例：**
```
日期         店铺          差评状态    质量状态    绩效状态    聊天状态
2025-10-02  361南宸专卖店  待执行      NULL       NULL       待执行
2025-10-03  361南宸专卖店  NULL       待执行      NULL       NULL
2025-09-30  361南宸专卖店  NULL       NULL       待执行      NULL
```

---

## 🔍 常用操作

### 查询任务状态

```python
from testyd.mysql.crawler_db_interface import CrawlerDBInterface

db = CrawlerDBInterface()
conn = db.pool.connection()
cursor = conn.cursor()

# 查询某个日期的任务状态
cursor.execute("""
    SELECT 
        shop_name,
        quality_status,
        badsscore_status,
        kpi_days_status,
        chat_status
    FROM pdd_tasks
    WHERE time_period = '2025-10-03'
    LIMIT 10
""")

results = cursor.fetchall()
for row in results:
    print(row)

conn.close()
```

### 重置任务状态

```python
# 重置某个日期的某个任务类型
cursor.execute("""
    UPDATE pdd_tasks
    SET quality_status = '待执行'
    WHERE time_period = '2025-10-03'
      AND quality_status = '已完成'
""")
conn.commit()
```

### 清理历史数据

```python
# 删除30天前的数据
cursor.execute("""
    DELETE FROM pdd_tasks
    WHERE time_period < DATE_SUB(CURDATE(), INTERVAL 30 DAY)
""")
conn.commit()
```

---

## 📝 维护指南

### 日常维护

1. **监控任务执行**
   - 检查日志文件
   - 查询数据库任务状态
   - 监控MinIO上传情况

2. **处理失败任务**
   - 失败任务保持"待执行"状态
   - 重新运行爬虫程序即可重试

3. **清理历史数据**
   - 定期清理30天前的任务记录
   - 定期清理本地文件

### 故障排查

**问题1：任务没有生成**
- 检查配置文件中 `enabled` 是否为 `True`
- 检查数据库字段是否存在
- 查看任务生成器日志

**问题2：任务状态没有更新**
- 检查爬虫程序是否调用了 `update_task_status`
- 检查数据库连接是否正常
- 查看爬虫程序日志

**问题3：MinIO上传失败**
- 检查MinIO服务是否运行
- 检查文件路径是否正确
- 检查网络连接

---

## 🎯 最佳实践

1. **配置优先** - 所有配置都在 `crawler_config.py` 中管理
2. **状态驱动** - 通过任务状态控制执行流程
3. **失败重试** - 失败任务保持待执行状态，便于重试
4. **日志记录** - 详细记录执行过程，便于排查问题
5. **定期清理** - 定期清理历史数据，避免数据库膨胀

---

## 📚 相关文档

- `完整使用指南.md` - 详细使用说明
- `README_新架构使用指南.md` - 架构设计文档
- `新架构总结.md` - 架构总结
- `最终验证报告.md` - 测试报告

---

## 📅 更新日志

### 2025-10-03
- ✅ 创建架构文档
- ✅ 添加 chat 任务配置（T-1）
- ✅ 完成124家店铺完整测试
- ✅ 验证任务状态管理逻辑

---

**维护者：** AI Assistant  
**最后更新：** 2025-10-03

