# -*- coding: utf-8 -*-
import json
import os
import requests
import pandas as pd
import time
from pathlib import Path
from tqdm import tqdm
import shutil
from datetime import datetime, timedelta
import sys
import math

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
if hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8')
if hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8')

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(r'D:\testyd')
sys.path.append(r'D:\testyd\mysql')

from merge_excel_files import ExcelMerger
from crawler_db_interface import CrawlerDBInterface

# é…ç½®
BASE_ARCHIVE_DIR = Path('D:/pdd/è¯„ä»·æ–‡ä»¶å­˜æ¡£')  # åŸºç¡€å­˜æ¡£ç›®å½•
MERGED_FILES_DIR = Path('D:/pdd/åˆå¹¶æ–‡ä»¶/è¯„ä»·æ–‡ä»¶å­˜æ¡£')  # åˆå¹¶æ–‡ä»¶å­˜å‚¨ç›®å½•

# MinIO APIé…ç½®
MINIO_API_URL = "http://127.0.0.1:8009/api/upload"
MINIO_BUCKET = "warehouse"

# Dremio APIé…ç½®
DREMIO_API_URL = "http://localhost:8003/api"

# åˆ›å»ºåŸºç¡€å­˜æ¡£ç›®å½•å’Œåˆå¹¶æ–‡ä»¶ç›®å½•
os.makedirs(BASE_ARCHIVE_DIR, exist_ok=True)
os.makedirs(MERGED_FILES_DIR, exist_ok=True)

TODAY_STR = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')


def fetch_reviews_data(cookie: str, shop_name: str):
    """
    è·å–æ‹¼å¤šå¤šå·®è¯„æ•°æ®
    """
    # æ˜¨å¤©çš„å¼€å§‹å’Œç»“æŸæ—¶é—´æˆ³ï¼ˆç§’çº§ï¼‰
    start_time = int(datetime.combine(datetime.now().date() - timedelta(days=1), datetime.min.time()).timestamp())
    end_time = int(datetime.combine(datetime.now().date() - timedelta(days=1), datetime.max.time()).timestamp())
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯
    print(f'ğŸ” æŸ¥è¯¢æ—¥æœŸ: {TODAY_STR}')
    print(f'ğŸ” å¼€å§‹æ—¶é—´æˆ³: {start_time} ({datetime.fromtimestamp(start_time)})')
    print(f'ğŸ” ç»“æŸæ—¶é—´æˆ³: {end_time} ({datetime.fromtimestamp(end_time)})')
    
    url = 'https://mms.pinduoduo.com/saturn/reviews/list'
    
    headers = {
        'accept': '*/*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'anti-content': '',
        'cache-control': 'no-cache',
        'Content-Type': 'application/json',
        'Cookie': cookie,
        'etag': '',
        'origin': 'https://mms.pinduoduo.com',
        'priority': 'u=1, i',
        'referer': 'https://mms.pinduoduo.com/goods/evaluation/index?msfrom=mms_globalsearch',
        'sec-ch-ua': '"Chromium";v="130", "Google Chrome";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'empty',
        'sec-fetch-mode': 'cors',
        'sec-fetch-site': 'same-origin',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'
    }
    
    # é¦–å…ˆè·å–ç¬¬ä¸€é¡µæ•°æ®ï¼Œç¡®å®šæ€»æ•°æ®é‡
    first_page_data = {
        "startTime": start_time,
        "endTime": end_time,
        "pageNo": 1,
        "pageSize": 40,
        "descScore": ["1", "2", "3"],
        "mainReviewContentStatus": "1",
        "orderSn": ""
    }
    
    print(f'ğŸ” æ­£åœ¨è·å– {shop_name} çš„å·®è¯„æ•°æ®...')
    
    try:
        response = requests.post(url, headers=headers, json=first_page_data, timeout=30)
        response.raise_for_status()
        
        # æ‰“å°å®Œæ•´çš„å“åº”å†…å®¹ç”¨äºè°ƒè¯•
        print(f'ğŸ” {shop_name} APIå“åº”çŠ¶æ€ç : {response.status_code}')
        print(f'ğŸ” {shop_name} APIå“åº”å†…å®¹: {response.text[:1000]}...')  # åªæ‰“å°å‰1000å­—ç¬¦
        
        result = response.json()
        print(f'ğŸ” {shop_name} è§£æåçš„JSON: {result}')
        
        if not result.get('success'):
            print(f'[é”™è¯¯] APIè¿”å›å¤±è´¥: {result}')
            raise RuntimeError(f'APIè¿”å›é”™è¯¯: {result.get("error_msg", "æœªçŸ¥é”™è¯¯")}')
        
        total_rows = result.get('result', {}).get('totalRows', 0)
        print(f'ğŸ“Š {shop_name} å…±æœ‰ {total_rows} æ¡å·®è¯„æ•°æ®')
        
        if total_rows == 0:
            return []
        
        # è®¡ç®—éœ€è¦çš„é¡µæ•°
        page_size = 40
        total_pages = math.ceil(total_rows / page_size)
        
        all_data = []
        
        # è·å–æ‰€æœ‰é¡µé¢çš„æ•°æ®
        for page_no in range(1, total_pages + 1):
            page_data = {
                "startTime": start_time,
                "endTime": end_time,
                "pageNo": page_no,
                "pageSize": page_size,
                "descScore": ["1", "2", "3"],
                "mainReviewContentStatus": "1",
                "orderSn": ""
            }
            
            print(f'ğŸ“„ æ­£åœ¨è·å–ç¬¬ {page_no}/{total_pages} é¡µæ•°æ®...')
            
            page_response = requests.post(url, headers=headers, json=page_data, timeout=30)
            page_response.raise_for_status()
            
            print(f'ğŸ“„ ç¬¬ {page_no} é¡µå“åº”çŠ¶æ€ç : {page_response.status_code}')
            print(f'ğŸ“„ ç¬¬ {page_no} é¡µå“åº”å†…å®¹: {page_response.text[:500]}...')
            
            page_result = page_response.json()
            print(f'ğŸ“„ ç¬¬ {page_no} é¡µè§£æåçš„JSON: {page_result}')
            
            if page_result.get('success'):
                page_items = page_result.get('result', {}).get('data', [])
                print(f'ğŸ“„ ç¬¬ {page_no} é¡µè·å–åˆ° {len(page_items)} æ¡æ•°æ®')
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ŒæŸ¥çœ‹æ•°æ®ç»“æ„
                if page_items:
                    print(f'ğŸ“„ ç¬¬ä¸€æ¡æ•°æ®ç»“æ„: {page_items[0]}')
                    if 'orderSnapshotInfo' in page_items[0]:
                        print(f'ğŸ“„ orderSnapshotInfoå†…å®¹: {page_items[0]["orderSnapshotInfo"]}')
                
                all_data.extend(page_items)
            else:
                print(f'[è­¦å‘Š] ç¬¬ {page_no} é¡µè·å–å¤±è´¥: {page_result.get("error_msg", "æœªçŸ¥é”™è¯¯")}')
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        print(f'[æˆåŠŸ] {shop_name} æ•°æ®è·å–å®Œæˆï¼Œå…± {len(all_data)} æ¡è®°å½•')
        return all_data
    except Exception as e:
        print(f'[é”™è¯¯] {shop_name} æ•°æ®è·å–å¤±è´¥: {str(e)}')
        raise


def parse_reviews_data(data_list):
    """
    è§£æå·®è¯„æ•°æ®ï¼Œè½¬æ¢ä¸ºExcelæ ¼å¼
    """
    if not data_list:
        return []
    
    keys = [
        'descScore',           # ç”¨æˆ·è¯„ä»·åˆ†
        'comment',             # ç”¨æˆ·è¯„è®º
        'orderSn',             # è®¢å•ç¼–å·
        'name',                # å–å®¶æ˜µç§°ï¼ˆåœ¨ orderSnapshotInfo é‡Œï¼‰
        'goodsId',             # å•†å“ ID
        'goodsInfoUrl'         # é¡µè¿”å›é“¾æ¥ï¼ˆå•†å“é¡µï¼‰
    ]
    
    table = [keys]
    max_pictures = 0  # ç”¨äºè®°å½•æœ€å¤§å›¾ç‰‡æ•°é‡
    
    for item in data_list:
        row = []
        current_pictures = 0  # å½“å‰æ¡ç›®ä¸­çš„å›¾ç‰‡æ•°é‡
        
        # éå†æ¯ä¸ªå­—æ®µ
        for k in keys:
            if k == 'name':
                # nameå­—æ®µç›´æ¥åœ¨itemä¸­ï¼Œä¸åœ¨orderSnapshotInfoä¸­
                name_value = item.get('name', '')
                # ä¿®å¤ä»¥=å¼€å¤´çš„nameå€¼ï¼Œåœ¨å‰é¢åŠ ä¸Šå•å¼•å·é˜²æ­¢Excelè§£æä¸ºå…¬å¼
                if isinstance(name_value, str) and name_value.startswith('='):
                    name_value = "'" + name_value
                row.append(name_value)
            else:
                row.append(item.get(k, ''))
        
        # å¤„ç†å›¾ç‰‡é“¾æ¥ï¼Œæ¯ä¸ªå›¾ç‰‡å•ç‹¬ä¸€åˆ—
        pics = item.get('pictures', []) or []
        current_pictures = len(pics)
        if current_pictures > max_pictures:
            max_pictures = current_pictures
        for pic in pics:
            row.append(pic.get('url', ''))
        
        # å¦‚æœå½“å‰å›¾ç‰‡æ•°é‡å°‘äºæœ€å¤§å€¼ï¼Œå¡«å……ç©ºå­—ç¬¦ä¸²ä»¥å¯¹é½
        if current_pictures < max_pictures:
            row.extend([''] * (max_pictures - current_pictures))
        
        table.append(row)
    
    # æ›´æ–°è¡¨å¤´ä»¥åŒ…å«å›¾ç‰‡åˆ—
    table[0].extend([f'Picture_{i+1}' for i in range(max_pictures)])
    
    return table


def save_to_excel(table_data, shop_name, date_str=None):
    """
    å°†æ•°æ®ä¿å­˜åˆ°Excelæ–‡ä»¶
    """
    if date_str is None:
        date_str = TODAY_STR
    
    # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„ï¼šD:\pdd\è¯„ä»·æ–‡ä»¶å­˜æ¡£\2025-09-25\
    date_dir = BASE_ARCHIVE_DIR / date_str
    date_dir.mkdir(parents=True, exist_ok=True)
    
    # æ–‡ä»¶ä¿å­˜è·¯å¾„ï¼šD:\pdd\è¯„ä»·æ–‡ä»¶å­˜æ¡£\2025-09-25\åº—é“ºåç§°.xlsx
    file_name = f'{shop_name}.xlsx'
    save_path = date_dir / file_name
    
    # å¦‚æœç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
    if save_path.exists():
        print(f'ğŸ—‘ï¸  å‘ç°åŒåæ–‡ä»¶ï¼Œæ­£åœ¨åˆ é™¤: {save_path}')
        save_path.unlink()  # åˆ é™¤æ–‡ä»¶
        print(f'[æˆåŠŸ] åŒåæ–‡ä»¶å·²åˆ é™¤')
    
    # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
    if table_data:
        df = pd.DataFrame(table_data[1:], columns=table_data[0])
        df.to_excel(save_path, index=False, engine='openpyxl')
        print(f'ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {save_path}')
        return save_path
    else:
        print(f'[è­¦å‘Š] {shop_name} æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜')
        return None


def upload_merged_file_to_minio(merged_file_path: str, date_str: str = None) -> bool:
    """
    å°†åˆå¹¶åçš„Excelæ–‡ä»¶è½¬æ¢ä¸ºParquetæ ¼å¼å¹¶ä¸Šä¼ åˆ°MinIO
    
    Args:
        merged_file_path: åˆå¹¶åçš„Excelæ–‡ä»¶è·¯å¾„
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œé»˜è®¤ä½¿ç”¨æ˜¨å¤©
    
    Returns:
        bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
    """
    if date_str is None:
        date_str = TODAY_STR
    
    try:
        # è¯»å–åˆå¹¶åçš„Excelæ–‡ä»¶
        df = pd.read_excel(merged_file_path)
        
        # æ¸…ç†NaNå€¼ï¼Œå°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²æˆ–None
        df = df.fillna('')  # å°†æ‰€æœ‰NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
        
        # æ„å»ºMinIOè·¯å¾„ï¼šods/pdd/pdd_badscore/dt=æ—¥æœŸ/merged_badscore_data.parquet
        minio_path = f"ods/pdd/pdd_badscore/dt={date_str}/merged_badscore_data.parquet"
        
        # å‡†å¤‡ä¸Šä¼ æ•°æ®
        upload_data = {
            "data": df.to_dict('records'),  # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            "target_path": minio_path,
            "format": "parquet",
            "bucket": MINIO_BUCKET
        }
        
        # å‘é€POSTè¯·æ±‚åˆ°MinIO API
        headers = {'Content-Type': 'application/json'}
        response = requests.post(MINIO_API_URL, json=upload_data, headers=headers)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print(f"[æˆåŠŸ] æˆåŠŸä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO: {minio_path}")
                return True
            else:
                print(f"[é”™è¯¯] MinIOä¸Šä¼ å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return False
        else:
            print(f"[é”™è¯¯] MinIO APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        print(f"[é”™è¯¯] ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIOæ—¶å‡ºé”™: {str(e)}")
        return False


if __name__ == '__main__':
    print("=" * 60)
    print(f"æ‹¼å¤šå¤šå·®è¯„æ•°æ®é‡‡é›† - {TODAY_STR}")
    print("=" * 60)

    # åˆå§‹åŒ–æ•°æ®åº“æ¥å£
    db_interface = CrawlerDBInterface(
        platform='pdd',
        shops_table='pdd_shops',
        tasks_table='pdd_tasks',
        database='company'
    )

    # è·å–å¾…å¤„ç†ä»»åŠ¡ï¼ˆä¸ç”Ÿæˆä»»åŠ¡ï¼Œç”± generate_daily_tasks.py ç»Ÿä¸€ç”Ÿæˆï¼‰
    print(f"\n=== è·å–å¾…å¤„ç†ä»»åŠ¡ ===")
    pending_tasks = db_interface.get_pending_tasks(TODAY_STR, 'badsscore_status')

    if not pending_tasks:
        print("âœ“ æ²¡æœ‰å¾…å¤„ç†ä»»åŠ¡ï¼Œæ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        print("\næç¤ºï¼šå¦‚éœ€é‡æ–°æ‰§è¡Œï¼Œè¯·å…ˆè¿è¡Œ generate_daily_tasks.py ç”Ÿæˆä»»åŠ¡")
        import sys
        sys.exit(0)

    print(f"æ‰¾åˆ° {len(pending_tasks)} ä¸ªå¾…å¤„ç†ä»»åŠ¡")

    # 3. æ‰¹é‡è·å–æ‰€æœ‰åº—é“ºçš„å·®è¯„æ•°æ®
    downloaded_files = []
    success_count = 0

    for task in pending_tasks:
        shop_name = task[1] if len(task) > 1 else None  # dt.shop_name
        cookie = task[11] if len(task) > 11 else None  # s.cookie (ç´¢å¼•11)

        if not cookie:
            print(f'[è­¦å‘Š] {shop_name}  cookie ä¸ºç©ºï¼Œè·³è¿‡')
            continue

        print(f"\n=== å¤„ç†åº—é“º: {shop_name} ===")

        try:
            # è·å–å·®è¯„æ•°æ®
            reviews_data = fetch_reviews_data(cookie, shop_name)

            if reviews_data:
                # è§£ææ•°æ®
                table_data = parse_reviews_data(reviews_data)

                # ä¿å­˜åˆ°Excel
                save_path = save_to_excel(table_data, shop_name)
                if save_path:
                    downloaded_files.append(save_path)
                    print(f'[æˆåŠŸ] {shop_name}  å·®è¯„æ•°æ®å¤„ç†å®Œæˆ: {save_path}')
                    success_count += 1
                else:
                    print(f'[è­¦å‘Š] {shop_name}  æ²¡æœ‰æ•°æ®ä¿å­˜')
            else:
                print(f'â„¹ï¸  {shop_name}  æ²¡æœ‰å·®è¯„æ•°æ®')

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²å®Œæˆ
            db_interface.update_task_status(TODAY_STR, shop_name, 'badsscore_status', 'å·²å®Œæˆ')

        except Exception as e:
            # å¤±è´¥æ—¶ä¸æ›´æ–°çŠ¶æ€ï¼Œä¿æŒå¾…æ‰§è¡ŒçŠ¶æ€ä¾¿äºé‡è¯•
            print(f'[é”™è¯¯] {shop_name}  å¤±è´¥ï¼š{e}ï¼Œä¿æŒå¾…æ‰§è¡ŒçŠ¶æ€')

    print(f"\n=== æ•°æ®å¤„ç†å®Œæˆ ===")
    print(f"æˆåŠŸå¤„ç†: {success_count}/{len(pending_tasks)} ä¸ªåº—é“º")

    # 4. å¦‚æœæœ‰ä¸‹è½½çš„æ–‡ä»¶ï¼Œè¿›è¡Œåˆå¹¶å’Œä¸Šä¼ 
    if downloaded_files:
        print(f'\nğŸ”„ å¼€å§‹åˆå¹¶ {len(downloaded_files)} ä¸ªExcelæ–‡ä»¶...')

        # åˆ›å»ºæ—¥æœŸæ–‡ä»¶å¤¹è·¯å¾„
        date_dir = BASE_ARCHIVE_DIR / TODAY_STR

        # ä½¿ç”¨ExcelMergeråˆå¹¶æ–‡ä»¶
        merger = ExcelMerger(str(date_dir), output_dir=str(MERGED_FILES_DIR))
        merge_success = merger.merge_excel_files(f"{TODAY_STR}.xlsx")

        if merge_success:
            # åˆå¹¶åçš„æ–‡ä»¶ç›´æ¥åœ¨æœ€ç»ˆç›®å½•ä¸­
            final_merged_file_path = MERGED_FILES_DIR / f"{TODAY_STR}.xlsx"

            print(f'[æˆåŠŸ] æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³: {final_merged_file_path}')

            # ä¸Šä¼ åˆå¹¶åçš„æ–‡ä»¶åˆ°MinIO
            print(f'ğŸ”„ æ­£åœ¨ä¸Šä¼ åˆå¹¶æ–‡ä»¶åˆ°MinIO...')
            upload_success = upload_merged_file_to_minio(str(final_merged_file_path), TODAY_STR)

            if upload_success:
                print(f'[æˆåŠŸ] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ æˆåŠŸ')
            else:
                print(f'[è­¦å‘Š] åˆå¹¶æ–‡ä»¶MinIOä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°æ–‡ä»¶å·²ä¿å­˜')
        else:
            print('[é”™è¯¯] æ–‡ä»¶åˆå¹¶å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆåˆå¹¶æ–‡ä»¶')
    else:
        print('[è­¦å‘Š] æ²¡æœ‰æ–°ä¸‹è½½çš„æ–‡ä»¶ï¼Œè·³è¿‡åˆå¹¶å’Œä¸Šä¼ æ­¥éª¤')

    # 5. æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ å®Œæˆåï¼Œåˆ·æ–°æ•°æ®é›†å’Œåå°„
    print('\nğŸ”„ æ­£åœ¨åˆ·æ–°Dremioæ•°æ®é›†å’Œåå°„...')
    try:
        refresh_dataset_response = requests.post(
            f"{DREMIO_API_URL}/dataset/refresh-metadata",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_badscore"}
        )
        if refresh_dataset_response.status_code == 200:
            print('[æˆåŠŸ] æ•°æ®é›†åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] æ•°æ®é›†åˆ·æ–°å¤±è´¥: {refresh_dataset_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] æ•°æ®é›†åˆ·æ–°å¼‚å¸¸: {e}')

    try:
        refresh_reflection_response = requests.post(
            f"{DREMIO_API_URL}/reflection/refresh",
            headers={"Content-Type": "application/json"},
            json={"dataset_path": "minio.warehouse.ods.pdd.pdd_badscore"}
        )
        if refresh_reflection_response.status_code == 200:
            print('[æˆåŠŸ] åå°„åˆ·æ–°æˆåŠŸ')
        else:
            print(f'[è­¦å‘Š] åå°„åˆ·æ–°å¤±è´¥: {refresh_reflection_response.status_code}')
    except Exception as e:
        print(f'[é”™è¯¯] åå°„åˆ·æ–°å¼‚å¸¸: {e}')

    print('\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼')