# ğŸ† Prefect æœ€ä½³å®è·µæŒ‡å—

## ğŸ“‹ ç›®å½•
1. [é¡¹ç›®ç»“æ„æœ€ä½³å®è·µ](#é¡¹ç›®ç»“æ„æœ€ä½³å®è·µ)
2. [ä»£ç ç»„ç»‡å’Œè®¾è®¡æ¨¡å¼](#ä»£ç ç»„ç»‡å’Œè®¾è®¡æ¨¡å¼)
3. [é”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥](#é”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥)
4. [æ€§èƒ½ä¼˜åŒ–æŠ€å·§](#æ€§èƒ½ä¼˜åŒ–æŠ€å·§)
5. [ç›‘æ§å’Œæ—¥å¿—è®°å½•](#ç›‘æ§å’Œæ—¥å¿—è®°å½•)
6. [éƒ¨ç½²å’Œè¿ç»´æœ€ä½³å®è·µ](#éƒ¨ç½²å’Œè¿ç»´æœ€ä½³å®è·µ)
7. [å®‰å…¨æ€§è€ƒè™‘](#å®‰å…¨æ€§è€ƒè™‘)
8. [æµ‹è¯•ç­–ç•¥](#æµ‹è¯•ç­–ç•¥)
9. [å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ](#å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ)

---

## ğŸ“ é¡¹ç›®ç»“æ„æœ€ä½³å®è·µ

### ğŸ—ï¸ æ¨èçš„é¡¹ç›®ç»“æ„
```
prefect_project/
â”œâ”€â”€ flows/                    # æµç¨‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing/      # æ•°æ®å¤„ç†æµç¨‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ etl_flows.py
â”‚   â”‚   â””â”€â”€ validation_flows.py
â”‚   â”œâ”€â”€ reporting/            # æŠ¥å‘Šç”Ÿæˆæµç¨‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ report_flows.py
â”‚   â””â”€â”€ monitoring/           # ç›‘æ§æµç¨‹
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ health_check_flows.py
â”œâ”€â”€ tasks/                    # ä»»åŠ¡å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_tasks.py
â”‚   â”œâ”€â”€ api_tasks.py
â”‚   â””â”€â”€ file_tasks.py
â”œâ”€â”€ utils/                    # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â””â”€â”€ helpers.py
â”œâ”€â”€ config/                   # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ deployment_configs.py
â”œâ”€â”€ tests/                    # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_flows.py
â”‚   â””â”€â”€ test_tasks.py
â”œâ”€â”€ deployments/              # éƒ¨ç½²è„šæœ¬
â”‚   â”œâ”€â”€ create_deployments.py
â”‚   â””â”€â”€ deployment_configs.yaml
â”œâ”€â”€ scripts/                  # ç®¡ç†è„šæœ¬
â”‚   â”œâ”€â”€ service_manager.py
â”‚   â””â”€â”€ health_check.py
â”œâ”€â”€ requirements.txt          # ä¾èµ–ç®¡ç†
â”œâ”€â”€ .env                      # ç¯å¢ƒå˜é‡
â”œâ”€â”€ .gitignore               # Gitå¿½ç•¥æ–‡ä»¶
â””â”€â”€ README.md                # é¡¹ç›®æ–‡æ¡£
```

### ğŸ“ æ–‡ä»¶å‘½åçº¦å®š
```python
# âœ… å¥½çš„å‘½å
data_processing_flow.py
user_authentication_task.py
database_connection_utils.py

# âŒ é¿å…çš„å‘½å
flow1.py
task.py
utils.py
```

---

## ğŸ¯ ä»£ç ç»„ç»‡å’Œè®¾è®¡æ¨¡å¼

### ğŸ”§ ä»»åŠ¡è®¾è®¡åŸåˆ™

#### 1. å•ä¸€èŒè´£åŸåˆ™
```python
# âœ… å¥½çš„è®¾è®¡ - æ¯ä¸ªä»»åŠ¡åªåšä¸€ä»¶äº‹
@task(name="è¯»å–CSVæ–‡ä»¶")
def read_csv_file(file_path: str) -> pd.DataFrame:
    return pd.read_csv(file_path)

@task(name="æ•°æ®æ¸…æ´—")
def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    return df.dropna()

@task(name="ä¿å­˜åˆ°æ•°æ®åº“")
def save_to_database(df: pd.DataFrame, table_name: str) -> int:
    return df.to_sql(table_name, connection)

# âŒ é¿å…çš„è®¾è®¡ - ä»»åŠ¡èŒè´£è¿‡å¤š
@task(name="å¤„ç†æ‰€æœ‰æ•°æ®")
def process_all_data(file_path: str, table_name: str):
    df = pd.read_csv(file_path)
    df = df.dropna()
    df.to_sql(table_name, connection)
    # å¤ªå¤šèŒè´£åœ¨ä¸€ä¸ªä»»åŠ¡ä¸­
```

#### 2. å‚æ•°åŒ–è®¾è®¡
```python
# âœ… å¥½çš„è®¾è®¡ - é«˜åº¦å‚æ•°åŒ–
@task(name="APIæ•°æ®è·å–")
def fetch_api_data(
    url: str,
    headers: Optional[Dict] = None,
    timeout: int = 30,
    retries: int = 3
) -> Dict:
    # å®ç°ç»†èŠ‚
    pass

# âŒ é¿å…çš„è®¾è®¡ - ç¡¬ç¼–ç 
@task(name="è·å–ç”¨æˆ·æ•°æ®")
def fetch_user_data():
    url = "https://api.example.com/users"  # ç¡¬ç¼–ç 
    # å®ç°ç»†èŠ‚
    pass
```

### ğŸŒŠ æµç¨‹è®¾è®¡æ¨¡å¼

#### 1. ç®¡é“æ¨¡å¼
```python
@flow(name="æ•°æ®å¤„ç†ç®¡é“")
def data_pipeline(input_file: str, output_table: str):
    # çº¿æ€§å¤„ç†ç®¡é“
    raw_data = extract_data(input_file)
    clean_data = transform_data(raw_data)
    result = load_data(clean_data, output_table)
    return result
```

#### 2. æ‰‡å‡º-æ‰‡å…¥æ¨¡å¼
```python
@flow(name="å¹¶è¡Œå¤„ç†æµç¨‹")
def parallel_processing_flow(data_sources: List[str]):
    # æ‰‡å‡º - å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æº
    results = []
    for source in data_sources:
        result = process_data_source.submit(source)
        results.append(result)
    
    # æ‰‡å…¥ - åˆå¹¶ç»“æœ
    combined_result = combine_results(results)
    return combined_result
```

#### 3. æ¡ä»¶åˆ†æ”¯æ¨¡å¼
```python
@flow(name="æ¡ä»¶å¤„ç†æµç¨‹")
def conditional_flow(data_type: str, data: Any):
    if data_type == "csv":
        result = process_csv_data(data)
    elif data_type == "json":
        result = process_json_data(data)
    else:
        result = process_generic_data(data)
    
    return validate_result(result)
```

---

## ğŸ”„ é”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥

### ğŸ›¡ï¸ é‡è¯•ç­–ç•¥æœ€ä½³å®è·µ

#### 1. åˆ†å±‚é‡è¯•ç­–ç•¥
```python
# ç½‘ç»œè¯·æ±‚ - å¿«é€Ÿé‡è¯•
@task(name="APIè¯·æ±‚", retries=3, retry_delay_seconds=2)
def api_request(url: str) -> Dict:
    pass

# æ•°æ®åº“æ“ä½œ - ä¸­ç­‰é‡è¯•
@task(name="æ•°æ®åº“å†™å…¥", retries=2, retry_delay_seconds=10)
def database_write(data: pd.DataFrame) -> int:
    pass

# æ–‡ä»¶æ“ä½œ - é•¿é‡è¯•é—´éš”
@task(name="æ–‡ä»¶ä¸Šä¼ ", retries=5, retry_delay_seconds=30)
def file_upload(file_path: str) -> str:
    pass
```

#### 2. æŒ‡æ•°é€€é¿é‡è¯•
```python
from prefect.tasks import task_input_hash
from datetime import timedelta

@task(
    name="æŒ‡æ•°é€€é¿ä»»åŠ¡",
    retries=5,
    retry_delay_seconds=[1, 2, 4, 8, 16]  # æŒ‡æ•°é€€é¿
)
def exponential_backoff_task(data: Any) -> Any:
    # ä»»åŠ¡å®ç°
    pass
```

#### 3. æ¡ä»¶é‡è¯•
```python
@task(name="æ¡ä»¶é‡è¯•ä»»åŠ¡")
def conditional_retry_task(data: Any) -> Any:
    try:
        # ä»»åŠ¡é€»è¾‘
        result = process_data(data)
        return result
    except TemporaryError as e:
        # ä¸´æ—¶é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
        logger.warning(f"ä¸´æ—¶é”™è¯¯ï¼Œå°†é‡è¯•: {e}")
        raise
    except PermanentError as e:
        # æ°¸ä¹…é”™è¯¯ï¼Œä¸åº”é‡è¯•
        logger.error(f"æ°¸ä¹…é”™è¯¯ï¼Œåœæ­¢é‡è¯•: {e}")
        return None
```

### ğŸš¨ é”™è¯¯å¤„ç†æ¨¡å¼

#### 1. ä¼˜é›…é™çº§
```python
@task(name="ä¼˜é›…é™çº§ä»»åŠ¡")
def graceful_degradation_task(primary_source: str, fallback_source: str) -> Any:
    try:
        return fetch_from_primary(primary_source)
    except Exception as e:
        logger.warning(f"ä¸»æ•°æ®æºå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®æº: {e}")
        return fetch_from_fallback(fallback_source)
```

#### 2. æ–­è·¯å™¨æ¨¡å¼
```python
class CircuitBreaker:
    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

@task(name="æ–­è·¯å™¨ä»»åŠ¡")
def circuit_breaker_task(data: Any, circuit_breaker: CircuitBreaker) -> Any:
    if circuit_breaker.state == "OPEN":
        if time.time() - circuit_breaker.last_failure_time > circuit_breaker.timeout:
            circuit_breaker.state = "HALF_OPEN"
        else:
            raise Exception("æ–­è·¯å™¨å¼€å¯ï¼Œè·³è¿‡æ‰§è¡Œ")
    
    try:
        result = risky_operation(data)
        circuit_breaker.failure_count = 0
        circuit_breaker.state = "CLOSED"
        return result
    except Exception as e:
        circuit_breaker.failure_count += 1
        circuit_breaker.last_failure_time = time.time()
        
        if circuit_breaker.failure_count >= circuit_breaker.failure_threshold:
            circuit_breaker.state = "OPEN"
        
        raise
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### ğŸš€ å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

#### 1. é€‰æ‹©åˆé€‚çš„ä»»åŠ¡è¿è¡Œå™¨
```python
from prefect.task_runners import ConcurrentTaskRunner, SequentialTaskRunner

# CPUå¯†é›†å‹ä»»åŠ¡ - ä½¿ç”¨è¿›ç¨‹æ± 
@flow(task_runner=ConcurrentTaskRunner())
def cpu_intensive_flow():
    pass

# I/Oå¯†é›†å‹ä»»åŠ¡ - ä½¿ç”¨çº¿ç¨‹æ± 
@flow(task_runner=ConcurrentTaskRunner(max_workers=10))
def io_intensive_flow():
    pass

# éœ€è¦é¡ºåºæ‰§è¡Œçš„ä»»åŠ¡
@flow(task_runner=SequentialTaskRunner())
def sequential_flow():
    pass
```

#### 2. æ‰¹å¤„ç†ä¼˜åŒ–
```python
# âœ… å¥½çš„è®¾è®¡ - æ‰¹å¤„ç†
@task(name="æ‰¹é‡æ•°æ®å¤„ç†")
def batch_process_data(data_batch: List[Any], batch_size: int = 100) -> List[Any]:
    results = []
    for i in range(0, len(data_batch), batch_size):
        batch = data_batch[i:i + batch_size]
        batch_result = process_batch(batch)
        results.extend(batch_result)
    return results

# âŒ é¿å…çš„è®¾è®¡ - é€ä¸ªå¤„ç†
@task(name="å•ä¸ªæ•°æ®å¤„ç†")
def single_process_data(data_item: Any) -> Any:
    return process_single_item(data_item)
```

### ğŸ’¾ å†…å­˜ç®¡ç†

#### 1. æµå¼å¤„ç†
```python
@task(name="æµå¼æ•°æ®å¤„ç†")
def stream_process_large_file(file_path: str, chunk_size: int = 10000) -> int:
    total_processed = 0
    
    for chunk in pd.read_csv(file_path, chunksize=chunk_size):
        processed_chunk = process_chunk(chunk)
        save_chunk(processed_chunk)
        total_processed += len(chunk)
        
        # é‡Šæ”¾å†…å­˜
        del chunk, processed_chunk
    
    return total_processed
```

#### 2. ç»“æœç¼“å­˜
```python
from prefect.tasks import task_input_hash

@task(
    name="ç¼“å­˜ä»»åŠ¡",
    cache_key_fn=task_input_hash,
    cache_expiration=timedelta(hours=1)
)
def expensive_computation(input_data: str) -> str:
    # æ˜‚è´µçš„è®¡ç®—æ“ä½œ
    time.sleep(10)
    return f"processed_{input_data}"
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—è®°å½•

### ğŸ“ æ—¥å¿—è®°å½•æœ€ä½³å®è·µ

#### 1. ç»“æ„åŒ–æ—¥å¿—
```python
import structlog
from prefect import get_run_logger

@task(name="ç»“æ„åŒ–æ—¥å¿—ä»»åŠ¡")
def structured_logging_task(user_id: str, action: str) -> Dict:
    logger = get_run_logger()
    
    # ç»“æ„åŒ–æ—¥å¿—è®°å½•
    logger.info(
        "ç”¨æˆ·æ“ä½œå¼€å§‹",
        extra={
            "user_id": user_id,
            "action": action,
            "timestamp": datetime.now().isoformat(),
            "component": "data_processor"
        }
    )
    
    try:
        result = perform_action(user_id, action)
        
        logger.info(
            "ç”¨æˆ·æ“ä½œæˆåŠŸ",
            extra={
                "user_id": user_id,
                "action": action,
                "result_size": len(result),
                "duration_ms": 1500
            }
        )
        
        return result
        
    except Exception as e:
        logger.error(
            "ç”¨æˆ·æ“ä½œå¤±è´¥",
            extra={
                "user_id": user_id,
                "action": action,
                "error": str(e),
                "error_type": type(e).__name__
            }
        )
        raise
```

#### 2. æ€§èƒ½ç›‘æ§
```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        logger = get_run_logger()
        start_time = time.time()
        
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            
            logger.info(
                f"ä»»åŠ¡ {func.__name__} æ‰§è¡ŒæˆåŠŸ",
                extra={
                    "duration_seconds": duration,
                    "function_name": func.__name__,
                    "args_count": len(args),
                    "kwargs_count": len(kwargs)
                }
            )
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(
                f"ä»»åŠ¡ {func.__name__} æ‰§è¡Œå¤±è´¥",
                extra={
                    "duration_seconds": duration,
                    "function_name": func.__name__,
                    "error": str(e)
                }
            )
            raise
    
    return wrapper

@task(name="æ€§èƒ½ç›‘æ§ä»»åŠ¡")
@monitor_performance
def monitored_task(data: Any) -> Any:
    # ä»»åŠ¡å®ç°
    return process_data(data)
```

### ğŸ“ˆ æŒ‡æ ‡æ”¶é›†

#### 1. è‡ªå®šä¹‰æŒ‡æ ‡
```python
from prefect.artifacts import create_table_artifact, create_markdown_artifact

@task(name="æŒ‡æ ‡æ”¶é›†ä»»åŠ¡")
def collect_metrics_task(processing_results: List[Dict]) -> Dict:
    # è®¡ç®—æŒ‡æ ‡
    total_records = sum(r['record_count'] for r in processing_results)
    avg_processing_time = sum(r['duration'] for r in processing_results) / len(processing_results)
    success_rate = sum(1 for r in processing_results if r['status'] == 'success') / len(processing_results)
    
    metrics = {
        "total_records": total_records,
        "avg_processing_time": avg_processing_time,
        "success_rate": success_rate,
        "timestamp": datetime.now().isoformat()
    }
    
    # åˆ›å»ºæŒ‡æ ‡æŠ¥å‘Š
    create_table_artifact(
        key="processing-metrics",
        table=processing_results,
        description="å¤„ç†ç»“æœè¯¦ç»†æŒ‡æ ‡"
    )
    
    create_markdown_artifact(
        key="metrics-summary",
        markdown=f"""
# ğŸ“Š å¤„ç†æŒ‡æ ‡æ‘˜è¦

- **æ€»è®°å½•æ•°**: {total_records:,}
- **å¹³å‡å¤„ç†æ—¶é—´**: {avg_processing_time:.2f} ç§’
- **æˆåŠŸç‡**: {success_rate:.1%}
- **ç”Ÿæˆæ—¶é—´**: {metrics['timestamp']}
        """,
        description="æŒ‡æ ‡æ‘˜è¦æŠ¥å‘Š"
    )
    
    return metrics
```

---

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´æœ€ä½³å®è·µ

### ğŸ—ï¸ éƒ¨ç½²ç­–ç•¥

#### 1. ç¯å¢ƒåˆ†ç¦»
```python
# config/settings.py
import os
from enum import Enum

class Environment(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION = "production"

class Settings:
    def __init__(self):
        self.environment = Environment(os.getenv("ENVIRONMENT", "development"))
        self.api_url = self._get_api_url()
        self.database_url = self._get_database_url()
        self.log_level = self._get_log_level()
    
    def _get_api_url(self) -> str:
        urls = {
            Environment.DEVELOPMENT: "http://localhost:4200/api",
            Environment.STAGING: "http://staging-prefect.company.com/api",
            Environment.PRODUCTION: "http://prefect.company.com/api"
        }
        return urls[self.environment]
    
    def _get_database_url(self) -> str:
        if self.environment == Environment.PRODUCTION:
            return os.getenv("PROD_DATABASE_URL")
        elif self.environment == Environment.STAGING:
            return os.getenv("STAGING_DATABASE_URL")
        else:
            return "sqlite:///dev.db"
    
    def _get_log_level(self) -> str:
        levels = {
            Environment.DEVELOPMENT: "DEBUG",
            Environment.STAGING: "INFO",
            Environment.PRODUCTION: "WARNING"
        }
        return levels[self.environment]

settings = Settings()
```

#### 2. é…ç½®ç®¡ç†
```python
# deployments/create_deployments.py
from config.settings import settings, Environment

def create_environment_specific_deployments():
    if settings.environment == Environment.PRODUCTION:
        # ç”Ÿäº§ç¯å¢ƒ - ä¿å®ˆçš„è°ƒåº¦
        schedule = CronSchedule(cron="0 2 * * *", timezone="Asia/Shanghai")
        tags = ["ç”Ÿäº§", "å…³é”®ä¸šåŠ¡"]
        
    elif settings.environment == Environment.STAGING:
        # æµ‹è¯•ç¯å¢ƒ - é¢‘ç¹æµ‹è¯•
        schedule = CronSchedule(cron="0 */2 * * *", timezone="Asia/Shanghai")
        tags = ["æµ‹è¯•", "éªŒè¯"]
        
    else:
        # å¼€å‘ç¯å¢ƒ - æ‰‹åŠ¨æ‰§è¡Œ
        schedule = None
        tags = ["å¼€å‘", "è°ƒè¯•"]
    
    deployment = Deployment.build_from_flow(
        flow=data_processing_flow,
        name=f"æ•°æ®å¤„ç†-{settings.environment.value}",
        schedule=schedule,
        tags=tags,
        parameters={
            "api_url": settings.api_url,
            "database_url": settings.database_url
        }
    )
    
    deployment.apply()
```

### ğŸ”§ æœåŠ¡ç®¡ç†

#### 1. å¥åº·æ£€æŸ¥
```python
# scripts/health_check.py
import requests
import sys
from datetime import datetime, timedelta

def check_prefect_server_health():
    """æ£€æŸ¥ Prefect æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
    try:
        response = requests.get("http://localhost:4200/api/health", timeout=10)
        if response.status_code == 200:
            print("âœ… Prefect æœåŠ¡å™¨è¿è¡Œæ­£å¸¸")
            return True
        else:
            print(f"âŒ Prefect æœåŠ¡å™¨å“åº”å¼‚å¸¸: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Prefect æœåŠ¡å™¨: {e}")
        return False

def check_recent_flow_runs():
    """æ£€æŸ¥æœ€è¿‘çš„æµç¨‹è¿è¡ŒçŠ¶æ€"""
    try:
        # æ£€æŸ¥æœ€è¿‘1å°æ—¶çš„æµç¨‹è¿è¡Œ
        since = datetime.now() - timedelta(hours=1)
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ Prefect API è·å–æµç¨‹è¿è¡ŒçŠ¶æ€
        # ç®€åŒ–ç¤ºä¾‹
        print("âœ… æœ€è¿‘æµç¨‹è¿è¡ŒçŠ¶æ€æ­£å¸¸")
        return True
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥æµç¨‹è¿è¡ŒçŠ¶æ€å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å¥åº·æ£€æŸ¥å‡½æ•°"""
    checks = [
        ("Prefect æœåŠ¡å™¨", check_prefect_server_health),
        ("æµç¨‹è¿è¡ŒçŠ¶æ€", check_recent_flow_runs)
    ]
    
    all_healthy = True
    
    for check_name, check_func in checks:
        print(f"ğŸ” æ£€æŸ¥ {check_name}...")
        if not check_func():
            all_healthy = False
    
    if all_healthy:
        print("\nğŸ‰ æ‰€æœ‰å¥åº·æ£€æŸ¥é€šè¿‡ï¼")
        sys.exit(0)
    else:
        print("\nâš ï¸ å‘ç°å¥åº·é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        sys.exit(1)

if __name__ == "__main__":
    main()
```

---

## ğŸ”’ å®‰å…¨æ€§è€ƒè™‘

### ğŸ” å¯†é’¥ç®¡ç†

#### 1. ä½¿ç”¨ Prefect Blocks
```python
from prefect.blocks.system import Secret

# åˆ›å»ºå¯†é’¥
def create_secrets():
    # æ•°æ®åº“å¯†ç 
    db_password = Secret(value="your-secure-password")
    db_password.save("database-password")
    
    # APIå¯†é’¥
    api_key = Secret(value="your-api-key")
    api_key.save("external-api-key")

# ä½¿ç”¨å¯†é’¥
@task(name="å®‰å…¨æ•°æ®åº“è¿æ¥")
def secure_database_connection():
    db_password = Secret.load("database-password")
    connection_string = f"postgresql://user:{db_password.get()}@localhost/db"
    return create_connection(connection_string)
```

#### 2. ç¯å¢ƒå˜é‡ç®¡ç†
```python
import os
from typing import Optional

class SecureConfig:
    @staticmethod
    def get_secret(key: str, default: Optional[str] = None) -> str:
        """å®‰å…¨åœ°è·å–ç¯å¢ƒå˜é‡"""
        value = os.getenv(key, default)
        if value is None:
            raise ValueError(f"å¿…éœ€çš„ç¯å¢ƒå˜é‡ {key} æœªè®¾ç½®")
        return value
    
    @property
    def database_url(self) -> str:
        return self.get_secret("DATABASE_URL")
    
    @property
    def api_key(self) -> str:
        return self.get_secret("API_KEY")

config = SecureConfig()
```

### ğŸ›¡ï¸ æ•°æ®ä¿æŠ¤

#### 1. æ•°æ®è„±æ•
```python
import hashlib
import re

@task(name="æ•°æ®è„±æ•")
def anonymize_data(df: pd.DataFrame) -> pd.DataFrame:
    """å¯¹æ•æ„Ÿæ•°æ®è¿›è¡Œè„±æ•å¤„ç†"""
    df_copy = df.copy()
    
    # é‚®ç®±è„±æ•
    if 'email' in df_copy.columns:
        df_copy['email'] = df_copy['email'].apply(mask_email)
    
    # æ‰‹æœºå·è„±æ•
    if 'phone' in df_copy.columns:
        df_copy['phone'] = df_copy['phone'].apply(mask_phone)
    
    # IDå“ˆå¸ŒåŒ–
    if 'user_id' in df_copy.columns:
        df_copy['user_id'] = df_copy['user_id'].apply(hash_id)
    
    return df_copy

def mask_email(email: str) -> str:
    """é‚®ç®±è„±æ•"""
    if '@' in email:
        local, domain = email.split('@', 1)
        masked_local = local[:2] + '*' * (len(local) - 2)
        return f"{masked_local}@{domain}"
    return email

def mask_phone(phone: str) -> str:
    """æ‰‹æœºå·è„±æ•"""
    if len(phone) >= 7:
        return phone[:3] + '*' * (len(phone) - 6) + phone[-3:]
    return phone

def hash_id(user_id: str) -> str:
    """IDå“ˆå¸ŒåŒ–"""
    return hashlib.sha256(user_id.encode()).hexdigest()[:16]
```

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### ğŸ”¬ å•å…ƒæµ‹è¯•

#### 1. ä»»åŠ¡æµ‹è¯•
```python
# tests/test_tasks.py
import pytest
import pandas as pd
from unittest.mock import Mock, patch

from tasks.data_tasks import clean_data, transform_data

class TestDataTasks:
    def test_clean_data_removes_nulls(self):
        """æµ‹è¯•æ•°æ®æ¸…æ´—åŠŸèƒ½"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'name': ['Alice', None, 'Bob'],
            'age': [25, 30, None],
            'email': ['alice@test.com', 'invalid', 'bob@test.com']
        })
        
        # æ‰§è¡Œæ¸…æ´—
        result = clean_data(test_data)
        
        # éªŒè¯ç»“æœ
        assert len(result) == 1  # åªæœ‰Aliceçš„è®°å½•å®Œæ•´
        assert result.iloc[0]['name'] == 'Alice'
    
    def test_transform_data_aggregation(self):
        """æµ‹è¯•æ•°æ®è½¬æ¢åŠŸèƒ½"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'date': ['2024-01-01', '2024-01-01', '2024-01-02'],
            'sales': [100, 200, 150],
            'quantity': [1, 2, 1]
        })
        test_data['date'] = pd.to_datetime(test_data['date'])
        
        # æ‰§è¡Œè½¬æ¢
        result = transform_data(test_data)
        
        # éªŒè¯ç»“æœ
        assert len(result) == 2  # ä¸¤ä¸ªä¸åŒçš„æ—¥æœŸ
        assert result[result['date'] == '2024-01-01']['total_sales'].iloc[0] == 300
```

#### 2. æµç¨‹æµ‹è¯•
```python
# tests/test_flows.py
import pytest
from unittest.mock import Mock, patch

from flows.data_processing.etl_flows import data_processing_flow

class TestDataProcessingFlow:
    @patch('flows.data_processing.etl_flows.fetch_api_data')
    @patch('flows.data_processing.etl_flows.store_data')
    def test_data_processing_flow_success(self, mock_store, mock_fetch):
        """æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹æˆåŠŸåœºæ™¯"""
        # æ¨¡æ‹ŸAPIè¿”å›æ•°æ®
        mock_fetch.return_value = [
            {'id': 1, 'name': 'test', 'value': 100}
        ]
        mock_store.return_value = 1
        
        # æ‰§è¡Œæµç¨‹
        result = data_processing_flow("test_api_url")
        
        # éªŒè¯è°ƒç”¨
        mock_fetch.assert_called_once_with("test_api_url")
        mock_store.assert_called_once()
        
        # éªŒè¯ç»“æœ
        assert "æˆåŠŸ" in result
    
    @patch('flows.data_processing.etl_flows.fetch_api_data')
    def test_data_processing_flow_api_failure(self, mock_fetch):
        """æµ‹è¯•APIå¤±è´¥åœºæ™¯"""
        # æ¨¡æ‹ŸAPIå¤±è´¥
        mock_fetch.side_effect = Exception("APIè¿æ¥å¤±è´¥")
        
        # éªŒè¯å¼‚å¸¸æŠ›å‡º
        with pytest.raises(Exception, match="APIè¿æ¥å¤±è´¥"):
            data_processing_flow("invalid_api_url")
```

### ğŸ¯ é›†æˆæµ‹è¯•

#### 1. ç«¯åˆ°ç«¯æµ‹è¯•
```python
# tests/test_integration.py
import pytest
import tempfile
import sqlite3
from pathlib import Path

from flows.data_processing.etl_flows import complete_data_pipeline

class TestIntegration:
    def test_complete_pipeline_integration(self):
        """ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
            db_path = Path(temp_dir) / "test.db"
            
            # æ‰§è¡Œå®Œæ•´æµç¨‹
            result = complete_data_pipeline(
                api_url="https://api.mock.com/data",
                db_path=str(db_path)
            )
            
            # éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM processed_data")
            count = cursor.fetchone()[0]
            conn.close()
            
            # éªŒè¯ç»“æœ
            assert count > 0
            assert "æˆåŠŸ" in result
```

---

## â“ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### ğŸ”§ æ€§èƒ½é—®é¢˜

#### é—®é¢˜1: æµç¨‹æ‰§è¡Œç¼“æ…¢
```python
# è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œå’Œæ‰¹å¤„ç†

# âŒ æ…¢çš„å®ç°
@flow
def slow_processing_flow(items: List[str]):
    results = []
    for item in items:
        result = process_single_item(item)  # ä¸²è¡Œå¤„ç†
        results.append(result)
    return results

# âœ… å¿«çš„å®ç°
@flow(task_runner=ConcurrentTaskRunner(max_workers=5))
def fast_processing_flow(items: List[str]):
    # å¹¶è¡Œæäº¤ä»»åŠ¡
    futures = []
    for item in items:
        future = process_single_item.submit(item)
        futures.append(future)
    
    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    results = [future.result() for future in futures]
    return results
```

#### é—®é¢˜2: å†…å­˜ä½¿ç”¨è¿‡é«˜
```python
# è§£å†³æ–¹æ¡ˆ: æµå¼å¤„ç†å’ŒåŠæ—¶é‡Šæ”¾å†…å­˜

# âŒ å†…å­˜æ¶ˆè€—å¤§çš„å®ç°
@task
def memory_intensive_task(large_file: str):
    # ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ–‡ä»¶
    df = pd.read_csv(large_file)  # å¯èƒ½å¾ˆå¤§
    processed_df = process_dataframe(df)
    return processed_df

# âœ… å†…å­˜å‹å¥½çš„å®ç°
@task
def memory_efficient_task(large_file: str, chunk_size: int = 10000):
    results = []
    for chunk in pd.read_csv(large_file, chunksize=chunk_size):
        processed_chunk = process_dataframe(chunk)
        results.append(processed_chunk)
        del chunk  # åŠæ—¶é‡Šæ”¾å†…å­˜
    
    return pd.concat(results, ignore_index=True)
```

### ğŸš¨ é”™è¯¯å¤„ç†é—®é¢˜

#### é—®é¢˜3: ä»»åŠ¡é¢‘ç¹å¤±è´¥
```python
# è§£å†³æ–¹æ¡ˆ: å®ç°æ™ºèƒ½é‡è¯•å’Œæ–­è·¯å™¨

@task(
    name="æ™ºèƒ½é‡è¯•ä»»åŠ¡",
    retries=3,
    retry_delay_seconds=[1, 5, 15]  # é€’å¢å»¶è¿Ÿ
)
def smart_retry_task(data: Any):
    try:
        return risky_operation(data)
    except TemporaryError:
        # ä¸´æ—¶é”™è¯¯ï¼Œå¯ä»¥é‡è¯•
        raise
    except PermanentError as e:
        # æ°¸ä¹…é”™è¯¯ï¼Œè®°å½•å¹¶è·³è¿‡
        logger.error(f"æ°¸ä¹…é”™è¯¯ï¼Œè·³è¿‡: {e}")
        return None
```

#### é—®é¢˜4: éƒ¨åˆ†å¤±è´¥å½±å“æ•´ä¸ªæµç¨‹
```python
# è§£å†³æ–¹æ¡ˆ: å®ç°å®¹é”™å¤„ç†

@flow
def fault_tolerant_flow(items: List[Any]):
    successful_results = []
    failed_items = []
    
    for item in items:
        try:
            result = process_item(item)
            successful_results.append(result)
        except Exception as e:
            logger.warning(f"é¡¹ç›®å¤„ç†å¤±è´¥: {item}, é”™è¯¯: {e}")
            failed_items.append({"item": item, "error": str(e)})
    
    # è®°å½•å¤„ç†æ‘˜è¦
    logger.info(f"å¤„ç†å®Œæˆ: æˆåŠŸ {len(successful_results)}, å¤±è´¥ {len(failed_items)}")
    
    return {
        "successful": successful_results,
        "failed": failed_items,
        "success_rate": len(successful_results) / len(items)
    }
```

### ğŸ”„ éƒ¨ç½²é—®é¢˜

#### é—®é¢˜5: éƒ¨ç½²é…ç½®ä¸ä¸€è‡´
```python
# è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨é…ç½®æ¨¡æ¿å’ŒéªŒè¯

# config/deployment_template.py
from typing import Dict, Any
from prefect.deployments import Deployment

class DeploymentTemplate:
    @staticmethod
    def create_standard_deployment(
        flow,
        name: str,
        schedule=None,
        tags: List[str] = None,
        parameters: Dict[str, Any] = None
    ) -> Deployment:
        """åˆ›å»ºæ ‡å‡†åŒ–éƒ¨ç½²é…ç½®"""
        
        # æ ‡å‡†åŒ–æ ‡ç­¾
        standard_tags = ["prefect", "automated"]
        if tags:
            standard_tags.extend(tags)
        
        # æ ‡å‡†åŒ–å‚æ•°
        standard_parameters = {
            "timeout_seconds": 3600,
            "retry_count": 3,
            **parameters or {}
        }
        
        return Deployment.build_from_flow(
            flow=flow,
            name=name,
            schedule=schedule,
            tags=standard_tags,
            parameters=standard_parameters,
            description=f"æ ‡å‡†åŒ–éƒ¨ç½²: {name}"
        )
```

---

## ğŸ“š æ€»ç»“å’Œå»ºè®®

### ğŸ¯ æ ¸å¿ƒåŸåˆ™
1. **ç®€å•æ€§**: ä¿æŒä»»åŠ¡å’Œæµç¨‹ç®€å•æ˜äº†
2. **å¯é æ€§**: å®ç°å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
3. **å¯è§‚æµ‹æ€§**: æ·»åŠ å……åˆ†çš„æ—¥å¿—å’Œç›‘æ§
4. **å¯ç»´æŠ¤æ€§**: ç¼–å†™æ¸…æ™°çš„ä»£ç å’Œæ–‡æ¡£
5. **å®‰å…¨æ€§**: ä¿æŠ¤æ•æ„Ÿæ•°æ®å’Œå¯†é’¥

### ğŸ“ˆ æŒç»­æ”¹è¿›
1. **å®šæœŸå®¡æŸ¥**: å®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–æµç¨‹æ€§èƒ½
2. **ç›‘æ§æŒ‡æ ‡**: æŒç»­ç›‘æ§å…³é”®ä¸šåŠ¡æŒ‡æ ‡
3. **ç”¨æˆ·åé¦ˆ**: æ”¶é›†å’Œå“åº”ç”¨æˆ·åé¦ˆ
4. **æŠ€æœ¯æ›´æ–°**: è·Ÿä¸ŠPrefectçš„æœ€æ–°åŠŸèƒ½å’Œæœ€ä½³å®è·µ

### ğŸ”— æœ‰ç”¨èµ„æº
- [Prefectå®˜æ–¹æ–‡æ¡£](https://docs.prefect.io/)
- [Prefectç¤¾åŒºè®ºå›](https://discourse.prefect.io/)
- [Prefect GitHub](https://github.com/PrefectHQ/prefect)
- [æœ€ä½³å®è·µç¤ºä¾‹](https://github.com/PrefectHQ/prefect-recipes)

---

**ğŸ’¡ è®°ä½**: æœ€ä½³å®è·µæ˜¯æŒ‡å¯¼åŸåˆ™ï¼Œä¸æ˜¯ä¸¥æ ¼è§„åˆ™ã€‚æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚å’Œç¯å¢ƒè°ƒæ•´è¿™äº›å»ºè®®ï¼