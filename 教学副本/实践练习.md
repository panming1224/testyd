# Python实践练习题

## 📚 练习说明

这些练习题按难度分为初级、中级和高级，每个练习都有详细的要求和参考答案。建议按顺序完成，每完成一个练习就运行测试，确保代码正确。

---

## 🟢 初级练习（第1-2周）

### 练习1：基础数据操作

**题目：** 创建一个简单的店铺信息管理程序

**要求：**
1. 定义一个包含店铺信息的字典
2. 实现添加、查询、修改店铺信息的功能
3. 使用循环显示所有店铺信息

```python
# 练习1参考答案
def shop_manager():
    """店铺信息管理器"""
    shops = {}
    
    def add_shop(name, profile, status="待下载"):
        """添加店铺"""
        shops[name] = {
            "profile": profile,
            "status": status,
            "created_time": "2025-09-26"
        }
        print(f"✅ 店铺 {name} 添加成功")
    
    def get_shop(name):
        """查询店铺"""
        if name in shops:
            return shops[name]
        else:
            print(f"❌ 店铺 {name} 不存在")
            return None
    
    def update_shop_status(name, status):
        """更新店铺状态"""
        if name in shops:
            shops[name]["status"] = status
            print(f"✅ 店铺 {name} 状态更新为：{status}")
        else:
            print(f"❌ 店铺 {name} 不存在")
    
    def list_all_shops():
        """显示所有店铺"""
        print("\n📋 所有店铺信息：")
        print("-" * 50)
        for name, info in shops.items():
            print(f"店铺：{name}")
            print(f"  配置：{info['profile']}")
            print(f"  状态：{info['status']}")
            print(f"  创建时间：{info['created_time']}")
            print("-" * 30)
    
    # 测试代码
    add_shop("测试店铺1", "Profile 1")
    add_shop("测试店铺2", "Profile 2", "已完成")
    
    list_all_shops()
    
    update_shop_status("测试店铺1", "已完成")
    
    shop_info = get_shop("测试店铺1")
    if shop_info:
        print(f"查询结果：{shop_info}")

# 运行练习
shop_manager()
```

### 练习2：文件路径处理

**题目：** 创建一个文件路径工具类

**要求：**
1. 使用pathlib处理文件路径
2. 实现创建目录、检查文件存在、生成文件名等功能
3. 处理不同操作系统的路径差异

```python
# 练习2参考答案
from pathlib import Path
from datetime import datetime

class FilePathHelper:
    """文件路径助手类"""
    
    def __init__(self, base_path):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def create_date_directory(self, date_str=None):
        """创建日期目录"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        date_dir = self.base_path / date_str
        date_dir.mkdir(parents=True, exist_ok=True)
        print(f"📁 创建目录：{date_dir}")
        return date_dir
    
    def generate_filename(self, shop_name, file_type="xlsx"):
        """生成文件名"""
        # 清理文件名中的特殊字符
        clean_name = "".join(c for c in shop_name if c.isalnum() or c in (' ', '-', '_')).strip()
        filename = f"{clean_name}.{file_type}"
        return filename
    
    def get_file_path(self, shop_name, date_str=None, file_type="xlsx"):
        """获取完整文件路径"""
        date_dir = self.create_date_directory(date_str)
        filename = self.generate_filename(shop_name, file_type)
        return date_dir / filename
    
    def check_file_exists(self, file_path):
        """检查文件是否存在"""
        path = Path(file_path)
        exists = path.exists()
        print(f"📄 文件 {path.name} {'存在' if exists else '不存在'}")
        return exists
    
    def list_files_in_directory(self, directory=None, pattern="*"):
        """列出目录中的文件"""
        if directory is None:
            directory = self.base_path
        else:
            directory = Path(directory)
        
        files = list(directory.glob(pattern))
        print(f"📂 目录 {directory} 中的文件：")
        for file in files:
            print(f"  - {file.name}")
        return files

# 测试代码
def test_file_helper():
    helper = FilePathHelper("D:/testyd/练习")
    
    # 创建文件路径
    file_path1 = helper.get_file_path("测试店铺 1", "2025-09-26")
    file_path2 = helper.get_file_path("测试店铺@2", "2025-09-26")
    
    print(f"文件路径1：{file_path1}")
    print(f"文件路径2：{file_path2}")
    
    # 检查文件存在
    helper.check_file_exists(file_path1)
    
    # 创建测试文件
    file_path1.touch()  # 创建空文件
    helper.check_file_exists(file_path1)
    
    # 列出文件
    helper.list_files_in_directory(file_path1.parent, "*.xlsx")

# 运行测试
test_file_helper()
```

### 练习3：简单的日志系统

**题目：** 创建一个简单的日志记录系统

**要求：**
1. 实现不同级别的日志（INFO、WARNING、ERROR）
2. 日志同时输出到控制台和文件
3. 日志格式包含时间、级别、消息

```python
# 练习3参考答案
import logging
from datetime import datetime
from pathlib import Path

class SimpleLogger:
    """简单日志记录器"""
    
    def __init__(self, name="MyApp", log_file="app.log"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # 避免重复添加处理器
        if not self.logger.handlers:
            # 创建格式化器
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            
            # 控制台处理器
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
            
            # 文件处理器
            log_path = Path(log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def info(self, message):
        """信息日志"""
        self.logger.info(message)
    
    def warning(self, message):
        """警告日志"""
        self.logger.warning(message)
    
    def error(self, message):
        """错误日志"""
        self.logger.error(message)
    
    def log_function_start(self, function_name, **kwargs):
        """记录函数开始"""
        params = ", ".join(f"{k}={v}" for k, v in kwargs.items())
        self.info(f"🚀 开始执行 {function_name}({params})")
    
    def log_function_end(self, function_name, success=True, result=None):
        """记录函数结束"""
        status = "✅ 成功" if success else "❌ 失败"
        message = f"{status} 完成 {function_name}"
        if result is not None:
            message += f"，结果：{result}"
        
        if success:
            self.info(message)
        else:
            self.error(message)

# 测试代码
def test_logger():
    logger = SimpleLogger("TestApp", "D:/testyd/练习/test.log")
    
    # 模拟下载函数
    def download_shop_data(shop_name, profile):
        logger.log_function_start("download_shop_data", 
                                shop_name=shop_name, profile=profile)
        
        try:
            # 模拟下载过程
            logger.info(f"正在连接到 {shop_name} 的数据源...")
            logger.info(f"使用配置文件：{profile}")
            
            # 模拟可能的警告
            if "测试" in shop_name:
                logger.warning("这是测试店铺，数据可能不完整")
            
            # 模拟成功
            logger.log_function_end("download_shop_data", success=True, 
                                  result="文件已保存")
            return True
            
        except Exception as e:
            logger.log_function_end("download_shop_data", success=False, 
                                  result=str(e))
            return False
    
    # 测试日志记录
    download_shop_data("测试店铺1", "Profile 1")
    download_shop_data("正式店铺1", "Profile 2")
    
    logger.info("所有下载任务完成")

# 运行测试
test_logger()
```

---

## 🟡 中级练习（第3-4周）

### 练习4：Excel数据处理器

**题目：** 创建一个Excel数据处理类

**要求：**
1. 读取Excel文件并分析数据结构
2. 实现数据筛选、修改、统计功能
3. 支持批量处理多个Excel文件

```python
# 练习4参考答案
import pandas as pd
from pathlib import Path
import numpy as np

class ExcelProcessor:
    """Excel数据处理器"""
    
    def __init__(self):
        self.data = None
        self.file_path = None
    
    def load_excel(self, file_path, sheet_name=0):
        """加载Excel文件"""
        try:
            self.file_path = Path(file_path)
            self.data = pd.read_excel(file_path, sheet_name=sheet_name)
            print(f"✅ 成功加载文件：{self.file_path.name}")
            print(f"📊 数据形状：{self.data.shape}")
            return True
        except Exception as e:
            print(f"❌ 加载文件失败：{e}")
            return False
    
    def analyze_data(self):
        """分析数据结构"""
        if self.data is None:
            print("❌ 请先加载数据")
            return
        
        print("\n📋 数据分析报告：")
        print("-" * 50)
        print(f"行数：{len(self.data)}")
        print(f"列数：{len(self.data.columns)}")
        print(f"列名：{list(self.data.columns)}")
        
        print("\n📊 数据类型：")
        for col in self.data.columns:
            dtype = self.data[col].dtype
            null_count = self.data[col].isnull().sum()
            print(f"  {col}: {dtype} (空值: {null_count})")
        
        print("\n📈 数值列统计：")
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(self.data[numeric_cols].describe())
        else:
            print("  无数值列")
    
    def filter_data(self, column, condition, value):
        """筛选数据"""
        if self.data is None:
            print("❌ 请先加载数据")
            return None
        
        try:
            if condition == "等于":
                filtered = self.data[self.data[column] == value]
            elif condition == "包含":
                filtered = self.data[self.data[column].str.contains(str(value), na=False)]
            elif condition == "大于":
                filtered = self.data[self.data[column] > value]
            elif condition == "小于":
                filtered = self.data[self.data[column] < value]
            else:
                print(f"❌ 不支持的条件：{condition}")
                return None
            
            print(f"✅ 筛选结果：{len(filtered)} 行数据")
            return filtered
            
        except Exception as e:
            print(f"❌ 筛选失败：{e}")
            return None
    
    def update_column(self, column, condition_column, condition_value, new_value):
        """更新列数据"""
        if self.data is None:
            print("❌ 请先加载数据")
            return False
        
        try:
            mask = self.data[condition_column] == condition_value
            affected_rows = mask.sum()
            
            if affected_rows > 0:
                self.data.loc[mask, column] = new_value
                print(f"✅ 更新了 {affected_rows} 行数据")
                return True
            else:
                print("⚠️ 没有符合条件的数据")
                return False
                
        except Exception as e:
            print(f"❌ 更新失败：{e}")
            return False
    
    def save_excel(self, output_path=None):
        """保存Excel文件"""
        if self.data is None:
            print("❌ 没有数据可保存")
            return False
        
        try:
            if output_path is None:
                output_path = self.file_path
            
            self.data.to_excel(output_path, index=False, engine='openpyxl')
            print(f"✅ 文件已保存：{output_path}")
            return True
            
        except Exception as e:
            print(f"❌ 保存失败：{e}")
            return False
    
    def batch_process(self, directory, pattern="*.xlsx"):
        """批量处理Excel文件"""
        directory = Path(directory)
        files = list(directory.glob(pattern))
        
        print(f"📂 找到 {len(files)} 个Excel文件")
        
        results = []
        for file_path in files:
            print(f"\n🔄 处理文件：{file_path.name}")
            
            processor = ExcelProcessor()
            if processor.load_excel(file_path):
                processor.analyze_data()
                results.append({
                    'file': file_path.name,
                    'rows': len(processor.data),
                    'columns': len(processor.data.columns),
                    'success': True
                })
            else:
                results.append({
                    'file': file_path.name,
                    'success': False
                })
        
        # 汇总报告
        print("\n📊 批量处理报告：")
        print("-" * 50)
        successful = sum(1 for r in results if r['success'])
        print(f"成功处理：{successful}/{len(results)} 个文件")
        
        for result in results:
            if result['success']:
                print(f"✅ {result['file']}: {result['rows']}行 x {result['columns']}列")
            else:
                print(f"❌ {result['file']}: 处理失败")
        
        return results

# 测试代码
def test_excel_processor():
    # 创建测试数据
    test_data = {
        '店铺名称': ['店铺1', '店铺2', '店铺3', '店铺4'],
        '配置文件': ['Profile 1', 'Profile 2', 'Profile 1', 'Profile 3'],
        '状态': ['待下载', '已完成', '待下载', '已完成'],
        '销售额': [1000, 2000, 1500, 3000],
        '库存量': [100, 50, 200, 80]
    }
    
    df = pd.DataFrame(test_data)
    test_file = Path("D:/testyd/练习/test_data.xlsx")
    test_file.parent.mkdir(parents=True, exist_ok=True)
    df.to_excel(test_file, index=False, engine='openpyxl')
    
    # 测试处理器
    processor = ExcelProcessor()
    
    # 加载和分析
    processor.load_excel(test_file)
    processor.analyze_data()
    
    # 筛选数据
    filtered = processor.filter_data('状态', '等于', '待下载')
    if filtered is not None:
        print(f"\n筛选结果：\n{filtered}")
    
    # 更新数据
    processor.update_column('状态', '店铺名称', '店铺1', '已完成')
    
    # 保存文件
    output_file = test_file.parent / "processed_data.xlsx"
    processor.save_excel(output_file)

# 运行测试
test_excel_processor()
```

### 练习5：网络请求封装器

**题目：** 创建一个网络请求封装类

**要求：**
1. 封装GET、POST请求方法
2. 支持重试机制和超时设置
3. 实现Cookie管理和请求头设置

```python
# 练习5参考答案
import requests
import time
import json
from typing import Dict, Optional, Any

class HttpClient:
    """HTTP客户端封装器"""
    
    def __init__(self, base_url="", timeout=30, max_retries=3):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        
        # 默认请求头
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def set_headers(self, headers: Dict[str, str]):
        """设置请求头"""
        self.session.headers.update(headers)
        print(f"✅ 已更新请求头：{list(headers.keys())}")
    
    def set_cookies(self, cookies: Dict[str, str]):
        """设置Cookie"""
        self.session.cookies.update(cookies)
        print(f"✅ 已更新Cookie：{len(cookies)} 个")
    
    def _make_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """发送请求的内部方法"""
        # 处理URL
        if not url.startswith('http'):
            url = f"{self.base_url}/{url.lstrip('/')}"
        
        # 设置超时
        kwargs.setdefault('timeout', self.timeout)
        
        for attempt in range(self.max_retries):
            try:
                print(f"🌐 发送{method}请求：{url} (尝试 {attempt + 1}/{self.max_retries})")
                
                response = self.session.request(method, url, **kwargs)
                
                print(f"📡 响应状态码：{response.status_code}")
                
                # 检查状态码
                response.raise_for_status()
                
                return response
                
            except requests.exceptions.Timeout:
                print(f"⏰ 请求超时 (尝试 {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # 指数退避
                    
            except requests.exceptions.RequestException as e:
                print(f"❌ 请求失败：{e} (尝试 {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
        
        print(f"❌ 所有重试都失败了")
        return None
    
    def get(self, url: str, params: Optional[Dict] = None) -> Optional[Dict]:
        """发送GET请求"""
        response = self._make_request('GET', url, params=params)
        return self._parse_response(response)
    
    def post(self, url: str, data: Optional[Dict] = None, 
             json_data: Optional[Dict] = None) -> Optional[Dict]:
        """发送POST请求"""
        kwargs = {}
        if data:
            kwargs['data'] = data
        if json_data:
            kwargs['json'] = json_data
            
        response = self._make_request('POST', url, **kwargs)
        return self._parse_response(response)
    
    def download_file(self, url: str, file_path: str) -> bool:
        """下载文件"""
        response = self._make_request('GET', url, stream=True)
        
        if response is None:
            return False
        
        try:
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"✅ 文件下载成功：{file_path}")
            return True
            
        except Exception as e:
            print(f"❌ 文件保存失败：{e}")
            return False
    
    def _parse_response(self, response: Optional[requests.Response]) -> Optional[Dict]:
        """解析响应"""
        if response is None:
            return None
        
        try:
            # 尝试解析JSON
            if 'application/json' in response.headers.get('content-type', ''):
                return response.json()
            else:
                return {'text': response.text, 'status_code': response.status_code}
                
        except json.JSONDecodeError:
            return {'text': response.text, 'status_code': response.status_code}
    
    def close(self):
        """关闭会话"""
        self.session.close()
        print("✅ HTTP会话已关闭")

# 测试代码
def test_http_client():
    # 创建客户端
    client = HttpClient(base_url="https://httpbin.org", timeout=10, max_retries=2)
    
    # 设置请求头
    client.set_headers({
        'Accept': 'application/json',
        'Custom-Header': 'test-value'
    })
    
    # 测试GET请求
    print("\n🔍 测试GET请求：")
    get_result = client.get('/get', params={'key': 'value', 'test': '测试'})
    if get_result:
        print(f"GET响应：{json.dumps(get_result, indent=2, ensure_ascii=False)}")
    
    # 测试POST请求
    print("\n📤 测试POST请求：")
    post_data = {'name': '测试', 'value': 123}
    post_result = client.post('/post', json_data=post_data)
    if post_result:
        print(f"POST响应：{json.dumps(post_result, indent=2, ensure_ascii=False)}")
    
    # 测试错误处理
    print("\n❌ 测试错误处理：")
    error_result = client.get('/status/404')  # 这会返回404错误
    
    # 关闭客户端
    client.close()

# 运行测试
test_http_client()
```

---

## 🔴 高级练习（第5-6周）

### 练习6：多线程下载器

**题目：** 创建一个支持多线程的文件下载器

**要求：**
1. 使用线程池并发下载多个文件
2. 实现下载进度监控
3. 支持断点续传和错误重试

```python
# 练习6参考答案
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import requests
from typing import List, Dict, Callable, Optional

class MultiThreadDownloader:
    """多线程下载器"""
    
    def __init__(self, max_workers=4, timeout=30):
        self.max_workers = max_workers
        self.timeout = timeout
        self.download_stats = {}
        self.lock = threading.Lock()
    
    def download_file(self, url: str, file_path: str, 
                     progress_callback: Optional[Callable] = None) -> Dict:
        """下载单个文件"""
        thread_id = threading.current_thread().ident
        file_path = Path(file_path)
        
        result = {
            'url': url,
            'file_path': str(file_path),
            'success': False,
            'error': None,
            'size': 0,
            'thread_id': thread_id
        }
        
        try:
            print(f"🧵 线程{thread_id}: 开始下载 {file_path.name}")
            
            # 检查是否支持断点续传
            resume_pos = 0
            if file_path.exists():
                resume_pos = file_path.stat().st_size
                print(f"📁 发现已存在文件，从位置 {resume_pos} 继续下载")
            
            # 设置请求头支持断点续传
            headers = {}
            if resume_pos > 0:
                headers['Range'] = f'bytes={resume_pos}-'
            
            # 发送请求
            response = requests.get(url, headers=headers, stream=True, timeout=self.timeout)
            response.raise_for_status()
            
            # 获取文件总大小
            total_size = int(response.headers.get('content-length', 0))
            if resume_pos > 0:
                total_size += resume_pos
            
            # 打开文件（追加模式如果是断点续传）
            mode = 'ab' if resume_pos > 0 else 'wb'
            
            with open(file_path, mode) as f:
                downloaded = resume_pos
                
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        # 调用进度回调
                        if progress_callback:
                            progress = (downloaded / total_size * 100) if total_size > 0 else 0
                            progress_callback(file_path.name, downloaded, total_size, progress)
            
            result['success'] = True
            result['size'] = downloaded
            print(f"✅ 线程{thread_id}: {file_path.name} 下载完成 ({downloaded} 字节)")
            
        except Exception as e:
            result['error'] = str(e)
            print(f"❌ 线程{thread_id}: {file_path.name} 下载失败: {e}")
        
        return result
    
    def progress_callback(self, filename: str, downloaded: int, total: int, progress: float):
        """进度回调函数"""
        with self.lock:
            self.download_stats[filename] = {
                'downloaded': downloaded,
                'total': total,
                'progress': progress
            }
            
            # 每下载1MB显示一次进度
            if downloaded % (1024 * 1024) < 8192:  # 8KB chunk size
                print(f"📊 {filename}: {progress:.1f}% ({downloaded}/{total} 字节)")
    
    def batch_download(self, download_tasks: List[Dict]) -> List[Dict]:
        """批量下载文件"""
        print(f"🚀 开始批量下载，使用 {self.max_workers} 个线程")
        print(f"📋 任务数量：{len(download_tasks)}")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # 提交所有下载任务
            future_to_task = {}
            
            for task in download_tasks:
                future = executor.submit(
                    self.download_file,
                    task['url'],
                    task['file_path'],
                    self.progress_callback
                )
                future_to_task[future] = task
            
            # 处理完成的任务
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"❌ 任务执行异常：{e}")
                    results.append({
                        'url': task['url'],
                        'file_path': task['file_path'],
                        'success': False,
                        'error': str(e)
                    })
        
        # 统计结果
        successful = sum(1 for r in results if r['success'])
        total_size = sum(r['size'] for r in results if r['success'])
        
        print(f"\n📊 下载完成统计：")
        print(f"✅ 成功：{successful}/{len(results)}")
        print(f"📦 总大小：{total_size / (1024*1024):.2f} MB")
        
        return results
    
    def monitor_progress(self, interval=2):
        """监控下载进度"""
        def monitor():
            while True:
                time.sleep(interval)
                with self.lock:
                    if not self.download_stats:
                        continue
                    
                    print("\n📈 实时进度：")
                    for filename, stats in self.download_stats.items():
                        progress = stats['progress']
                        downloaded = stats['downloaded'] / (1024*1024)  # MB
                        total = stats['total'] / (1024*1024)  # MB
                        print(f"  {filename}: {progress:.1f}% ({downloaded:.1f}/{total:.1f} MB)")
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        return monitor_thread

# 测试代码
def test_multi_thread_downloader():
    downloader = MultiThreadDownloader(max_workers=3, timeout=30)
    
    # 创建测试下载任务
    download_tasks = [
        {
            'url': 'https://httpbin.org/bytes/1048576',  # 1MB测试文件
            'file_path': 'D:/testyd/练习/downloads/test1.bin'
        },
        {
            'url': 'https://httpbin.org/bytes/2097152',  # 2MB测试文件
            'file_path': 'D:/testyd/练习/downloads/test2.bin'
        },
        {
            'url': 'https://httpbin.org/bytes/1572864',  # 1.5MB测试文件
            'file_path': 'D:/testyd/练习/downloads/test3.bin'
        }
    ]
    
    # 创建下载目录
    download_dir = Path('D:/testyd/练习/downloads')
    download_dir.mkdir(parents=True, exist_ok=True)
    
    # 启动进度监控
    monitor_thread = downloader.monitor_progress(interval=1)
    
    # 开始批量下载
    results = downloader.batch_download(download_tasks)
    
    # 显示详细结果
    print("\n📋 详细结果：")
    for result in results:
        status = "✅ 成功" if result['success'] else "❌ 失败"
        size_mb = result['size'] / (1024*1024)
        print(f"{status} {Path(result['file_path']).name}: {size_mb:.2f} MB")
        if not result['success']:
            print(f"   错误：{result['error']}")

# 运行测试
test_multi_thread_downloader()
```

### 练习7：配置管理器

**题目：** 创建一个灵活的配置管理系统

**要求：**
1. 支持多种配置格式（JSON、YAML、INI）
2. 实现配置热重载和验证
3. 支持环境变量覆盖和配置继承

```python
# 练习7参考答案
import json
import configparser
import os
from pathlib import Path
from typing import Dict, Any, Optional, Union
import threading
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigManager:
    """配置管理器"""
    
    def __init__(self, config_file: str, auto_reload=True):
        self.config_file = Path(config_file)
        self.config_data = {}
        self.lock = threading.RLock()
        self.observers = []
        self.auto_reload = auto_reload
        
        # 加载配置
        self.load_config()
        
        # 启动自动重载
        if auto_reload:
            self.start_auto_reload()
    
    def load_config(self):
        """加载配置文件"""
        with self.lock:
            try:
                if not self.config_file.exists():
                    print(f"⚠️ 配置文件不存在：{self.config_file}")
                    return
                
                file_ext = self.config_file.suffix.lower()
                
                if file_ext == '.json':
                    self._load_json()
                elif file_ext == '.ini':
                    self._load_ini()
                elif file_ext in ['.yml', '.yaml']:
                    self._load_yaml()
                else:
                    raise ValueError(f"不支持的配置文件格式：{file_ext}")
                
                # 应用环境变量覆盖
                self._apply_env_overrides()
                
                print(f"✅ 配置加载成功：{self.config_file}")
                
            except Exception as e:
                print(f"❌ 配置加载失败：{e}")
    
    def _load_json(self):
        """加载JSON配置"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config_data = json.load(f)
    
    def _load_ini(self):
        """加载INI配置"""
        config = configparser.ConfigParser()
        config.read(self.config_file, encoding='utf-8')
        
        self.config_data = {}
        for section in config.sections():
            self.config_data[section] = dict(config[section])
    
    def _load_yaml(self):
        """加载YAML配置"""
        try:
            import yaml
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config_data = yaml.safe_load(f)
        except ImportError:
            raise ImportError("需要安装PyYAML库：pip install PyYAML")
    
    def _apply_env_overrides(self):
        """应用环境变量覆盖"""
        env_prefix = "APP_CONFIG_"
        
        for key, value in os.environ.items():
            if key.startswith(env_prefix):
                config_key = key[len(env_prefix):].lower()
                
                # 支持嵌套键（用双下划线分隔）
                keys = config_key.split('__')
                
                # 尝试转换值类型
                converted_value = self._convert_env_value(value)
                
                # 设置配置值
                self._set_nested_value(self.config_data, keys, converted_value)
                print(f"🔧 环境变量覆盖：{config_key} = {converted_value}")
    
    def _convert_env_value(self, value: str) -> Any:
        """转换环境变量值类型"""
        # 尝试转换为数字
        try:
            if '.' in value:
                return float(value)
            else:
                return int(value)
        except ValueError:
            pass
        
        # 尝试转换为布尔值
        if value.lower() in ['true', 'false']:
            return value.lower() == 'true'
        
        # 尝试转换为JSON
        if value.startswith('{') or value.startswith('['):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                pass
        
        # 返回字符串
        return value
    
    def _set_nested_value(self, data: Dict, keys: list, value: Any):
        """设置嵌套字典值"""
        for key in keys[:-1]:
            if key not in data:
                data[key] = {}
            data = data[key]
        data[keys[-1]] = value
    
    def get(self, key: str, default: Any = None) -> Any:
        """获取配置值"""
        with self.lock:
            keys = key.split('.')
            data = self.config_data
            
            try:
                for k in keys:
                    data = data[k]
                return data
            except (KeyError, TypeError):
                return default
    
    def set(self, key: str, value: Any):
        """设置配置值"""
        with self.lock:
            keys = key.split('.')
            self._set_nested_value(self.config_data, keys, value)
    
    def save_config(self):
        """保存配置到文件"""
        with self.lock:
            try:
                file_ext = self.config_file.suffix.lower()
                
                if file_ext == '.json':
                    with open(self.config_file, 'w', encoding='utf-8') as f:
                        json.dump(self.config_data, f, indent=2, ensure_ascii=False)
                else:
                    print(f"⚠️ 暂不支持保存 {file_ext} 格式")
                    return
                
                print(f"✅ 配置已保存：{self.config_file}")
                
            except Exception as e:
                print(f"❌ 配置保存失败：{e}")
    
    def validate_config(self, schema: Dict) -> bool:
        """验证配置"""
        def validate_item(data, schema_item, path=""):
            if isinstance(schema_item, dict):
                if 'type' in schema_item:
                    expected_type = schema_item['type']
                    if expected_type == 'string' and not isinstance(data, str):
                        print(f"❌ {path}: 期望字符串，实际 {type(data).__name__}")
                        return False
                    elif expected_type == 'number' and not isinstance(data, (int, float)):
                        print(f"❌ {path}: 期望数字，实际 {type(data).__name__}")
                        return False
                    elif expected_type == 'boolean' and not isinstance(data, bool):
                        print(f"❌ {path}: 期望布尔值，实际 {type(data).__name__}")
                        return False
                
                if 'required' in schema_item:
                    for req_key in schema_item['required']:
                        if req_key not in data:
                            print(f"❌ {path}: 缺少必需字段 {req_key}")
                            return False
                
                if 'properties' in schema_item:
                    for prop_key, prop_schema in schema_item['properties'].items():
                        if prop_key in data:
                            if not validate_item(data[prop_key], prop_schema, f"{path}.{prop_key}"):
                                return False
            
            return True
        
        with self.lock:
            is_valid = validate_item(self.config_data, schema)
            if is_valid:
                print("✅ 配置验证通过")
            return is_valid
    
    def start_auto_reload(self):
        """启动自动重载"""
        class ConfigFileHandler(FileSystemEventHandler):
            def __init__(self, config_manager):
                self.config_manager = config_manager
            
            def on_modified(self, event):
                if not event.is_directory and Path(event.src_path) == self.config_manager.config_file:
                    print(f"🔄 检测到配置文件变化，重新加载...")
                    time.sleep(0.1)  # 等待文件写入完成
                    self.config_manager.load_config()
        
        observer = Observer()
        observer.schedule(
            ConfigFileHandler(self),
            str(self.config_file.parent),
            recursive=False
        )
        observer.start()
        self.observers.append(observer)
        print(f"👀 已启动配置文件监控：{self.config_file}")
    
    def stop_auto_reload(self):
        """停止自动重载"""
        for observer in self.observers:
            observer.stop()
            observer.join()
        self.observers.clear()
        print("🛑 已停止配置文件监控")
    
    def __del__(self):
        """析构函数"""
        self.stop_auto_reload()

# 测试代码
def test_config_manager():
    # 创建测试配置文件
    config_dir = Path("D:/testyd/练习/config")
    config_dir.mkdir(parents=True, exist_ok=True)
    
    config_file = config_dir / "app_config.json"
    test_config = {
        "app": {
            "name": "测试应用",
            "version": "1.0.0",
            "debug": True
        },
        "database": {
            "host": "localhost",
            "port": 3306,
            "username": "admin",
            "password": "password"
        },
        "download": {
            "max_workers": 4,
            "timeout": 30,
            "retry_count": 3
        }
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, indent=2, ensure_ascii=False)
    
    # 测试配置管理器
    config = ConfigManager(config_file, auto_reload=True)
    
    # 测试获取配置
    print(f"应用名称：{config.get('app.name')}")
    print(f"数据库端口：{config.get('database.port')}")
    print(f"最大工作线程：{config.get('download.max_workers')}")
    print(f"不存在的配置：{config.get('nonexistent.key', '默认值')}")
    
    # 测试设置配置
    config.set('app.debug', False)
    config.set('new_section.new_key', 'new_value')
    
    # 测试配置验证
    schema = {
        "type": "object",
        "required": ["app", "database"],
        "properties": {
            "app": {
                "type": "object",
                "required": ["name", "version"],
                "properties": {
                    "name": {"type": "string"},
                    "version": {"type": "string"},
                    "debug": {"type": "boolean"}
                }
            },
            "database": {
                "type": "object",
                "required": ["host", "port"],
                "properties": {
                    "host": {"type": "string"},
                    "port": {"type": "number"}
                }
            }
        }
    }
    
    config.validate_config(schema)
    
    # 保存配置
    config.save_config()
    
    print("\n💡 提示：修改配置文件内容，观察自动重载效果")
    
    # 等待一段时间以观察自动重载
    time.sleep(2)
    
    return config

# 运行测试
config = test_config_manager()
```

---

## 🎯 综合项目练习

### 练习8：完整的数据处理流水线

**题目：** 创建一个完整的数据处理流水线

**要求：**
1. 整合前面所有练习的功能
2. 实现数据采集、处理、存储的完整流程
3. 添加监控、日志、错误处理等功能

```python
# 练习8参考答案
import asyncio
import aiohttp
import aiofiles
from datetime import datetime, timedelta
from pathlib import Path
import json
import pandas as pd
from typing import List, Dict, Any
import logging
from concurrent.futures import ThreadPoolExecutor
import time

class DataPipeline:
    """数据处理流水线"""
    
    def __init__(self, config_file: str):
        self.config = self._load_config(config_file)
        self.logger = self._setup_logger()
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'processed_records': 0
        }
    
    def _load_config(self, config_file: str) -> Dict:
        """加载配置文件"""
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _setup_logger(self) -> logging.Logger:
        """设置日志"""
        logger = logging.getLogger('DataPipeline')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # 文件处理器
            log_file = Path(self.config['logging']['file'])
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            # 控制台处理器
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            # 格式化器
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    async def run_pipeline(self):
        """运行完整的数据流水线"""
        self.stats['start_time'] = datetime.now()
        self.logger.info("🚀 数据流水线启动")
        
        try:
            # 步骤1：数据采集
            raw_data = await self._collect_data()
            
            # 步骤2：数据处理
            processed_data = await self._process_data(raw_data)
            
            # 步骤3：数据存储
            await self._store_data(processed_data)
            
            # 步骤4：生成报告
            await self._generate_report()
            
            self.stats['end_time'] = datetime.now()
            self.logger.info("✅ 数据流水线完成")
            
        except Exception as e:
            self.logger.error(f"❌ 流水线执行失败：{e}")
            raise
    
    async def _collect_data(self) -> List[Dict]:
        """数据采集阶段"""
        self.logger.info("📥 开始数据采集")
        
        data_sources = self.config['data_sources']
        all_data = []
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for source in data_sources:
                if source['type'] == 'api':
                    task = self._fetch_api_data(session, source)
                elif source['type'] == 'file':
                    task = self._fetch_file_data(source)
                else:
                    continue
                
                tasks.append(task)
            
            # 并发执行所有采集任务
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"❌ 数据源 {i} 采集失败：{result}")
                    self.stats['failed_tasks'] += 1
                else:
                    all_data.extend(result)
                    self.stats['completed_tasks'] += 1
        
        self.logger.info(f"📊 数据采集完成，共获取 {len(all_data)} 条记录")
        return all_data
    
    async def _fetch_api_data(self, session: aiohttp.ClientSession, source: Dict) -> List[Dict]:
        """从API获取数据"""
        try:
            async with session.get(source['url'], timeout=30) as response:
                response.raise_for_status()
                data = await response.json()
                
                # 根据配置提取数据
                if 'data_path' in source:
                    for key in source['data_path'].split('.'):
                        data = data[key]
                
                return data if isinstance(data, list) else [data]
                
        except Exception as e:
            self.logger.error(f"❌ API数据获取失败 {source['url']}: {e}")
            raise
    
    async def _fetch_file_data(self, source: Dict) -> List[Dict]:
        """从文件获取数据"""
        try:
            file_path = Path(source['path'])
            
            if file_path.suffix.lower() == '.json':
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                    return data if isinstance(data, list) else [data]
            
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                # 使用线程池处理Excel文件
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor() as executor:
                    df = await loop.run_in_executor(
                        executor, pd.read_excel, str(file_path)
                    )
                    return df.to_dict('records')
            
            else:
                raise ValueError(f"不支持的文件格式：{file_path.suffix}")
                
        except Exception as e:
            self.logger.error(f"❌ 文件数据获取失败 {source['path']}: {e}")
            raise
    
    async def _process_data(self, raw_data: List[Dict]) -> List[Dict]:
        """数据处理阶段"""
        self.logger.info("🔄 开始数据处理")
        
        processed_data = []
        processing_rules = self.config['processing']['rules']
        
        for record in raw_data:
            try:
                processed_record = await self._process_single_record(record, processing_rules)
                if processed_record:
                    processed_data.append(processed_record)
                    self.stats['processed_records'] += 1
                    
            except Exception as e:
                self.logger.warning(f"⚠️ 记录处理失败：{e}")
                continue
        
        self.logger.info(f"📊 数据处理完成，处理了 {len(processed_data)} 条记录")
        return processed_data
    
    async def _process_single_record(self, record: Dict, rules: List[Dict]) -> Dict:
        """处理单条记录"""
        processed = record.copy()
        
        for rule in rules:
            rule_type = rule['type']
            
            if rule_type == 'rename':
                # 重命名字段
                old_name = rule['from']
                new_name = rule['to']
                if old_name in processed:
                    processed[new_name] = processed.pop(old_name)
            
            elif rule_type == 'transform':
                # 数据转换
                field = rule['field']
                transform_type = rule['transform']
                
                if field in processed:
                    if transform_type == 'uppercase':
                        processed[field] = str(processed[field]).upper()
                    elif transform_type == 'lowercase':
                        processed[field] = str(processed[field]).lower()
                    elif transform_type == 'date_format':
                        # 日期格式化
                        date_str = processed[field]
                        if isinstance(date_str, str):
                            try:
                                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                                processed[field] = dt.strftime(rule['format'])
                            except ValueError:
                                pass
            
            elif rule_type == 'filter':
                # 数据过滤
                field = rule['field']
                condition = rule['condition']
                value = rule['value']
                
                if field in processed:
                    record_value = processed[field]
                    
                    if condition == 'equals' and record_value != value:
                        return None
                    elif condition == 'contains' and value not in str(record_value):
                        return None
                    elif condition == 'greater_than' and record_value <= value:
                        return None
            
            elif rule_type == 'add_field':
                # 添加字段
                field_name = rule['name']
                field_value = rule['value']
                
                # 支持动态值
                if field_value == '${current_time}':
                    field_value = datetime.now().isoformat()
                elif field_value == '${date}':
                    field_value = datetime.now().strftime('%Y-%m-%d')
                
                processed[field_name] = field_value
        
        return processed
    
    async def _store_data(self, data: List[Dict]):
        """数据存储阶段"""
        self.logger.info("💾 开始数据存储")
        
        storage_config = self.config['storage']
        
        for storage in storage_config:
            try:
                if storage['type'] == 'json':
                    await self._store_to_json(data, storage)
                elif storage['type'] == 'excel':
                    await self._store_to_excel(data, storage)
                elif storage['type'] == 'database':
                    await self._store_to_database(data, storage)
                
                self.logger.info(f"✅ 数据已存储到 {storage['type']}")
                
            except Exception as e:
                self.logger.error(f"❌ 存储失败 {storage['type']}: {e}")
    
    async def _store_to_json(self, data: List[Dict], config: Dict):
        """存储到JSON文件"""
        output_path = Path(config['path'])
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2, ensure_ascii=False))
    
    async def _store_to_excel(self, data: List[Dict], config: Dict):
        """存储到Excel文件"""
        output_path = Path(config['path'])
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # 使用线程池处理Excel写入
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            df = pd.DataFrame(data)
            await loop.run_in_executor(
                executor, df.to_excel, str(output_path), False
            )
    
    async def _store_to_database(self, data: List[Dict], config: Dict):
        """存储到数据库"""
        # 这里可以实现数据库存储逻辑
        self.logger.info(f"📊 模拟存储 {len(data)} 条记录到数据库")
    
    async def _generate_report(self):
        """生成处理报告"""
        self.logger.info("📋 生成处理报告")
        
        duration = self.stats['end_time'] - self.stats['start_time']
        
        report = {
            'pipeline_info': {
                'start_time': self.stats['start_time'].isoformat(),
                'end_time': self.stats['end_time'].isoformat(),
                'duration_seconds': duration.total_seconds(),
                'status': 'completed'
            },
            'statistics': {
                'total_tasks': self.stats['total_tasks'],
                'completed_tasks': self.stats['completed_tasks'],
                'failed_tasks': self.stats['failed_tasks'],
                'processed_records': self.stats['processed_records'],
                'success_rate': (self.stats['completed_tasks'] / max(self.stats['total_tasks'], 1)) * 100
            }
        }
        
        # 保存报告
        report_path = Path(self.config['output']['report_path'])
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report, indent=2, ensure_ascii=False))
        
        self.logger.info(f"📊 报告已生成：{report_path}")
        
        # 打印摘要
        print("\n" + "="*60)
        print("📊 数据流水线执行报告")
        print("="*60)
        print(f"⏱️  执行时间：{duration}")
        print(f"📋 总任务数：{self.stats['total_tasks']}")
        print(f"✅ 成功任务：{self.stats['completed_tasks']}")
        print(f"❌ 失败任务：{self.stats['failed_tasks']}")
        print(f"📊 处理记录：{self.stats['processed_records']}")
        print(f"📈 成功率：{report['statistics']['success_rate']:.1f}%")
        print("="*60)

# 配置文件示例
def create_sample_config():
    config = {
        "logging": {
            "file": "D:/testyd/练习/pipeline.log"
        },
        "data_sources": [
            {
                "type": "api",
                "url": "https://httpbin.org/json",
                "data_path": ""
            },
            {
                "type": "file",
                "path": "D:/testyd/练习/sample_data.json"
            }
        ],
        "processing": {
            "rules": [
                {
                    "type": "add_field",
                    "name": "processed_time",
                    "value": "${current_time}"
                },
                {
                    "type": "add_field",
                    "name": "source",
                    "value": "data_pipeline"
                }
            ]
        },
        "storage": [
            {
                "type": "json",
                "path": "D:/testyd/练习/output/processed_data.json"
            },
            {
                "type": "excel",
                "path": "D:/testyd/练习/output/processed_data.xlsx"
            }
        ],
        "output": {
            "report_path": "D:/testyd/练习/output/pipeline_report.json"
        }
    }
    
    config_file = Path("D:/testyd/练习/pipeline_config.json")
    config_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    return str(config_file)

# 创建示例数据
def create_sample_data():
    sample_data = [
        {
            "id": 1,
            "name": "测试商品1",
            "price": 99.99,
            "category": "电子产品",
            "created_at": "2025-09-26T10:00:00Z"
        },
        {
            "id": 2,
            "name": "测试商品2",
            "price": 199.99,
            "category": "家居用品",
            "created_at": "2025-09-26T11:00:00Z"
        }
    ]
    
    data_file = Path("D:/testyd/练习/sample_data.json")
    data_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False)

# 测试完整流水线
async def test_data_pipeline():
    # 创建配置和示例数据
    config_file = create_sample_config()
    create_sample_data()
    
    # 运行流水线
    pipeline = DataPipeline(config_file)
    await pipeline.run_pipeline()

# 运行测试
if __name__ == "__main__":
    asyncio.run(test_data_pipeline())
```

---

## 🎓 学习建议

### 学习路径
1. **第1-2周**：完成初级练习，掌握基础语法
2. **第3-4周**：完成中级练习，理解面向对象和异常处理
3. **第5-6周**：完成高级练习，学习并发编程和高级特性
4. **第7周**：完成综合项目，整合所有知识点

### 实践要点
1. **每个练习都要亲自运行**，观察输出结果
2. **尝试修改代码**，看看会发生什么
3. **遇到错误不要害怕**，学会读懂错误信息
4. **多写注释**，帮助理解代码逻辑
5. **参考原始的jd_store.py**，对比学习

### 扩展练习
完成基础练习后，可以尝试：
1. 添加更多的数据处理功能
2. 实现图形界面（使用tkinter）
3. 添加数据库操作
4. 实现Web API接口
5. 添加单元测试

### 学习资源
- Python官方文档：https://docs.python.org/zh-cn/3/
- 菜鸟教程：https://www.runoob.com/python3/
- 廖雪峰Python教程：https://www.liaoxuefeng.com/wiki/1016959663602400

记住：**编程是一门实践性很强的技能，多写多练是关键！** 🚀