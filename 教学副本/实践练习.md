# Pythonå®è·µç»ƒä¹ é¢˜

## ğŸ“š ç»ƒä¹ è¯´æ˜

è¿™äº›ç»ƒä¹ é¢˜æŒ‰éš¾åº¦åˆ†ä¸ºåˆçº§ã€ä¸­çº§å’Œé«˜çº§ï¼Œæ¯ä¸ªç»ƒä¹ éƒ½æœ‰è¯¦ç»†çš„è¦æ±‚å’Œå‚è€ƒç­”æ¡ˆã€‚å»ºè®®æŒ‰é¡ºåºå®Œæˆï¼Œæ¯å®Œæˆä¸€ä¸ªç»ƒä¹ å°±è¿è¡Œæµ‹è¯•ï¼Œç¡®ä¿ä»£ç æ­£ç¡®ã€‚

---

## ğŸŸ¢ åˆçº§ç»ƒä¹ ï¼ˆç¬¬1-2å‘¨ï¼‰

### ç»ƒä¹ 1ï¼šåŸºç¡€æ•°æ®æ“ä½œ

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªç®€å•çš„åº—é“ºä¿¡æ¯ç®¡ç†ç¨‹åº

**è¦æ±‚ï¼š**
1. å®šä¹‰ä¸€ä¸ªåŒ…å«åº—é“ºä¿¡æ¯çš„å­—å…¸
2. å®ç°æ·»åŠ ã€æŸ¥è¯¢ã€ä¿®æ”¹åº—é“ºä¿¡æ¯çš„åŠŸèƒ½
3. ä½¿ç”¨å¾ªç¯æ˜¾ç¤ºæ‰€æœ‰åº—é“ºä¿¡æ¯

```python
# ç»ƒä¹ 1å‚è€ƒç­”æ¡ˆ
def shop_manager():
    """åº—é“ºä¿¡æ¯ç®¡ç†å™¨"""
    shops = {}
    
    def add_shop(name, profile, status="å¾…ä¸‹è½½"):
        """æ·»åŠ åº—é“º"""
        shops[name] = {
            "profile": profile,
            "status": status,
            "created_time": "2025-09-26"
        }
        print(f"âœ… åº—é“º {name} æ·»åŠ æˆåŠŸ")
    
    def get_shop(name):
        """æŸ¥è¯¢åº—é“º"""
        if name in shops:
            return shops[name]
        else:
            print(f"âŒ åº—é“º {name} ä¸å­˜åœ¨")
            return None
    
    def update_shop_status(name, status):
        """æ›´æ–°åº—é“ºçŠ¶æ€"""
        if name in shops:
            shops[name]["status"] = status
            print(f"âœ… åº—é“º {name} çŠ¶æ€æ›´æ–°ä¸ºï¼š{status}")
        else:
            print(f"âŒ åº—é“º {name} ä¸å­˜åœ¨")
    
    def list_all_shops():
        """æ˜¾ç¤ºæ‰€æœ‰åº—é“º"""
        print("\nğŸ“‹ æ‰€æœ‰åº—é“ºä¿¡æ¯ï¼š")
        print("-" * 50)
        for name, info in shops.items():
            print(f"åº—é“ºï¼š{name}")
            print(f"  é…ç½®ï¼š{info['profile']}")
            print(f"  çŠ¶æ€ï¼š{info['status']}")
            print(f"  åˆ›å»ºæ—¶é—´ï¼š{info['created_time']}")
            print("-" * 30)
    
    # æµ‹è¯•ä»£ç 
    add_shop("æµ‹è¯•åº—é“º1", "Profile 1")
    add_shop("æµ‹è¯•åº—é“º2", "Profile 2", "å·²å®Œæˆ")
    
    list_all_shops()
    
    update_shop_status("æµ‹è¯•åº—é“º1", "å·²å®Œæˆ")
    
    shop_info = get_shop("æµ‹è¯•åº—é“º1")
    if shop_info:
        print(f"æŸ¥è¯¢ç»“æœï¼š{shop_info}")

# è¿è¡Œç»ƒä¹ 
shop_manager()
```

### ç»ƒä¹ 2ï¼šæ–‡ä»¶è·¯å¾„å¤„ç†

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªæ–‡ä»¶è·¯å¾„å·¥å…·ç±»

**è¦æ±‚ï¼š**
1. ä½¿ç”¨pathlibå¤„ç†æ–‡ä»¶è·¯å¾„
2. å®ç°åˆ›å»ºç›®å½•ã€æ£€æŸ¥æ–‡ä»¶å­˜åœ¨ã€ç”Ÿæˆæ–‡ä»¶åç­‰åŠŸèƒ½
3. å¤„ç†ä¸åŒæ“ä½œç³»ç»Ÿçš„è·¯å¾„å·®å¼‚

```python
# ç»ƒä¹ 2å‚è€ƒç­”æ¡ˆ
from pathlib import Path
from datetime import datetime

class FilePathHelper:
    """æ–‡ä»¶è·¯å¾„åŠ©æ‰‹ç±»"""
    
    def __init__(self, base_path):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def create_date_directory(self, date_str=None):
        """åˆ›å»ºæ—¥æœŸç›®å½•"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y-%m-%d')
        
        date_dir = self.base_path / date_str
        date_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºç›®å½•ï¼š{date_dir}")
        return date_dir
    
    def generate_filename(self, shop_name, file_type="xlsx"):
        """ç”Ÿæˆæ–‡ä»¶å"""
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        clean_name = "".join(c for c in shop_name if c.isalnum() or c in (' ', '-', '_')).strip()
        filename = f"{clean_name}.{file_type}"
        return filename
    
    def get_file_path(self, shop_name, date_str=None, file_type="xlsx"):
        """è·å–å®Œæ•´æ–‡ä»¶è·¯å¾„"""
        date_dir = self.create_date_directory(date_str)
        filename = self.generate_filename(shop_name, file_type)
        return date_dir / filename
    
    def check_file_exists(self, file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        path = Path(file_path)
        exists = path.exists()
        print(f"ğŸ“„ æ–‡ä»¶ {path.name} {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
        return exists
    
    def list_files_in_directory(self, directory=None, pattern="*"):
        """åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶"""
        if directory is None:
            directory = self.base_path
        else:
            directory = Path(directory)
        
        files = list(directory.glob(pattern))
        print(f"ğŸ“‚ ç›®å½• {directory} ä¸­çš„æ–‡ä»¶ï¼š")
        for file in files:
            print(f"  - {file.name}")
        return files

# æµ‹è¯•ä»£ç 
def test_file_helper():
    helper = FilePathHelper("D:/testyd/ç»ƒä¹ ")
    
    # åˆ›å»ºæ–‡ä»¶è·¯å¾„
    file_path1 = helper.get_file_path("æµ‹è¯•åº—é“º 1", "2025-09-26")
    file_path2 = helper.get_file_path("æµ‹è¯•åº—é“º@2", "2025-09-26")
    
    print(f"æ–‡ä»¶è·¯å¾„1ï¼š{file_path1}")
    print(f"æ–‡ä»¶è·¯å¾„2ï¼š{file_path2}")
    
    # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
    helper.check_file_exists(file_path1)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    file_path1.touch()  # åˆ›å»ºç©ºæ–‡ä»¶
    helper.check_file_exists(file_path1)
    
    # åˆ—å‡ºæ–‡ä»¶
    helper.list_files_in_directory(file_path1.parent, "*.xlsx")

# è¿è¡Œæµ‹è¯•
test_file_helper()
```

### ç»ƒä¹ 3ï¼šç®€å•çš„æ—¥å¿—ç³»ç»Ÿ

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªç®€å•çš„æ—¥å¿—è®°å½•ç³»ç»Ÿ

**è¦æ±‚ï¼š**
1. å®ç°ä¸åŒçº§åˆ«çš„æ—¥å¿—ï¼ˆINFOã€WARNINGã€ERRORï¼‰
2. æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
3. æ—¥å¿—æ ¼å¼åŒ…å«æ—¶é—´ã€çº§åˆ«ã€æ¶ˆæ¯

```python
# ç»ƒä¹ 3å‚è€ƒç­”æ¡ˆ
import logging
from datetime import datetime
from pathlib import Path

class SimpleLogger:
    """ç®€å•æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, name="MyApp", log_file="app.log"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # é¿å…é‡å¤æ·»åŠ å¤„ç†å™¨
        if not self.logger.handlers:
            # åˆ›å»ºæ ¼å¼åŒ–å™¨
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
            
            # æ–‡ä»¶å¤„ç†å™¨
            log_path = Path(log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def info(self, message):
        """ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message)
    
    def warning(self, message):
        """è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message)
    
    def error(self, message):
        """é”™è¯¯æ—¥å¿—"""
        self.logger.error(message)
    
    def log_function_start(self, function_name, **kwargs):
        """è®°å½•å‡½æ•°å¼€å§‹"""
        params = ", ".join(f"{k}={v}" for k, v in kwargs.items())
        self.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {function_name}({params})")
    
    def log_function_end(self, function_name, success=True, result=None):
        """è®°å½•å‡½æ•°ç»“æŸ"""
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        message = f"{status} å®Œæˆ {function_name}"
        if result is not None:
            message += f"ï¼Œç»“æœï¼š{result}"
        
        if success:
            self.info(message)
        else:
            self.error(message)

# æµ‹è¯•ä»£ç 
def test_logger():
    logger = SimpleLogger("TestApp", "D:/testyd/ç»ƒä¹ /test.log")
    
    # æ¨¡æ‹Ÿä¸‹è½½å‡½æ•°
    def download_shop_data(shop_name, profile):
        logger.log_function_start("download_shop_data", 
                                shop_name=shop_name, profile=profile)
        
        try:
            # æ¨¡æ‹Ÿä¸‹è½½è¿‡ç¨‹
            logger.info(f"æ­£åœ¨è¿æ¥åˆ° {shop_name} çš„æ•°æ®æº...")
            logger.info(f"ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼š{profile}")
            
            # æ¨¡æ‹Ÿå¯èƒ½çš„è­¦å‘Š
            if "æµ‹è¯•" in shop_name:
                logger.warning("è¿™æ˜¯æµ‹è¯•åº—é“ºï¼Œæ•°æ®å¯èƒ½ä¸å®Œæ•´")
            
            # æ¨¡æ‹ŸæˆåŠŸ
            logger.log_function_end("download_shop_data", success=True, 
                                  result="æ–‡ä»¶å·²ä¿å­˜")
            return True
            
        except Exception as e:
            logger.log_function_end("download_shop_data", success=False, 
                                  result=str(e))
            return False
    
    # æµ‹è¯•æ—¥å¿—è®°å½•
    download_shop_data("æµ‹è¯•åº—é“º1", "Profile 1")
    download_shop_data("æ­£å¼åº—é“º1", "Profile 2")
    
    logger.info("æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ")

# è¿è¡Œæµ‹è¯•
test_logger()
```

---

## ğŸŸ¡ ä¸­çº§ç»ƒä¹ ï¼ˆç¬¬3-4å‘¨ï¼‰

### ç»ƒä¹ 4ï¼šExcelæ•°æ®å¤„ç†å™¨

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªExcelæ•°æ®å¤„ç†ç±»

**è¦æ±‚ï¼š**
1. è¯»å–Excelæ–‡ä»¶å¹¶åˆ†ææ•°æ®ç»“æ„
2. å®ç°æ•°æ®ç­›é€‰ã€ä¿®æ”¹ã€ç»Ÿè®¡åŠŸèƒ½
3. æ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªExcelæ–‡ä»¶

```python
# ç»ƒä¹ 4å‚è€ƒç­”æ¡ˆ
import pandas as pd
from pathlib import Path
import numpy as np

class ExcelProcessor:
    """Excelæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.data = None
        self.file_path = None
    
    def load_excel(self, file_path, sheet_name=0):
        """åŠ è½½Excelæ–‡ä»¶"""
        try:
            self.file_path = Path(file_path)
            self.data = pd.read_excel(file_path, sheet_name=sheet_name)
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶ï¼š{self.file_path.name}")
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶ï¼š{self.data.shape}")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ï¼š{e}")
            return False
    
    def analyze_data(self):
        """åˆ†ææ•°æ®ç»“æ„"""
        if self.data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        print("\nğŸ“‹ æ•°æ®åˆ†ææŠ¥å‘Šï¼š")
        print("-" * 50)
        print(f"è¡Œæ•°ï¼š{len(self.data)}")
        print(f"åˆ—æ•°ï¼š{len(self.data.columns)}")
        print(f"åˆ—åï¼š{list(self.data.columns)}")
        
        print("\nğŸ“Š æ•°æ®ç±»å‹ï¼š")
        for col in self.data.columns:
            dtype = self.data[col].dtype
            null_count = self.data[col].isnull().sum()
            print(f"  {col}: {dtype} (ç©ºå€¼: {null_count})")
        
        print("\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡ï¼š")
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            print(self.data[numeric_cols].describe())
        else:
            print("  æ— æ•°å€¼åˆ—")
    
    def filter_data(self, column, condition, value):
        """ç­›é€‰æ•°æ®"""
        if self.data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return None
        
        try:
            if condition == "ç­‰äº":
                filtered = self.data[self.data[column] == value]
            elif condition == "åŒ…å«":
                filtered = self.data[self.data[column].str.contains(str(value), na=False)]
            elif condition == "å¤§äº":
                filtered = self.data[self.data[column] > value]
            elif condition == "å°äº":
                filtered = self.data[self.data[column] < value]
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ¡ä»¶ï¼š{condition}")
                return None
            
            print(f"âœ… ç­›é€‰ç»“æœï¼š{len(filtered)} è¡Œæ•°æ®")
            return filtered
            
        except Exception as e:
            print(f"âŒ ç­›é€‰å¤±è´¥ï¼š{e}")
            return None
    
    def update_column(self, column, condition_column, condition_value, new_value):
        """æ›´æ–°åˆ—æ•°æ®"""
        if self.data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return False
        
        try:
            mask = self.data[condition_column] == condition_value
            affected_rows = mask.sum()
            
            if affected_rows > 0:
                self.data.loc[mask, column] = new_value
                print(f"âœ… æ›´æ–°äº† {affected_rows} è¡Œæ•°æ®")
                return True
            else:
                print("âš ï¸ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                return False
                
        except Exception as e:
            print(f"âŒ æ›´æ–°å¤±è´¥ï¼š{e}")
            return False
    
    def save_excel(self, output_path=None):
        """ä¿å­˜Excelæ–‡ä»¶"""
        if self.data is None:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        try:
            if output_path is None:
                output_path = self.file_path
            
            self.data.to_excel(output_path, index=False, engine='openpyxl')
            print(f"âœ… æ–‡ä»¶å·²ä¿å­˜ï¼š{output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥ï¼š{e}")
            return False
    
    def batch_process(self, directory, pattern="*.xlsx"):
        """æ‰¹é‡å¤„ç†Excelæ–‡ä»¶"""
        directory = Path(directory)
        files = list(directory.glob(pattern))
        
        print(f"ğŸ“‚ æ‰¾åˆ° {len(files)} ä¸ªExcelæ–‡ä»¶")
        
        results = []
        for file_path in files:
            print(f"\nğŸ”„ å¤„ç†æ–‡ä»¶ï¼š{file_path.name}")
            
            processor = ExcelProcessor()
            if processor.load_excel(file_path):
                processor.analyze_data()
                results.append({
                    'file': file_path.name,
                    'rows': len(processor.data),
                    'columns': len(processor.data.columns),
                    'success': True
                })
            else:
                results.append({
                    'file': file_path.name,
                    'success': False
                })
        
        # æ±‡æ€»æŠ¥å‘Š
        print("\nğŸ“Š æ‰¹é‡å¤„ç†æŠ¥å‘Šï¼š")
        print("-" * 50)
        successful = sum(1 for r in results if r['success'])
        print(f"æˆåŠŸå¤„ç†ï¼š{successful}/{len(results)} ä¸ªæ–‡ä»¶")
        
        for result in results:
            if result['success']:
                print(f"âœ… {result['file']}: {result['rows']}è¡Œ x {result['columns']}åˆ—")
            else:
                print(f"âŒ {result['file']}: å¤„ç†å¤±è´¥")
        
        return results

# æµ‹è¯•ä»£ç 
def test_excel_processor():
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = {
        'åº—é“ºåç§°': ['åº—é“º1', 'åº—é“º2', 'åº—é“º3', 'åº—é“º4'],
        'é…ç½®æ–‡ä»¶': ['Profile 1', 'Profile 2', 'Profile 1', 'Profile 3'],
        'çŠ¶æ€': ['å¾…ä¸‹è½½', 'å·²å®Œæˆ', 'å¾…ä¸‹è½½', 'å·²å®Œæˆ'],
        'é”€å”®é¢': [1000, 2000, 1500, 3000],
        'åº“å­˜é‡': [100, 50, 200, 80]
    }
    
    df = pd.DataFrame(test_data)
    test_file = Path("D:/testyd/ç»ƒä¹ /test_data.xlsx")
    test_file.parent.mkdir(parents=True, exist_ok=True)
    df.to_excel(test_file, index=False, engine='openpyxl')
    
    # æµ‹è¯•å¤„ç†å™¨
    processor = ExcelProcessor()
    
    # åŠ è½½å’Œåˆ†æ
    processor.load_excel(test_file)
    processor.analyze_data()
    
    # ç­›é€‰æ•°æ®
    filtered = processor.filter_data('çŠ¶æ€', 'ç­‰äº', 'å¾…ä¸‹è½½')
    if filtered is not None:
        print(f"\nç­›é€‰ç»“æœï¼š\n{filtered}")
    
    # æ›´æ–°æ•°æ®
    processor.update_column('çŠ¶æ€', 'åº—é“ºåç§°', 'åº—é“º1', 'å·²å®Œæˆ')
    
    # ä¿å­˜æ–‡ä»¶
    output_file = test_file.parent / "processed_data.xlsx"
    processor.save_excel(output_file)

# è¿è¡Œæµ‹è¯•
test_excel_processor()
```

### ç»ƒä¹ 5ï¼šç½‘ç»œè¯·æ±‚å°è£…å™¨

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªç½‘ç»œè¯·æ±‚å°è£…ç±»

**è¦æ±‚ï¼š**
1. å°è£…GETã€POSTè¯·æ±‚æ–¹æ³•
2. æ”¯æŒé‡è¯•æœºåˆ¶å’Œè¶…æ—¶è®¾ç½®
3. å®ç°Cookieç®¡ç†å’Œè¯·æ±‚å¤´è®¾ç½®

```python
# ç»ƒä¹ 5å‚è€ƒç­”æ¡ˆ
import requests
import time
import json
from typing import Dict, Optional, Any

class HttpClient:
    """HTTPå®¢æˆ·ç«¯å°è£…å™¨"""
    
    def __init__(self, base_url="", timeout=30, max_retries=3):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.max_retries = max_retries
        self.session = requests.Session()
        
        # é»˜è®¤è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def set_headers(self, headers: Dict[str, str]):
        """è®¾ç½®è¯·æ±‚å¤´"""
        self.session.headers.update(headers)
        print(f"âœ… å·²æ›´æ–°è¯·æ±‚å¤´ï¼š{list(headers.keys())}")
    
    def set_cookies(self, cookies: Dict[str, str]):
        """è®¾ç½®Cookie"""
        self.session.cookies.update(cookies)
        print(f"âœ… å·²æ›´æ–°Cookieï¼š{len(cookies)} ä¸ª")
    
    def _make_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:
        """å‘é€è¯·æ±‚çš„å†…éƒ¨æ–¹æ³•"""
        # å¤„ç†URL
        if not url.startswith('http'):
            url = f"{self.base_url}/{url.lstrip('/')}"
        
        # è®¾ç½®è¶…æ—¶
        kwargs.setdefault('timeout', self.timeout)
        
        for attempt in range(self.max_retries):
            try:
                print(f"ğŸŒ å‘é€{method}è¯·æ±‚ï¼š{url} (å°è¯• {attempt + 1}/{self.max_retries})")
                
                response = self.session.request(method, url, **kwargs)
                
                print(f"ğŸ“¡ å“åº”çŠ¶æ€ç ï¼š{response.status_code}")
                
                # æ£€æŸ¥çŠ¶æ€ç 
                response.raise_for_status()
                
                return response
                
            except requests.exceptions.Timeout:
                print(f"â° è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    
            except requests.exceptions.RequestException as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ï¼š{e} (å°è¯• {attempt + 1}/{self.max_retries})")
                if attempt < self.max_retries - 1:
                    time.sleep(2 ** attempt)
        
        print(f"âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
        return None
    
    def get(self, url: str, params: Optional[Dict] = None) -> Optional[Dict]:
        """å‘é€GETè¯·æ±‚"""
        response = self._make_request('GET', url, params=params)
        return self._parse_response(response)
    
    def post(self, url: str, data: Optional[Dict] = None, 
             json_data: Optional[Dict] = None) -> Optional[Dict]:
        """å‘é€POSTè¯·æ±‚"""
        kwargs = {}
        if data:
            kwargs['data'] = data
        if json_data:
            kwargs['json'] = json_data
            
        response = self._make_request('POST', url, **kwargs)
        return self._parse_response(response)
    
    def download_file(self, url: str, file_path: str) -> bool:
        """ä¸‹è½½æ–‡ä»¶"""
        response = self._make_request('GET', url, stream=True)
        
        if response is None:
            return False
        
        try:
            with open(file_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸï¼š{file_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{e}")
            return False
    
    def _parse_response(self, response: Optional[requests.Response]) -> Optional[Dict]:
        """è§£æå“åº”"""
        if response is None:
            return None
        
        try:
            # å°è¯•è§£æJSON
            if 'application/json' in response.headers.get('content-type', ''):
                return response.json()
            else:
                return {'text': response.text, 'status_code': response.status_code}
                
        except json.JSONDecodeError:
            return {'text': response.text, 'status_code': response.status_code}
    
    def close(self):
        """å…³é—­ä¼šè¯"""
        self.session.close()
        print("âœ… HTTPä¼šè¯å·²å…³é—­")

# æµ‹è¯•ä»£ç 
def test_http_client():
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = HttpClient(base_url="https://httpbin.org", timeout=10, max_retries=2)
    
    # è®¾ç½®è¯·æ±‚å¤´
    client.set_headers({
        'Accept': 'application/json',
        'Custom-Header': 'test-value'
    })
    
    # æµ‹è¯•GETè¯·æ±‚
    print("\nğŸ” æµ‹è¯•GETè¯·æ±‚ï¼š")
    get_result = client.get('/get', params={'key': 'value', 'test': 'æµ‹è¯•'})
    if get_result:
        print(f"GETå“åº”ï¼š{json.dumps(get_result, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•POSTè¯·æ±‚
    print("\nğŸ“¤ æµ‹è¯•POSTè¯·æ±‚ï¼š")
    post_data = {'name': 'æµ‹è¯•', 'value': 123}
    post_result = client.post('/post', json_data=post_data)
    if post_result:
        print(f"POSTå“åº”ï¼š{json.dumps(post_result, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•é”™è¯¯å¤„ç†
    print("\nâŒ æµ‹è¯•é”™è¯¯å¤„ç†ï¼š")
    error_result = client.get('/status/404')  # è¿™ä¼šè¿”å›404é”™è¯¯
    
    # å…³é—­å®¢æˆ·ç«¯
    client.close()

# è¿è¡Œæµ‹è¯•
test_http_client()
```

---

## ğŸ”´ é«˜çº§ç»ƒä¹ ï¼ˆç¬¬5-6å‘¨ï¼‰

### ç»ƒä¹ 6ï¼šå¤šçº¿ç¨‹ä¸‹è½½å™¨

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªæ”¯æŒå¤šçº¿ç¨‹çš„æ–‡ä»¶ä¸‹è½½å™¨

**è¦æ±‚ï¼š**
1. ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ä¸‹è½½å¤šä¸ªæ–‡ä»¶
2. å®ç°ä¸‹è½½è¿›åº¦ç›‘æ§
3. æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯é‡è¯•

```python
# ç»ƒä¹ 6å‚è€ƒç­”æ¡ˆ
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import requests
from typing import List, Dict, Callable, Optional

class MultiThreadDownloader:
    """å¤šçº¿ç¨‹ä¸‹è½½å™¨"""
    
    def __init__(self, max_workers=4, timeout=30):
        self.max_workers = max_workers
        self.timeout = timeout
        self.download_stats = {}
        self.lock = threading.Lock()
    
    def download_file(self, url: str, file_path: str, 
                     progress_callback: Optional[Callable] = None) -> Dict:
        """ä¸‹è½½å•ä¸ªæ–‡ä»¶"""
        thread_id = threading.current_thread().ident
        file_path = Path(file_path)
        
        result = {
            'url': url,
            'file_path': str(file_path),
            'success': False,
            'error': None,
            'size': 0,
            'thread_id': thread_id
        }
        
        try:
            print(f"ğŸ§µ çº¿ç¨‹{thread_id}: å¼€å§‹ä¸‹è½½ {file_path.name}")
            
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            resume_pos = 0
            if file_path.exists():
                resume_pos = file_path.stat().st_size
                print(f"ğŸ“ å‘ç°å·²å­˜åœ¨æ–‡ä»¶ï¼Œä»ä½ç½® {resume_pos} ç»§ç»­ä¸‹è½½")
            
            # è®¾ç½®è¯·æ±‚å¤´æ”¯æŒæ–­ç‚¹ç»­ä¼ 
            headers = {}
            if resume_pos > 0:
                headers['Range'] = f'bytes={resume_pos}-'
            
            # å‘é€è¯·æ±‚
            response = requests.get(url, headers=headers, stream=True, timeout=self.timeout)
            response.raise_for_status()
            
            # è·å–æ–‡ä»¶æ€»å¤§å°
            total_size = int(response.headers.get('content-length', 0))
            if resume_pos > 0:
                total_size += resume_pos
            
            # æ‰“å¼€æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼å¦‚æœæ˜¯æ–­ç‚¹ç»­ä¼ ï¼‰
            mode = 'ab' if resume_pos > 0 else 'wb'
            
            with open(file_path, mode) as f:
                downloaded = resume_pos
                
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        
                        # è°ƒç”¨è¿›åº¦å›è°ƒ
                        if progress_callback:
                            progress = (downloaded / total_size * 100) if total_size > 0 else 0
                            progress_callback(file_path.name, downloaded, total_size, progress)
            
            result['success'] = True
            result['size'] = downloaded
            print(f"âœ… çº¿ç¨‹{thread_id}: {file_path.name} ä¸‹è½½å®Œæˆ ({downloaded} å­—èŠ‚)")
            
        except Exception as e:
            result['error'] = str(e)
            print(f"âŒ çº¿ç¨‹{thread_id}: {file_path.name} ä¸‹è½½å¤±è´¥: {e}")
        
        return result
    
    def progress_callback(self, filename: str, downloaded: int, total: int, progress: float):
        """è¿›åº¦å›è°ƒå‡½æ•°"""
        with self.lock:
            self.download_stats[filename] = {
                'downloaded': downloaded,
                'total': total,
                'progress': progress
            }
            
            # æ¯ä¸‹è½½1MBæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if downloaded % (1024 * 1024) < 8192:  # 8KB chunk size
                print(f"ğŸ“Š {filename}: {progress:.1f}% ({downloaded}/{total} å­—èŠ‚)")
    
    def batch_download(self, download_tasks: List[Dict]) -> List[Dict]:
        """æ‰¹é‡ä¸‹è½½æ–‡ä»¶"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")
        print(f"ğŸ“‹ ä»»åŠ¡æ•°é‡ï¼š{len(download_tasks)}")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_task = {}
            
            for task in download_tasks:
                future = executor.submit(
                    self.download_file,
                    task['url'],
                    task['file_path'],
                    self.progress_callback
                )
                future_to_task[future] = task
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ï¼š{e}")
                    results.append({
                        'url': task['url'],
                        'file_path': task['file_path'],
                        'success': False,
                        'error': str(e)
                    })
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r['success'])
        total_size = sum(r['size'] for r in results if r['success'])
        
        print(f"\nğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡ï¼š")
        print(f"âœ… æˆåŠŸï¼š{successful}/{len(results)}")
        print(f"ğŸ“¦ æ€»å¤§å°ï¼š{total_size / (1024*1024):.2f} MB")
        
        return results
    
    def monitor_progress(self, interval=2):
        """ç›‘æ§ä¸‹è½½è¿›åº¦"""
        def monitor():
            while True:
                time.sleep(interval)
                with self.lock:
                    if not self.download_stats:
                        continue
                    
                    print("\nğŸ“ˆ å®æ—¶è¿›åº¦ï¼š")
                    for filename, stats in self.download_stats.items():
                        progress = stats['progress']
                        downloaded = stats['downloaded'] / (1024*1024)  # MB
                        total = stats['total'] / (1024*1024)  # MB
                        print(f"  {filename}: {progress:.1f}% ({downloaded:.1f}/{total:.1f} MB)")
        
        monitor_thread = threading.Thread(target=monitor, daemon=True)
        monitor_thread.start()
        return monitor_thread

# æµ‹è¯•ä»£ç 
def test_multi_thread_downloader():
    downloader = MultiThreadDownloader(max_workers=3, timeout=30)
    
    # åˆ›å»ºæµ‹è¯•ä¸‹è½½ä»»åŠ¡
    download_tasks = [
        {
            'url': 'https://httpbin.org/bytes/1048576',  # 1MBæµ‹è¯•æ–‡ä»¶
            'file_path': 'D:/testyd/ç»ƒä¹ /downloads/test1.bin'
        },
        {
            'url': 'https://httpbin.org/bytes/2097152',  # 2MBæµ‹è¯•æ–‡ä»¶
            'file_path': 'D:/testyd/ç»ƒä¹ /downloads/test2.bin'
        },
        {
            'url': 'https://httpbin.org/bytes/1572864',  # 1.5MBæµ‹è¯•æ–‡ä»¶
            'file_path': 'D:/testyd/ç»ƒä¹ /downloads/test3.bin'
        }
    ]
    
    # åˆ›å»ºä¸‹è½½ç›®å½•
    download_dir = Path('D:/testyd/ç»ƒä¹ /downloads')
    download_dir.mkdir(parents=True, exist_ok=True)
    
    # å¯åŠ¨è¿›åº¦ç›‘æ§
    monitor_thread = downloader.monitor_progress(interval=1)
    
    # å¼€å§‹æ‰¹é‡ä¸‹è½½
    results = downloader.batch_download(download_tasks)
    
    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
    print("\nğŸ“‹ è¯¦ç»†ç»“æœï¼š")
    for result in results:
        status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
        size_mb = result['size'] / (1024*1024)
        print(f"{status} {Path(result['file_path']).name}: {size_mb:.2f} MB")
        if not result['success']:
            print(f"   é”™è¯¯ï¼š{result['error']}")

# è¿è¡Œæµ‹è¯•
test_multi_thread_downloader()
```

### ç»ƒä¹ 7ï¼šé…ç½®ç®¡ç†å™¨

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªçµæ´»çš„é…ç½®ç®¡ç†ç³»ç»Ÿ

**è¦æ±‚ï¼š**
1. æ”¯æŒå¤šç§é…ç½®æ ¼å¼ï¼ˆJSONã€YAMLã€INIï¼‰
2. å®ç°é…ç½®çƒ­é‡è½½å’ŒéªŒè¯
3. æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–å’Œé…ç½®ç»§æ‰¿

```python
# ç»ƒä¹ 7å‚è€ƒç­”æ¡ˆ
import json
import configparser
import os
from pathlib import Path
from typing import Dict, Any, Optional, Union
import threading
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_file: str, auto_reload=True):
        self.config_file = Path(config_file)
        self.config_data = {}
        self.lock = threading.RLock()
        self.observers = []
        self.auto_reload = auto_reload
        
        # åŠ è½½é…ç½®
        self.load_config()
        
        # å¯åŠ¨è‡ªåŠ¨é‡è½½
        if auto_reload:
            self.start_auto_reload()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with self.lock:
            try:
                if not self.config_file.exists():
                    print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{self.config_file}")
                    return
                
                file_ext = self.config_file.suffix.lower()
                
                if file_ext == '.json':
                    self._load_json()
                elif file_ext == '.ini':
                    self._load_ini()
                elif file_ext in ['.yml', '.yaml']:
                    self._load_yaml()
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®æ–‡ä»¶æ ¼å¼ï¼š{file_ext}")
                
                # åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–
                self._apply_env_overrides()
                
                print(f"âœ… é…ç½®åŠ è½½æˆåŠŸï¼š{self.config_file}")
                
            except Exception as e:
                print(f"âŒ é…ç½®åŠ è½½å¤±è´¥ï¼š{e}")
    
    def _load_json(self):
        """åŠ è½½JSONé…ç½®"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config_data = json.load(f)
    
    def _load_ini(self):
        """åŠ è½½INIé…ç½®"""
        config = configparser.ConfigParser()
        config.read(self.config_file, encoding='utf-8')
        
        self.config_data = {}
        for section in config.sections():
            self.config_data[section] = dict(config[section])
    
    def _load_yaml(self):
        """åŠ è½½YAMLé…ç½®"""
        try:
            import yaml
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config_data = yaml.safe_load(f)
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…PyYAMLåº“ï¼špip install PyYAML")
    
    def _apply_env_overrides(self):
        """åº”ç”¨ç¯å¢ƒå˜é‡è¦†ç›–"""
        env_prefix = "APP_CONFIG_"
        
        for key, value in os.environ.items():
            if key.startswith(env_prefix):
                config_key = key[len(env_prefix):].lower()
                
                # æ”¯æŒåµŒå¥—é”®ï¼ˆç”¨åŒä¸‹åˆ’çº¿åˆ†éš”ï¼‰
                keys = config_key.split('__')
                
                # å°è¯•è½¬æ¢å€¼ç±»å‹
                converted_value = self._convert_env_value(value)
                
                # è®¾ç½®é…ç½®å€¼
                self._set_nested_value(self.config_data, keys, converted_value)
                print(f"ğŸ”§ ç¯å¢ƒå˜é‡è¦†ç›–ï¼š{config_key} = {converted_value}")
    
    def _convert_env_value(self, value: str) -> Any:
        """è½¬æ¢ç¯å¢ƒå˜é‡å€¼ç±»å‹"""
        # å°è¯•è½¬æ¢ä¸ºæ•°å­—
        try:
            if '.' in value:
                return float(value)
            else:
                return int(value)
        except ValueError:
            pass
        
        # å°è¯•è½¬æ¢ä¸ºå¸ƒå°”å€¼
        if value.lower() in ['true', 'false']:
            return value.lower() == 'true'
        
        # å°è¯•è½¬æ¢ä¸ºJSON
        if value.startswith('{') or value.startswith('['):
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                pass
        
        # è¿”å›å­—ç¬¦ä¸²
        return value
    
    def _set_nested_value(self, data: Dict, keys: list, value: Any):
        """è®¾ç½®åµŒå¥—å­—å…¸å€¼"""
        for key in keys[:-1]:
            if key not in data:
                data[key] = {}
            data = data[key]
        data[keys[-1]] = value
    
    def get(self, key: str, default: Any = None) -> Any:
        """è·å–é…ç½®å€¼"""
        with self.lock:
            keys = key.split('.')
            data = self.config_data
            
            try:
                for k in keys:
                    data = data[k]
                return data
            except (KeyError, TypeError):
                return default
    
    def set(self, key: str, value: Any):
        """è®¾ç½®é…ç½®å€¼"""
        with self.lock:
            keys = key.split('.')
            self._set_nested_value(self.config_data, keys, value)
    
    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        with self.lock:
            try:
                file_ext = self.config_file.suffix.lower()
                
                if file_ext == '.json':
                    with open(self.config_file, 'w', encoding='utf-8') as f:
                        json.dump(self.config_data, f, indent=2, ensure_ascii=False)
                else:
                    print(f"âš ï¸ æš‚ä¸æ”¯æŒä¿å­˜ {file_ext} æ ¼å¼")
                    return
                
                print(f"âœ… é…ç½®å·²ä¿å­˜ï¼š{self.config_file}")
                
            except Exception as e:
                print(f"âŒ é…ç½®ä¿å­˜å¤±è´¥ï¼š{e}")
    
    def validate_config(self, schema: Dict) -> bool:
        """éªŒè¯é…ç½®"""
        def validate_item(data, schema_item, path=""):
            if isinstance(schema_item, dict):
                if 'type' in schema_item:
                    expected_type = schema_item['type']
                    if expected_type == 'string' and not isinstance(data, str):
                        print(f"âŒ {path}: æœŸæœ›å­—ç¬¦ä¸²ï¼Œå®é™… {type(data).__name__}")
                        return False
                    elif expected_type == 'number' and not isinstance(data, (int, float)):
                        print(f"âŒ {path}: æœŸæœ›æ•°å­—ï¼Œå®é™… {type(data).__name__}")
                        return False
                    elif expected_type == 'boolean' and not isinstance(data, bool):
                        print(f"âŒ {path}: æœŸæœ›å¸ƒå°”å€¼ï¼Œå®é™… {type(data).__name__}")
                        return False
                
                if 'required' in schema_item:
                    for req_key in schema_item['required']:
                        if req_key not in data:
                            print(f"âŒ {path}: ç¼ºå°‘å¿…éœ€å­—æ®µ {req_key}")
                            return False
                
                if 'properties' in schema_item:
                    for prop_key, prop_schema in schema_item['properties'].items():
                        if prop_key in data:
                            if not validate_item(data[prop_key], prop_schema, f"{path}.{prop_key}"):
                                return False
            
            return True
        
        with self.lock:
            is_valid = validate_item(self.config_data, schema)
            if is_valid:
                print("âœ… é…ç½®éªŒè¯é€šè¿‡")
            return is_valid
    
    def start_auto_reload(self):
        """å¯åŠ¨è‡ªåŠ¨é‡è½½"""
        class ConfigFileHandler(FileSystemEventHandler):
            def __init__(self, config_manager):
                self.config_manager = config_manager
            
            def on_modified(self, event):
                if not event.is_directory and Path(event.src_path) == self.config_manager.config_file:
                    print(f"ğŸ”„ æ£€æµ‹åˆ°é…ç½®æ–‡ä»¶å˜åŒ–ï¼Œé‡æ–°åŠ è½½...")
                    time.sleep(0.1)  # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
                    self.config_manager.load_config()
        
        observer = Observer()
        observer.schedule(
            ConfigFileHandler(self),
            str(self.config_file.parent),
            recursive=False
        )
        observer.start()
        self.observers.append(observer)
        print(f"ğŸ‘€ å·²å¯åŠ¨é…ç½®æ–‡ä»¶ç›‘æ§ï¼š{self.config_file}")
    
    def stop_auto_reload(self):
        """åœæ­¢è‡ªåŠ¨é‡è½½"""
        for observer in self.observers:
            observer.stop()
            observer.join()
        self.observers.clear()
        print("ğŸ›‘ å·²åœæ­¢é…ç½®æ–‡ä»¶ç›‘æ§")
    
    def __del__(self):
        """ææ„å‡½æ•°"""
        self.stop_auto_reload()

# æµ‹è¯•ä»£ç 
def test_config_manager():
    # åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
    config_dir = Path("D:/testyd/ç»ƒä¹ /config")
    config_dir.mkdir(parents=True, exist_ok=True)
    
    config_file = config_dir / "app_config.json"
    test_config = {
        "app": {
            "name": "æµ‹è¯•åº”ç”¨",
            "version": "1.0.0",
            "debug": True
        },
        "database": {
            "host": "localhost",
            "port": 3306,
            "username": "admin",
            "password": "password"
        },
        "download": {
            "max_workers": 4,
            "timeout": 30,
            "retry_count": 3
        }
    }
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, indent=2, ensure_ascii=False)
    
    # æµ‹è¯•é…ç½®ç®¡ç†å™¨
    config = ConfigManager(config_file, auto_reload=True)
    
    # æµ‹è¯•è·å–é…ç½®
    print(f"åº”ç”¨åç§°ï¼š{config.get('app.name')}")
    print(f"æ•°æ®åº“ç«¯å£ï¼š{config.get('database.port')}")
    print(f"æœ€å¤§å·¥ä½œçº¿ç¨‹ï¼š{config.get('download.max_workers')}")
    print(f"ä¸å­˜åœ¨çš„é…ç½®ï¼š{config.get('nonexistent.key', 'é»˜è®¤å€¼')}")
    
    # æµ‹è¯•è®¾ç½®é…ç½®
    config.set('app.debug', False)
    config.set('new_section.new_key', 'new_value')
    
    # æµ‹è¯•é…ç½®éªŒè¯
    schema = {
        "type": "object",
        "required": ["app", "database"],
        "properties": {
            "app": {
                "type": "object",
                "required": ["name", "version"],
                "properties": {
                    "name": {"type": "string"},
                    "version": {"type": "string"},
                    "debug": {"type": "boolean"}
                }
            },
            "database": {
                "type": "object",
                "required": ["host", "port"],
                "properties": {
                    "host": {"type": "string"},
                    "port": {"type": "number"}
                }
            }
        }
    }
    
    config.validate_config(schema)
    
    # ä¿å­˜é…ç½®
    config.save_config()
    
    print("\nğŸ’¡ æç¤ºï¼šä¿®æ”¹é…ç½®æ–‡ä»¶å†…å®¹ï¼Œè§‚å¯Ÿè‡ªåŠ¨é‡è½½æ•ˆæœ")
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥è§‚å¯Ÿè‡ªåŠ¨é‡è½½
    time.sleep(2)
    
    return config

# è¿è¡Œæµ‹è¯•
config = test_config_manager()
```

---

## ğŸ¯ ç»¼åˆé¡¹ç›®ç»ƒä¹ 

### ç»ƒä¹ 8ï¼šå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿

**é¢˜ç›®ï¼š** åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿

**è¦æ±‚ï¼š**
1. æ•´åˆå‰é¢æ‰€æœ‰ç»ƒä¹ çš„åŠŸèƒ½
2. å®ç°æ•°æ®é‡‡é›†ã€å¤„ç†ã€å­˜å‚¨çš„å®Œæ•´æµç¨‹
3. æ·»åŠ ç›‘æ§ã€æ—¥å¿—ã€é”™è¯¯å¤„ç†ç­‰åŠŸèƒ½

```python
# ç»ƒä¹ 8å‚è€ƒç­”æ¡ˆ
import asyncio
import aiohttp
import aiofiles
from datetime import datetime, timedelta
from pathlib import Path
import json
import pandas as pd
from typing import List, Dict, Any
import logging
from concurrent.futures import ThreadPoolExecutor
import time

class DataPipeline:
    """æ•°æ®å¤„ç†æµæ°´çº¿"""
    
    def __init__(self, config_file: str):
        self.config = self._load_config(config_file)
        self.logger = self._setup_logger()
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'processed_records': 0
        }
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        logger = logging.getLogger('DataPipeline')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # æ–‡ä»¶å¤„ç†å™¨
            log_file = Path(self.config['logging']['file'])
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            
            # æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            # æ ¼å¼åŒ–å™¨
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            file_handler.setFormatter(formatter)
            console_handler.setFormatter(formatter)
            
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    async def run_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æµæ°´çº¿"""
        self.stats['start_time'] = datetime.now()
        self.logger.info("ğŸš€ æ•°æ®æµæ°´çº¿å¯åŠ¨")
        
        try:
            # æ­¥éª¤1ï¼šæ•°æ®é‡‡é›†
            raw_data = await self._collect_data()
            
            # æ­¥éª¤2ï¼šæ•°æ®å¤„ç†
            processed_data = await self._process_data(raw_data)
            
            # æ­¥éª¤3ï¼šæ•°æ®å­˜å‚¨
            await self._store_data(processed_data)
            
            # æ­¥éª¤4ï¼šç”ŸæˆæŠ¥å‘Š
            await self._generate_report()
            
            self.stats['end_time'] = datetime.now()
            self.logger.info("âœ… æ•°æ®æµæ°´çº¿å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼š{e}")
            raise
    
    async def _collect_data(self) -> List[Dict]:
        """æ•°æ®é‡‡é›†é˜¶æ®µ"""
        self.logger.info("ğŸ“¥ å¼€å§‹æ•°æ®é‡‡é›†")
        
        data_sources = self.config['data_sources']
        all_data = []
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            
            for source in data_sources:
                if source['type'] == 'api':
                    task = self._fetch_api_data(session, source)
                elif source['type'] == 'file':
                    task = self._fetch_file_data(source)
                else:
                    continue
                
                tasks.append(task)
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰é‡‡é›†ä»»åŠ¡
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"âŒ æ•°æ®æº {i} é‡‡é›†å¤±è´¥ï¼š{result}")
                    self.stats['failed_tasks'] += 1
                else:
                    all_data.extend(result)
                    self.stats['completed_tasks'] += 1
        
        self.logger.info(f"ğŸ“Š æ•°æ®é‡‡é›†å®Œæˆï¼Œå…±è·å– {len(all_data)} æ¡è®°å½•")
        return all_data
    
    async def _fetch_api_data(self, session: aiohttp.ClientSession, source: Dict) -> List[Dict]:
        """ä»APIè·å–æ•°æ®"""
        try:
            async with session.get(source['url'], timeout=30) as response:
                response.raise_for_status()
                data = await response.json()
                
                # æ ¹æ®é…ç½®æå–æ•°æ®
                if 'data_path' in source:
                    for key in source['data_path'].split('.'):
                        data = data[key]
                
                return data if isinstance(data, list) else [data]
                
        except Exception as e:
            self.logger.error(f"âŒ APIæ•°æ®è·å–å¤±è´¥ {source['url']}: {e}")
            raise
    
    async def _fetch_file_data(self, source: Dict) -> List[Dict]:
        """ä»æ–‡ä»¶è·å–æ•°æ®"""
        try:
            file_path = Path(source['path'])
            
            if file_path.suffix.lower() == '.json':
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                    return data if isinstance(data, list) else [data]
            
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†Excelæ–‡ä»¶
                loop = asyncio.get_event_loop()
                with ThreadPoolExecutor() as executor:
                    df = await loop.run_in_executor(
                        executor, pd.read_excel, str(file_path)
                    )
                    return df.to_dict('records')
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{file_path.suffix}")
                
        except Exception as e:
            self.logger.error(f"âŒ æ–‡ä»¶æ•°æ®è·å–å¤±è´¥ {source['path']}: {e}")
            raise
    
    async def _process_data(self, raw_data: List[Dict]) -> List[Dict]:
        """æ•°æ®å¤„ç†é˜¶æ®µ"""
        self.logger.info("ğŸ”„ å¼€å§‹æ•°æ®å¤„ç†")
        
        processed_data = []
        processing_rules = self.config['processing']['rules']
        
        for record in raw_data:
            try:
                processed_record = await self._process_single_record(record, processing_rules)
                if processed_record:
                    processed_data.append(processed_record)
                    self.stats['processed_records'] += 1
                    
            except Exception as e:
                self.logger.warning(f"âš ï¸ è®°å½•å¤„ç†å¤±è´¥ï¼š{e}")
                continue
        
        self.logger.info(f"ğŸ“Š æ•°æ®å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(processed_data)} æ¡è®°å½•")
        return processed_data
    
    async def _process_single_record(self, record: Dict, rules: List[Dict]) -> Dict:
        """å¤„ç†å•æ¡è®°å½•"""
        processed = record.copy()
        
        for rule in rules:
            rule_type = rule['type']
            
            if rule_type == 'rename':
                # é‡å‘½åå­—æ®µ
                old_name = rule['from']
                new_name = rule['to']
                if old_name in processed:
                    processed[new_name] = processed.pop(old_name)
            
            elif rule_type == 'transform':
                # æ•°æ®è½¬æ¢
                field = rule['field']
                transform_type = rule['transform']
                
                if field in processed:
                    if transform_type == 'uppercase':
                        processed[field] = str(processed[field]).upper()
                    elif transform_type == 'lowercase':
                        processed[field] = str(processed[field]).lower()
                    elif transform_type == 'date_format':
                        # æ—¥æœŸæ ¼å¼åŒ–
                        date_str = processed[field]
                        if isinstance(date_str, str):
                            try:
                                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                                processed[field] = dt.strftime(rule['format'])
                            except ValueError:
                                pass
            
            elif rule_type == 'filter':
                # æ•°æ®è¿‡æ»¤
                field = rule['field']
                condition = rule['condition']
                value = rule['value']
                
                if field in processed:
                    record_value = processed[field]
                    
                    if condition == 'equals' and record_value != value:
                        return None
                    elif condition == 'contains' and value not in str(record_value):
                        return None
                    elif condition == 'greater_than' and record_value <= value:
                        return None
            
            elif rule_type == 'add_field':
                # æ·»åŠ å­—æ®µ
                field_name = rule['name']
                field_value = rule['value']
                
                # æ”¯æŒåŠ¨æ€å€¼
                if field_value == '${current_time}':
                    field_value = datetime.now().isoformat()
                elif field_value == '${date}':
                    field_value = datetime.now().strftime('%Y-%m-%d')
                
                processed[field_name] = field_value
        
        return processed
    
    async def _store_data(self, data: List[Dict]):
        """æ•°æ®å­˜å‚¨é˜¶æ®µ"""
        self.logger.info("ğŸ’¾ å¼€å§‹æ•°æ®å­˜å‚¨")
        
        storage_config = self.config['storage']
        
        for storage in storage_config:
            try:
                if storage['type'] == 'json':
                    await self._store_to_json(data, storage)
                elif storage['type'] == 'excel':
                    await self._store_to_excel(data, storage)
                elif storage['type'] == 'database':
                    await self._store_to_database(data, storage)
                
                self.logger.info(f"âœ… æ•°æ®å·²å­˜å‚¨åˆ° {storage['type']}")
                
            except Exception as e:
                self.logger.error(f"âŒ å­˜å‚¨å¤±è´¥ {storage['type']}: {e}")
    
    async def _store_to_json(self, data: List[Dict], config: Dict):
        """å­˜å‚¨åˆ°JSONæ–‡ä»¶"""
        output_path = Path(config['path'])
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2, ensure_ascii=False))
    
    async def _store_to_excel(self, data: List[Dict], config: Dict):
        """å­˜å‚¨åˆ°Excelæ–‡ä»¶"""
        output_path = Path(config['path'])
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†Excelå†™å…¥
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            df = pd.DataFrame(data)
            await loop.run_in_executor(
                executor, df.to_excel, str(output_path), False
            )
    
    async def _store_to_database(self, data: List[Dict], config: Dict):
        """å­˜å‚¨åˆ°æ•°æ®åº“"""
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®åº“å­˜å‚¨é€»è¾‘
        self.logger.info(f"ğŸ“Š æ¨¡æ‹Ÿå­˜å‚¨ {len(data)} æ¡è®°å½•åˆ°æ•°æ®åº“")
    
    async def _generate_report(self):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        self.logger.info("ğŸ“‹ ç”Ÿæˆå¤„ç†æŠ¥å‘Š")
        
        duration = self.stats['end_time'] - self.stats['start_time']
        
        report = {
            'pipeline_info': {
                'start_time': self.stats['start_time'].isoformat(),
                'end_time': self.stats['end_time'].isoformat(),
                'duration_seconds': duration.total_seconds(),
                'status': 'completed'
            },
            'statistics': {
                'total_tasks': self.stats['total_tasks'],
                'completed_tasks': self.stats['completed_tasks'],
                'failed_tasks': self.stats['failed_tasks'],
                'processed_records': self.stats['processed_records'],
                'success_rate': (self.stats['completed_tasks'] / max(self.stats['total_tasks'], 1)) * 100
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(self.config['output']['report_path'])
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        async with aiofiles.open(report_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(report, indent=2, ensure_ascii=False))
        
        self.logger.info(f"ğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆï¼š{report_path}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®æµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š")
        print("="*60)
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´ï¼š{duration}")
        print(f"ğŸ“‹ æ€»ä»»åŠ¡æ•°ï¼š{self.stats['total_tasks']}")
        print(f"âœ… æˆåŠŸä»»åŠ¡ï¼š{self.stats['completed_tasks']}")
        print(f"âŒ å¤±è´¥ä»»åŠ¡ï¼š{self.stats['failed_tasks']}")
        print(f"ğŸ“Š å¤„ç†è®°å½•ï¼š{self.stats['processed_records']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡ï¼š{report['statistics']['success_rate']:.1f}%")
        print("="*60)

# é…ç½®æ–‡ä»¶ç¤ºä¾‹
def create_sample_config():
    config = {
        "logging": {
            "file": "D:/testyd/ç»ƒä¹ /pipeline.log"
        },
        "data_sources": [
            {
                "type": "api",
                "url": "https://httpbin.org/json",
                "data_path": ""
            },
            {
                "type": "file",
                "path": "D:/testyd/ç»ƒä¹ /sample_data.json"
            }
        ],
        "processing": {
            "rules": [
                {
                    "type": "add_field",
                    "name": "processed_time",
                    "value": "${current_time}"
                },
                {
                    "type": "add_field",
                    "name": "source",
                    "value": "data_pipeline"
                }
            ]
        },
        "storage": [
            {
                "type": "json",
                "path": "D:/testyd/ç»ƒä¹ /output/processed_data.json"
            },
            {
                "type": "excel",
                "path": "D:/testyd/ç»ƒä¹ /output/processed_data.xlsx"
            }
        ],
        "output": {
            "report_path": "D:/testyd/ç»ƒä¹ /output/pipeline_report.json"
        }
    }
    
    config_file = Path("D:/testyd/ç»ƒä¹ /pipeline_config.json")
    config_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    return str(config_file)

# åˆ›å»ºç¤ºä¾‹æ•°æ®
def create_sample_data():
    sample_data = [
        {
            "id": 1,
            "name": "æµ‹è¯•å•†å“1",
            "price": 99.99,
            "category": "ç”µå­äº§å“",
            "created_at": "2025-09-26T10:00:00Z"
        },
        {
            "id": 2,
            "name": "æµ‹è¯•å•†å“2",
            "price": 199.99,
            "category": "å®¶å±…ç”¨å“",
            "created_at": "2025-09-26T11:00:00Z"
        }
    ]
    
    data_file = Path("D:/testyd/ç»ƒä¹ /sample_data.json")
    data_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False)

# æµ‹è¯•å®Œæ•´æµæ°´çº¿
async def test_data_pipeline():
    # åˆ›å»ºé…ç½®å’Œç¤ºä¾‹æ•°æ®
    config_file = create_sample_config()
    create_sample_data()
    
    # è¿è¡Œæµæ°´çº¿
    pipeline = DataPipeline(config_file)
    await pipeline.run_pipeline()

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    asyncio.run(test_data_pipeline())
```

---

## ğŸ“ å­¦ä¹ å»ºè®®

### å­¦ä¹ è·¯å¾„
1. **ç¬¬1-2å‘¨**ï¼šå®Œæˆåˆçº§ç»ƒä¹ ï¼ŒæŒæ¡åŸºç¡€è¯­æ³•
2. **ç¬¬3-4å‘¨**ï¼šå®Œæˆä¸­çº§ç»ƒä¹ ï¼Œç†è§£é¢å‘å¯¹è±¡å’Œå¼‚å¸¸å¤„ç†
3. **ç¬¬5-6å‘¨**ï¼šå®Œæˆé«˜çº§ç»ƒä¹ ï¼Œå­¦ä¹ å¹¶å‘ç¼–ç¨‹å’Œé«˜çº§ç‰¹æ€§
4. **ç¬¬7å‘¨**ï¼šå®Œæˆç»¼åˆé¡¹ç›®ï¼Œæ•´åˆæ‰€æœ‰çŸ¥è¯†ç‚¹

### å®è·µè¦ç‚¹
1. **æ¯ä¸ªç»ƒä¹ éƒ½è¦äº²è‡ªè¿è¡Œ**ï¼Œè§‚å¯Ÿè¾“å‡ºç»“æœ
2. **å°è¯•ä¿®æ”¹ä»£ç **ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆ
3. **é‡åˆ°é”™è¯¯ä¸è¦å®³æ€•**ï¼Œå­¦ä¼šè¯»æ‡‚é”™è¯¯ä¿¡æ¯
4. **å¤šå†™æ³¨é‡Š**ï¼Œå¸®åŠ©ç†è§£ä»£ç é€»è¾‘
5. **å‚è€ƒåŸå§‹çš„jd_store.py**ï¼Œå¯¹æ¯”å­¦ä¹ 

### æ‰©å±•ç»ƒä¹ 
å®ŒæˆåŸºç¡€ç»ƒä¹ åï¼Œå¯ä»¥å°è¯•ï¼š
1. æ·»åŠ æ›´å¤šçš„æ•°æ®å¤„ç†åŠŸèƒ½
2. å®ç°å›¾å½¢ç•Œé¢ï¼ˆä½¿ç”¨tkinterï¼‰
3. æ·»åŠ æ•°æ®åº“æ“ä½œ
4. å®ç°Web APIæ¥å£
5. æ·»åŠ å•å…ƒæµ‹è¯•

### å­¦ä¹ èµ„æº
- Pythonå®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.python.org/zh-cn/3/
- èœé¸Ÿæ•™ç¨‹ï¼šhttps://www.runoob.com/python3/
- å»–é›ªå³°Pythonæ•™ç¨‹ï¼šhttps://www.liaoxuefeng.com/wiki/1016959663602400

è®°ä½ï¼š**ç¼–ç¨‹æ˜¯ä¸€é—¨å®è·µæ€§å¾ˆå¼ºçš„æŠ€èƒ½ï¼Œå¤šå†™å¤šç»ƒæ˜¯å…³é”®ï¼** ğŸš€