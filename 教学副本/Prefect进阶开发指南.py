#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Prefect è¿›é˜¶å¼€å‘æŒ‡å—
===================

æœ¬æ–‡ä»¶åŒ…å«äº† Prefect çš„è¿›é˜¶ç”¨æ³•å’Œå®é™…åº”ç”¨åœºæ™¯ï¼Œ
æ¶µç›–äº†å¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹ã€é”™è¯¯å¤„ç†ã€å¹¶è¡Œæ‰§è¡Œç­‰é«˜çº§ç‰¹æ€§ã€‚

ä½œè€…: AI Assistant
æ—¥æœŸ: 2024-01-01
ç‰ˆæœ¬: 1.0
"""

import asyncio
import json
import logging
import os
import pandas as pd
import requests
import sqlite3
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any

from prefect import flow, task, get_run_logger
from prefect.blocks.system import Secret
from prefect.deployments import Deployment
from prefect.server.schemas.schedules import CronSchedule
from prefect.task_runners import ConcurrentTaskRunner, SequentialTaskRunner
from prefect.artifacts import create_markdown_artifact
from prefect.results import PersistedResult
from prefect.filesystems import LocalFileSystem

# =============================================================================
# ğŸ“Š æ•°æ®å¤„ç†ä»»åŠ¡é›†åˆ
# =============================================================================

@task(name="æ•°æ®åº“è¿æ¥", description="å»ºç«‹æ•°æ®åº“è¿æ¥", retries=3, retry_delay_seconds=5)
def connect_database(db_path: str = "data/warehouse.db") -> sqlite3.Connection:
    """å»ºç«‹æ•°æ®åº“è¿æ¥"""
    logger = get_run_logger()
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        
        # å»ºç«‹è¿æ¥
        conn = sqlite3.connect(db_path)
        logger.info(f"âœ… æˆåŠŸè¿æ¥æ•°æ®åº“: {db_path}")
        
        # åˆ›å»ºåŸºç¡€è¡¨ç»“æ„
        conn.execute("""
            CREATE TABLE IF NOT EXISTS sales_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT NOT NULL,
                product_name TEXT NOT NULL,
                category TEXT NOT NULL,
                sales_amount REAL NOT NULL,
                quantity INTEGER NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.execute("""
            CREATE TABLE IF NOT EXISTS processed_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                date TEXT NOT NULL,
                total_sales REAL NOT NULL,
                total_quantity INTEGER NOT NULL,
                avg_price REAL NOT NULL,
                processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        return conn
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise

@task(name="APIæ•°æ®æ‹‰å–", description="ä»APIè·å–æ•°æ®", retries=2, retry_delay_seconds=10)
def fetch_api_data(api_url: str, headers: Optional[Dict] = None) -> List[Dict]:
    """ä»APIè·å–æ•°æ®"""
    logger = get_run_logger()
    
    try:
        # æ¨¡æ‹ŸAPIè°ƒç”¨
        if "mock" in api_url:
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            mock_data = []
            for i in range(100):
                mock_data.append({
                    "id": i + 1,
                    "date": (datetime.now() - timedelta(days=i)).strftime("%Y-%m-%d"),
                    "product_name": f"äº§å“_{i % 10 + 1}",
                    "category": ["ç”µå­äº§å“", "æœè£…", "é£Ÿå“", "å®¶å±…"][i % 4],
                    "sales_amount": round(100 + (i * 10.5), 2),
                    "quantity": i % 20 + 1
                })
            
            logger.info(f"âœ… æˆåŠŸè·å–æ¨¡æ‹Ÿæ•°æ®: {len(mock_data)} æ¡è®°å½•")
            return mock_data
        
        # å®é™…APIè°ƒç”¨
        response = requests.get(api_url, headers=headers or {}, timeout=30)
        response.raise_for_status()
        
        data = response.json()
        logger.info(f"âœ… æˆåŠŸè·å–APIæ•°æ®: {len(data)} æ¡è®°å½•")
        return data
        
    except requests.RequestException as e:
        logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise

@task(name="æ•°æ®æ¸…æ´—", description="æ¸…æ´—å’ŒéªŒè¯æ•°æ®", retries=1)
def clean_data(raw_data: List[Dict]) -> pd.DataFrame:
    """æ¸…æ´—å’ŒéªŒè¯æ•°æ®"""
    logger = get_run_logger()
    
    try:
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(raw_data)
        logger.info(f"ğŸ“Š åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
        
        # æ•°æ®æ¸…æ´—æ­¥éª¤
        initial_count = len(df)
        
        # 1. åˆ é™¤é‡å¤è®°å½•
        df = df.drop_duplicates()
        logger.info(f"ğŸ§¹ åˆ é™¤é‡å¤è®°å½•: {initial_count - len(df)} æ¡")
        
        # 2. å¤„ç†ç¼ºå¤±å€¼
        df = df.dropna(subset=['product_name', 'sales_amount'])
        logger.info(f"ğŸ§¹ åˆ é™¤ç¼ºå¤±å€¼è®°å½•: {initial_count - len(df)} æ¡")
        
        # 3. æ•°æ®ç±»å‹è½¬æ¢
        df['date'] = pd.to_datetime(df['date'])
        df['sales_amount'] = pd.to_numeric(df['sales_amount'], errors='coerce')
        df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')
        
        # 4. æ•°æ®éªŒè¯
        df = df[df['sales_amount'] > 0]
        df = df[df['quantity'] > 0]
        
        # 5. å¼‚å¸¸å€¼å¤„ç†
        Q1 = df['sales_amount'].quantile(0.25)
        Q3 = df['sales_amount'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers_count = len(df[(df['sales_amount'] < lower_bound) | (df['sales_amount'] > upper_bound)])
        df = df[(df['sales_amount'] >= lower_bound) & (df['sales_amount'] <= upper_bound)]
        
        logger.info(f"ğŸ§¹ åˆ é™¤å¼‚å¸¸å€¼: {outliers_count} æ¡")
        logger.info(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ€ç»ˆæ•°æ®è¡Œæ•°: {len(df)}")
        
        return df
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
        raise

@task(name="æ•°æ®è½¬æ¢", description="æ•°æ®è½¬æ¢å’Œèšåˆ", retries=1)
def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    """æ•°æ®è½¬æ¢å’Œèšåˆ"""
    logger = get_run_logger()
    
    try:
        # æŒ‰æ—¥æœŸèšåˆæ•°æ®
        daily_summary = df.groupby(df['date'].dt.date).agg({
            'sales_amount': ['sum', 'mean', 'count'],
            'quantity': 'sum'
        }).round(2)
        
        # é‡å‘½ååˆ—
        daily_summary.columns = ['total_sales', 'avg_sales', 'transaction_count', 'total_quantity']
        daily_summary = daily_summary.reset_index()
        daily_summary['date'] = daily_summary['date'].astype(str)
        
        # è®¡ç®—å¹³å‡ä»·æ ¼
        daily_summary['avg_price'] = (daily_summary['total_sales'] / daily_summary['total_quantity']).round(2)
        
        logger.info(f"âœ… æ•°æ®è½¬æ¢å®Œæˆï¼Œèšåˆåæ•°æ®è¡Œæ•°: {len(daily_summary)}")
        
        return daily_summary
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®è½¬æ¢å¤±è´¥: {str(e)}")
        raise

@task(name="æ•°æ®å­˜å‚¨", description="å°†æ•°æ®å­˜å‚¨åˆ°æ•°æ®åº“", retries=2)
def store_data(conn: sqlite3.Connection, df: pd.DataFrame, table_name: str = "processed_data") -> int:
    """å°†æ•°æ®å­˜å‚¨åˆ°æ•°æ®åº“"""
    logger = get_run_logger()
    
    try:
        # å­˜å‚¨æ•°æ®
        rows_affected = df.to_sql(table_name, conn, if_exists='append', index=False)
        conn.commit()
        
        logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(df)} æ¡è®°å½•åˆ°è¡¨ {table_name}")
        return len(df)
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å­˜å‚¨å¤±è´¥: {str(e)}")
        raise
    finally:
        conn.close()

@task(name="ç”ŸæˆæŠ¥å‘Š", description="ç”Ÿæˆæ•°æ®å¤„ç†æŠ¥å‘Š")
def generate_report(processed_count: int, start_time: datetime) -> str:
    """ç”Ÿæˆæ•°æ®å¤„ç†æŠ¥å‘Š"""
    logger = get_run_logger()
    
    try:
        end_time = datetime.now()
        duration = end_time - start_time
        
        report = f"""
# ğŸ“Š æ•°æ®å¤„ç†æŠ¥å‘Š

## å¤„ç†æ¦‚è¦
- **å¼€å§‹æ—¶é—´**: {start_time.strftime('%Y-%m-%d %H:%M:%S')}
- **ç»“æŸæ—¶é—´**: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
- **å¤„ç†æ—¶é•¿**: {duration.total_seconds():.2f} ç§’
- **å¤„ç†è®°å½•æ•°**: {processed_count} æ¡

## å¤„ç†çŠ¶æ€
âœ… **æˆåŠŸå®Œæˆ**

## ä¸‹ä¸€æ­¥æ“ä½œ
- æ•°æ®å·²å­˜å‚¨åˆ°æ•°æ®åº“
- å¯ä»¥è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†æå’Œå¯è§†åŒ–
- å»ºè®®è®¾ç½®å®šæœŸæ•°æ®è´¨é‡æ£€æŸ¥
        """
        
        # åˆ›å»º Prefect æŠ¥å‘Šå·¥ä»¶
        create_markdown_artifact(
            key="data-processing-report",
            markdown=report,
            description="æ•°æ®å¤„ç†å®ŒæˆæŠ¥å‘Š"
        )
        
        logger.info("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report
        
    except Exception as e:
        logger.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        raise

# =============================================================================
# ğŸ”„ å¹¶è¡Œå¤„ç†ä»»åŠ¡
# =============================================================================

@task(name="å¹¶è¡Œæ•°æ®å¤„ç†", description="å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æº")
async def parallel_data_processing(data_sources: List[str]) -> List[Dict]:
    """å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æº"""
    logger = get_run_logger()
    
    async def process_single_source(source: str) -> Dict:
        """å¤„ç†å•ä¸ªæ•°æ®æº"""
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        
        return {
            "source": source,
            "status": "completed",
            "records": len(source) * 10,  # æ¨¡æ‹Ÿè®°å½•æ•°
            "processed_at": datetime.now().isoformat()
        }
    
    try:
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ•°æ®æº
        tasks = [process_single_source(source) for source in data_sources]
        results = await asyncio.gather(*tasks)
        
        logger.info(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªæ•°æ®æº")
        return results
        
    except Exception as e:
        logger.error(f"âŒ å¹¶è¡Œå¤„ç†å¤±è´¥: {str(e)}")
        raise

@task(name="æ‰¹é‡æ–‡ä»¶å¤„ç†", description="æ‰¹é‡å¤„ç†æ–‡ä»¶")
def batch_file_processing(file_patterns: List[str], output_dir: str = "output") -> List[str]:
    """æ‰¹é‡å¤„ç†æ–‡ä»¶"""
    logger = get_run_logger()
    
    try:
        os.makedirs(output_dir, exist_ok=True)
        processed_files = []
        
        for pattern in file_patterns:
            # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†
            output_file = os.path.join(output_dir, f"processed_{pattern}.json")
            
            # ç”Ÿæˆæ¨¡æ‹Ÿå¤„ç†ç»“æœ
            result = {
                "input_pattern": pattern,
                "processed_at": datetime.now().isoformat(),
                "status": "success",
                "output_records": len(pattern) * 5
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            processed_files.append(output_file)
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {pattern} -> {output_file}")
        
        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶")
        return processed_files
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}")
        raise

# =============================================================================
# ğŸŒŠ å¤æ‚æ•°æ®æµç¨‹
# =============================================================================

@flow(name="å®Œæ•´æ•°æ®å¤„ç†æµç¨‹", description="ç«¯åˆ°ç«¯çš„æ•°æ®å¤„ç†æµç¨‹", 
      task_runner=ConcurrentTaskRunner())
def complete_data_pipeline(
    api_url: str = "https://api.mock.com/data",
    db_path: str = "data/warehouse.db"
) -> str:
    """å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹"""
    logger = get_run_logger()
    start_time = datetime.now()
    
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†æµç¨‹")
    
    try:
        # 1. å»ºç«‹æ•°æ®åº“è¿æ¥
        conn = connect_database(db_path)
        
        # 2. è·å–æ•°æ®
        raw_data = fetch_api_data(api_url)
        
        # 3. æ•°æ®æ¸…æ´—
        clean_df = clean_data(raw_data)
        
        # 4. æ•°æ®è½¬æ¢
        transformed_df = transform_data(clean_df)
        
        # 5. æ•°æ®å­˜å‚¨
        processed_count = store_data(conn, transformed_df)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        report = generate_report(processed_count, start_time)
        
        logger.info("âœ… å®Œæ•´æ•°æ®å¤„ç†æµç¨‹æ‰§è¡ŒæˆåŠŸ")
        return report
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®å¤„ç†æµç¨‹å¤±è´¥: {str(e)}")
        raise

@flow(name="å¹¶è¡Œæ•°æ®å¤„ç†æµç¨‹", description="å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æºçš„æµç¨‹",
      task_runner=ConcurrentTaskRunner())
async def parallel_processing_pipeline(
    data_sources: List[str] = None,
    file_patterns: List[str] = None
) -> Dict[str, Any]:
    """å¹¶è¡Œæ•°æ®å¤„ç†æµç¨‹"""
    logger = get_run_logger()
    
    if data_sources is None:
        data_sources = ["source_1", "source_2", "source_3", "source_4"]
    
    if file_patterns is None:
        file_patterns = ["pattern_a", "pattern_b", "pattern_c"]
    
    logger.info("ğŸš€ å¼€å§‹å¹¶è¡Œæ•°æ®å¤„ç†æµç¨‹")
    
    try:
        # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä»»åŠ¡
        data_results = await parallel_data_processing(data_sources)
        file_results = batch_file_processing(file_patterns)
        
        # æ±‡æ€»ç»“æœ
        summary = {
            "data_processing": {
                "sources_processed": len(data_results),
                "total_records": sum(result["records"] for result in data_results),
                "results": data_results
            },
            "file_processing": {
                "files_processed": len(file_results),
                "output_files": file_results
            },
            "completed_at": datetime.now().isoformat()
        }
        
        logger.info("âœ… å¹¶è¡Œæ•°æ®å¤„ç†æµç¨‹æ‰§è¡ŒæˆåŠŸ")
        return summary
        
    except Exception as e:
        logger.error(f"âŒ å¹¶è¡Œå¤„ç†æµç¨‹å¤±è´¥: {str(e)}")
        raise

# =============================================================================
# ğŸ”§ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
# =============================================================================

@task(name="å®¹é”™ä»»åŠ¡", description="å…·æœ‰å®Œå–„é”™è¯¯å¤„ç†çš„ä»»åŠ¡", 
      retries=3, retry_delay_seconds=5)
def fault_tolerant_task(data: Any, fail_probability: float = 0.3) -> Dict[str, Any]:
    """å…·æœ‰å®¹é”™æœºåˆ¶çš„ä»»åŠ¡"""
    logger = get_run_logger()
    
    import random
    
    try:
        # æ¨¡æ‹Ÿéšæœºå¤±è´¥
        if random.random() < fail_probability:
            raise Exception("æ¨¡æ‹Ÿä»»åŠ¡å¤±è´¥")
        
        # å¤„ç†æ•°æ®
        result = {
            "input_data": str(data)[:100],  # é™åˆ¶é•¿åº¦
            "processed_at": datetime.now().isoformat(),
            "status": "success",
            "processing_time": random.uniform(1, 5)
        }
        
        logger.info("âœ… å®¹é”™ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        return result
        
    except Exception as e:
        logger.warning(f"âš ï¸ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œå°†é‡è¯•: {str(e)}")
        raise

@flow(name="å®¹é”™æ•°æ®æµç¨‹", description="å…·æœ‰å®Œå–„é”™è¯¯å¤„ç†çš„æ•°æ®æµç¨‹")
def fault_tolerant_pipeline(input_data: List[Any]) -> List[Dict[str, Any]]:
    """å®¹é”™æ•°æ®å¤„ç†æµç¨‹"""
    logger = get_run_logger()
    
    logger.info("ğŸš€ å¼€å§‹å®¹é”™æ•°æ®å¤„ç†æµç¨‹")
    
    results = []
    failed_items = []
    
    for i, data in enumerate(input_data):
        try:
            result = fault_tolerant_task(data, fail_probability=0.2)
            results.append(result)
            logger.info(f"âœ… é¡¹ç›® {i+1} å¤„ç†æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ é¡¹ç›® {i+1} å¤„ç†å¤±è´¥: {str(e)}")
            failed_items.append({"index": i, "data": data, "error": str(e)})
    
    # ç”Ÿæˆå¤„ç†æ‘˜è¦
    summary = {
        "total_items": len(input_data),
        "successful_items": len(results),
        "failed_items": len(failed_items),
        "success_rate": len(results) / len(input_data) * 100,
        "results": results,
        "failures": failed_items
    }
    
    logger.info(f"âœ… å®¹é”™æµç¨‹å®Œæˆï¼ŒæˆåŠŸç‡: {summary['success_rate']:.1f}%")
    
    return summary

# =============================================================================
# ğŸ“ˆ ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†
# =============================================================================

@task(name="æ€§èƒ½ç›‘æ§", description="æ”¶é›†æ€§èƒ½æŒ‡æ ‡")
def collect_performance_metrics() -> Dict[str, Any]:
    """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
    logger = get_run_logger()
    
    try:
        import psutil
        
        # ç³»ç»ŸæŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "system": {
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "memory_available_gb": memory.available / (1024**3),
                "disk_percent": disk.percent,
                "disk_free_gb": disk.free / (1024**3)
            },
            "process": {
                "pid": os.getpid(),
                "memory_mb": psutil.Process().memory_info().rss / (1024**2)
            }
        }
        
        logger.info(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡æ”¶é›†å®Œæˆ: CPU {cpu_percent}%, å†…å­˜ {memory.percent}%")
        return metrics
        
    except ImportError:
        logger.warning("âš ï¸ psutil æœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€æŒ‡æ ‡")
        return {
            "timestamp": datetime.now().isoformat(),
            "basic_metrics": {
                "pid": os.getpid(),
                "working_directory": os.getcwd()
            }
        }
    except Exception as e:
        logger.error(f"âŒ æŒ‡æ ‡æ”¶é›†å¤±è´¥: {str(e)}")
        raise

@flow(name="ç›‘æ§æ•°æ®æµç¨‹", description="å¸¦ç›‘æ§çš„æ•°æ®å¤„ç†æµç¨‹")
def monitored_data_pipeline(data_size: int = 1000) -> Dict[str, Any]:
    """å¸¦ç›‘æ§çš„æ•°æ®å¤„ç†æµç¨‹"""
    logger = get_run_logger()
    
    logger.info("ğŸš€ å¼€å§‹ç›‘æ§æ•°æ®å¤„ç†æµç¨‹")
    
    # æ”¶é›†å¼€å§‹æ—¶çš„æ€§èƒ½æŒ‡æ ‡
    start_metrics = collect_performance_metrics()
    start_time = time.time()
    
    try:
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
        processed_data = []
        for i in range(data_size):
            processed_data.append({
                "id": i,
                "value": i * 2,
                "processed_at": datetime.now().isoformat()
            })
            
            # æ¯å¤„ç†100æ¡è®°å½•æ”¶é›†ä¸€æ¬¡æŒ‡æ ‡
            if (i + 1) % 100 == 0:
                current_metrics = collect_performance_metrics()
                logger.info(f"ğŸ“Š å·²å¤„ç† {i+1}/{data_size} æ¡è®°å½•")
        
        # æ”¶é›†ç»“æŸæ—¶çš„æ€§èƒ½æŒ‡æ ‡
        end_metrics = collect_performance_metrics()
        end_time = time.time()
        
        # è®¡ç®—å¤„ç†ç»Ÿè®¡
        processing_time = end_time - start_time
        throughput = data_size / processing_time
        
        result = {
            "processing_summary": {
                "total_records": data_size,
                "processing_time_seconds": processing_time,
                "throughput_records_per_second": throughput,
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.fromtimestamp(end_time).isoformat()
            },
            "performance_metrics": {
                "start": start_metrics,
                "end": end_metrics
            },
            "data_sample": processed_data[:5]  # åªè¿”å›å‰5æ¡ä½œä¸ºæ ·æœ¬
        }
        
        logger.info(f"âœ… ç›‘æ§æµç¨‹å®Œæˆï¼Œå¤„ç†é€Ÿåº¦: {throughput:.2f} è®°å½•/ç§’")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ç›‘æ§æµç¨‹å¤±è´¥: {str(e)}")
        raise

# =============================================================================
# ğŸš€ éƒ¨ç½²é…ç½®
# =============================================================================

def create_advanced_deployments():
    """åˆ›å»ºè¿›é˜¶éƒ¨ç½²é…ç½®"""
    
    # 1. å®Œæ•´æ•°æ®å¤„ç†æµç¨‹éƒ¨ç½²
    complete_pipeline_deployment = Deployment.build_from_flow(
        flow=complete_data_pipeline,
        name="å®Œæ•´æ•°æ®å¤„ç†-å®šæ—¶æ‰§è¡Œ",
        schedule=CronSchedule(cron="0 2 * * *", timezone="Asia/Shanghai"),  # æ¯å¤©å‡Œæ™¨2ç‚¹
        tags=["æ•°æ®å¤„ç†", "ETL", "å®šæ—¶ä»»åŠ¡"],
        description="å®Œæ•´çš„ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµç¨‹ï¼ŒåŒ…å«æ•°æ®è·å–ã€æ¸…æ´—ã€è½¬æ¢å’Œå­˜å‚¨",
        parameters={
            "api_url": "https://api.mock.com/data",
            "db_path": "data/warehouse.db"
        }
    )
    
    # 2. å¹¶è¡Œå¤„ç†æµç¨‹éƒ¨ç½²
    parallel_pipeline_deployment = Deployment.build_from_flow(
        flow=parallel_processing_pipeline,
        name="å¹¶è¡Œæ•°æ®å¤„ç†-æ‰‹åŠ¨æ‰§è¡Œ",
        tags=["å¹¶è¡Œå¤„ç†", "é«˜æ€§èƒ½", "æ‰‹åŠ¨æ‰§è¡Œ"],
        description="å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®æºå’Œæ–‡ä»¶çš„é«˜æ€§èƒ½æµç¨‹",
        parameters={
            "data_sources": ["source_1", "source_2", "source_3", "source_4", "source_5"],
            "file_patterns": ["pattern_a", "pattern_b", "pattern_c", "pattern_d"]
        }
    )
    
    # 3. å®¹é”™æµç¨‹éƒ¨ç½²
    fault_tolerant_deployment = Deployment.build_from_flow(
        flow=fault_tolerant_pipeline,
        name="å®¹é”™æ•°æ®å¤„ç†-å®šæ—¶æ‰§è¡Œ",
        schedule=CronSchedule(cron="0 */4 * * *", timezone="Asia/Shanghai"),  # æ¯4å°æ—¶
        tags=["å®¹é”™å¤„ç†", "å¯é æ€§", "å®šæ—¶ä»»åŠ¡"],
        description="å…·æœ‰å®Œå–„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶çš„æ•°æ®å¤„ç†æµç¨‹",
        parameters={
            "input_data": [f"data_item_{i}" for i in range(20)]
        }
    )
    
    # 4. ç›‘æ§æµç¨‹éƒ¨ç½²
    monitored_pipeline_deployment = Deployment.build_from_flow(
        flow=monitored_data_pipeline,
        name="ç›‘æ§æ•°æ®å¤„ç†-æ‰‹åŠ¨æ‰§è¡Œ",
        tags=["æ€§èƒ½ç›‘æ§", "æŒ‡æ ‡æ”¶é›†", "æ‰‹åŠ¨æ‰§è¡Œ"],
        description="å¸¦æœ‰æ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†çš„æ•°æ®å¤„ç†æµç¨‹",
        parameters={
            "data_size": 5000
        }
    )
    
    # åº”ç”¨æ‰€æœ‰éƒ¨ç½²
    deployments = [
        complete_pipeline_deployment,
        parallel_pipeline_deployment,
        fault_tolerant_deployment,
        monitored_pipeline_deployment
    ]
    
    for deployment in deployments:
        deployment.apply()
        print(f"âœ… éƒ¨ç½²å·²åˆ›å»º: {deployment.name}")
    
    print(f"\nğŸ‰ æˆåŠŸåˆ›å»º {len(deployments)} ä¸ªè¿›é˜¶éƒ¨ç½²é…ç½®ï¼")

# =============================================================================
# ğŸ§ª æµ‹è¯•å’Œç¤ºä¾‹
# =============================================================================

def test_advanced_flows():
    """æµ‹è¯•è¿›é˜¶æµç¨‹"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•è¿›é˜¶æµç¨‹...")
    
    # æµ‹è¯•å®Œæ•´æ•°æ®å¤„ç†æµç¨‹
    print("\n1. æµ‹è¯•å®Œæ•´æ•°æ®å¤„ç†æµç¨‹")
    try:
        result = complete_data_pipeline(api_url="https://api.mock.com/data")
        print("âœ… å®Œæ•´æ•°æ®å¤„ç†æµç¨‹æµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“Š å¤„ç†ç»“æœé¢„è§ˆ: {result[:200]}...")
    except Exception as e:
        print(f"âŒ å®Œæ•´æ•°æ®å¤„ç†æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•å®¹é”™æµç¨‹
    print("\n2. æµ‹è¯•å®¹é”™æµç¨‹")
    try:
        test_data = [f"test_item_{i}" for i in range(10)]
        result = fault_tolerant_pipeline(test_data)
        print("âœ… å®¹é”™æµç¨‹æµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“Š æˆåŠŸç‡: {result['success_rate']:.1f}%")
    except Exception as e:
        print(f"âŒ å®¹é”™æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•ç›‘æ§æµç¨‹
    print("\n3. æµ‹è¯•ç›‘æ§æµç¨‹")
    try:
        result = monitored_data_pipeline(data_size=100)
        print("âœ… ç›‘æ§æµç¨‹æµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {result['processing_summary']['throughput_records_per_second']:.2f} è®°å½•/ç§’")
    except Exception as e:
        print(f"âŒ ç›‘æ§æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nğŸ‰ è¿›é˜¶æµç¨‹æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    print("ğŸš€ Prefect è¿›é˜¶å¼€å‘æŒ‡å—")
    print("=" * 50)
    
    # é€‰æ‹©æ“ä½œ
    print("\nè¯·é€‰æ‹©æ“ä½œ:")
    print("1. åˆ›å»ºè¿›é˜¶éƒ¨ç½²é…ç½®")
    print("2. æµ‹è¯•è¿›é˜¶æµç¨‹")
    print("3. æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
    
    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
    
    if choice == "1":
        create_advanced_deployments()
    elif choice == "2":
        test_advanced_flows()
    elif choice == "3":
        print("""
ğŸ”§ Prefect è¿›é˜¶å¼€å‘æŒ‡å—ä½¿ç”¨è¯´æ˜

æœ¬æ–‡ä»¶åŒ…å«äº†ä»¥ä¸‹è¿›é˜¶åŠŸèƒ½:

ğŸ“Š æ•°æ®å¤„ç†ä»»åŠ¡:
- connect_database: æ•°æ®åº“è¿æ¥ç®¡ç†
- fetch_api_data: APIæ•°æ®è·å–
- clean_data: æ•°æ®æ¸…æ´—å’ŒéªŒè¯
- transform_data: æ•°æ®è½¬æ¢å’Œèšåˆ
- store_data: æ•°æ®å­˜å‚¨
- generate_report: æŠ¥å‘Šç”Ÿæˆ

ğŸ”„ å¹¶è¡Œå¤„ç†:
- parallel_data_processing: å¼‚æ­¥å¹¶è¡Œå¤„ç†
- batch_file_processing: æ‰¹é‡æ–‡ä»¶å¤„ç†

ğŸŒŠ å¤æ‚æµç¨‹:
- complete_data_pipeline: å®Œæ•´ETLæµç¨‹
- parallel_processing_pipeline: å¹¶è¡Œå¤„ç†æµç¨‹

ğŸ”§ é”™è¯¯å¤„ç†:
- fault_tolerant_task: å®¹é”™ä»»åŠ¡
- fault_tolerant_pipeline: å®¹é”™æµç¨‹

ğŸ“ˆ ç›‘æ§æŒ‡æ ‡:
- collect_performance_metrics: æ€§èƒ½ç›‘æ§
- monitored_data_pipeline: ç›‘æ§æµç¨‹

ğŸš€ ä½¿ç”¨æ–¹æ³•:
1. è¿è¡Œè„šæœ¬åˆ›å»ºéƒ¨ç½²é…ç½®
2. åœ¨ Prefect UI ä¸­æŸ¥çœ‹å’Œæ‰§è¡Œæµç¨‹
3. ç›‘æ§æ‰§è¡ŒçŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡

ğŸ’¡ æœ€ä½³å®è·µ:
- ä½¿ç”¨é€‚å½“çš„é‡è¯•ç­–ç•¥
- å®æ–½å®Œå–„çš„é”™è¯¯å¤„ç†
- æ”¶é›†æ€§èƒ½æŒ‡æ ‡å’Œç›‘æ§
- ä½¿ç”¨å¹¶è¡Œå¤„ç†æé«˜æ•ˆç‡
- åˆ›å»ºè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Š
        """)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¿è¡Œè„šæœ¬")